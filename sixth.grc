options:
  parameters:
    author: lab512
    category: '[GRC Hier Blocks]'
    cmake_opt: ''
    comment: ''
    copyright: ''
    description: ''
    gen_cmake: 'On'
    gen_linking: dynamic
    generate_options: qt_gui
    hier_block_src_path: '.:'
    id: forth
    max_nouts: '0'
    output_language: python
    placement: (0,0)
    qt_qss_theme: ''
    realtime_scheduling: ''
    run: 'True'
    run_command: '{python} -u {filename}'
    run_options: prompt
    sizing_mode: fixed
    thread_safe_setters: ''
    title: Tx RL Channel selector
    window_size: ''
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [8, 8]
    rotation: 0
    state: enabled

blocks:
- name: samp_rate
  id: variable
  parameters:
    comment: ''
    value: '32000'
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [9, 102]
    rotation: 0
    state: enabled
- name: analog_sig_source_x_0
  id: analog_sig_source_x
  parameters:
    affinity: ''
    alias: ''
    amp: '1'
    comment: ''
    freq: '1000'
    maxoutbuf: '0'
    minoutbuf: '0'
    offset: '0'
    phase: '0'
    samp_rate: samp_rate
    type: complex
    waveform: analog.GR_COS_WAVE
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [204, 9]
    rotation: 0
    state: true
- name: analog_sig_source_x_0_0
  id: analog_sig_source_x
  parameters:
    affinity: ''
    alias: ''
    amp: '1'
    comment: ''
    freq: '2000'
    maxoutbuf: '0'
    minoutbuf: '0'
    offset: '0'
    phase: '0'
    samp_rate: samp_rate
    type: complex
    waveform: analog.GR_COS_WAVE
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [206, 136]
    rotation: 0
    state: true
- name: analog_sig_source_x_0_0_0
  id: analog_sig_source_x
  parameters:
    affinity: ''
    alias: ''
    amp: '1'
    comment: ''
    freq: '3000'
    maxoutbuf: '0'
    minoutbuf: '0'
    offset: '0'
    phase: '0'
    samp_rate: samp_rate
    type: complex
    waveform: analog.GR_COS_WAVE
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [206, 263]
    rotation: 0
    state: true
- name: analog_sig_source_x_0_0_0_0
  id: analog_sig_source_x
  parameters:
    affinity: ''
    alias: ''
    amp: '1'
    comment: ''
    freq: '4000'
    maxoutbuf: '0'
    minoutbuf: '0'
    offset: '0'
    phase: '0'
    samp_rate: samp_rate
    type: complex
    waveform: analog.GR_COS_WAVE
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [206, 389]
    rotation: 0
    state: true
- name: analog_sig_source_x_0_0_0_0_0
  id: analog_sig_source_x
  parameters:
    affinity: ''
    alias: ''
    amp: '1'
    comment: ''
    freq: '5000'
    maxoutbuf: '0'
    minoutbuf: '0'
    offset: '0'
    phase: '0'
    samp_rate: samp_rate
    type: complex
    waveform: analog.GR_COS_WAVE
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [205, 513]
    rotation: 0
    state: true
- name: analog_sig_source_x_0_0_0_0_0_0
  id: analog_sig_source_x
  parameters:
    affinity: ''
    alias: ''
    amp: '1'
    comment: ''
    freq: '6000'
    maxoutbuf: '0'
    minoutbuf: '0'
    offset: '0'
    phase: '0'
    samp_rate: samp_rate
    type: complex
    waveform: analog.GR_COS_WAVE
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [205, 640]
    rotation: 0
    state: true
- name: analog_sig_source_x_0_0_0_0_0_0_0
  id: analog_sig_source_x
  parameters:
    affinity: ''
    alias: ''
    amp: '1'
    comment: ''
    freq: '7000'
    maxoutbuf: '0'
    minoutbuf: '0'
    offset: '0'
    phase: '0'
    samp_rate: samp_rate
    type: complex
    waveform: analog.GR_COS_WAVE
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [206, 765]
    rotation: 0
    state: true
- name: blocks_add_xx_0_0_0_0_0_0_0_0
  id: blocks_add_xx
  parameters:
    affinity: ''
    alias: ''
    comment: ''
    maxoutbuf: '0'
    minoutbuf: '0'
    num_inputs: '7'
    type: complex
    vlen: '1'
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [1144, 644]
    rotation: 0
    state: true
- name: channels_channel_model_0
  id: channels_channel_model
  parameters:
    affinity: ''
    alias: ''
    block_tags: 'False'
    comment: ''
    epsilon: '1.0'
    freq_offset: '0.001'
    maxoutbuf: '0'
    minoutbuf: '0'
    noise_voltage: '0.1'
    seed: '0'
    taps: 1.0 + 1.0j
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [720, 7]
    rotation: 0
    state: true
- name: channels_channel_model_0_0
  id: channels_channel_model
  parameters:
    affinity: ''
    alias: ''
    block_tags: 'False'
    comment: ''
    epsilon: '1.0'
    freq_offset: '0.001'
    maxoutbuf: '0'
    minoutbuf: '0'
    noise_voltage: '0.1'
    seed: '1'
    taps: 1.0 + 1.0j
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [720, 136]
    rotation: 0
    state: true
- name: channels_channel_model_0_0_0
  id: channels_channel_model
  parameters:
    affinity: ''
    alias: ''
    block_tags: 'False'
    comment: ''
    epsilon: '1.0'
    freq_offset: '0.001'
    maxoutbuf: '0'
    minoutbuf: '0'
    noise_voltage: '0.1'
    seed: '2'
    taps: 1.0 + 1.0j
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [719, 265]
    rotation: 0
    state: true
- name: channels_channel_model_0_0_0_0
  id: channels_channel_model
  parameters:
    affinity: ''
    alias: ''
    block_tags: 'False'
    comment: ''
    epsilon: '1.0'
    freq_offset: '0.001'
    maxoutbuf: '0'
    minoutbuf: '0'
    noise_voltage: '0.1'
    seed: '3'
    taps: 1.0 + 1.0j
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [720, 398]
    rotation: 0
    state: true
- name: channels_channel_model_0_0_0_0_0
  id: channels_channel_model
  parameters:
    affinity: ''
    alias: ''
    block_tags: 'False'
    comment: ''
    epsilon: '1.0'
    freq_offset: '0.001'
    maxoutbuf: '0'
    minoutbuf: '0'
    noise_voltage: '0.1'
    seed: '4'
    taps: 1.0 + 1.0j
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [721, 528]
    rotation: 0
    state: true
- name: channels_channel_model_0_0_0_0_0_0
  id: channels_channel_model
  parameters:
    affinity: ''
    alias: ''
    block_tags: 'False'
    comment: ''
    epsilon: '1.0'
    freq_offset: '0.001'
    maxoutbuf: '0'
    minoutbuf: '0'
    noise_voltage: '0.1'
    seed: '5'
    taps: 1.0 + 1.0j
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [723, 661]
    rotation: 0
    state: true
- name: channels_channel_model_0_0_0_0_0_0_0
  id: channels_channel_model
  parameters:
    affinity: ''
    alias: ''
    block_tags: 'False'
    comment: ''
    epsilon: '1.0'
    freq_offset: '0.001'
    maxoutbuf: '0'
    minoutbuf: '0'
    noise_voltage: '0.1'
    seed: '6'
    taps: 1.0 + 1.0j
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [725, 790]
    rotation: 0
    state: true
- name: channels_channel_model_0_1
  id: channels_channel_model
  parameters:
    affinity: ''
    alias: ''
    block_tags: 'False'
    comment: ''
    epsilon: '1.0'
    freq_offset: '0.001'
    maxoutbuf: '0'
    minoutbuf: '0'
    noise_voltage: '0.1'
    seed: '11'
    taps: 0.5 + 0.5j
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [1679, 468]
    rotation: 0
    state: true
- name: epy_block_1
  id: epy_block
  parameters:
    _source_code: "\"\"\"\nEmbedded Python Blocks:\n\nEach time this file is saved,\
      \ GRC will instantiate the first class it finds\nto get ports and parameters\
      \ of your block. The arguments to __init__  will\nbe the parameters. All of\
      \ them are required to have default values!\n\"\"\"\n\n\nimport numpy as np\n\
      import time\nimport schedule\nimport threading\nfrom datetime import datetime,\
      \ timedelta\nfrom timeslot import Timeslot\nfrom gnuradio import gr\n\nclass\
      \ blk(gr.sync_block):  # other base classes are basic_block, decim_block, interp_block\n\
      \    \"\"\"Embedded Python Block example - a simple multiply const\"\"\"\n \
      \   def __init__(self, example_param=1.0, poisson_lambda=5.0, interval = 32000):\
      \  #change to numpy array size of inputs. only default arguments here\n    \
      \    \"\"\"arguments to this function show up as parameters in GRC\"\"\"\n \
      \       gr.sync_block.__init__(\n            self,\n            name='Channels\
      \ emulator',   # will show up in GRC\n            in_sig=[np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64],\n\
      \            out_sig=[np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64]\n\
      \        )\n        # if an attribute with the same name as a parameter is found,\n\
      \        # a callback is registered (properties work, too).\n        self.example_param\
      \ = example_param\n        self.poisson_lambda = poisson_lambda\n        stop_run_continuously\
      \ = self.run_continuously() # enable therding for scheduler\n        self.interval\
      \ = interval\n        self.active = [False,False,False,False,False,False,False]\n\
      \        self.channels = [0,0,0,0,0,0,0]#cahnge to size of inputs\n        for\
      \ j in range(0,len(self.channels)): #cahnge to synchronus\n        \tself.channels[j]\
      \ = datetime.now()\n\t\n    def work(self, input_items, output_items):\n   \
      \     \"\"\"example: multiply with constant\"\"\"\n        for i in range(0,np.size(input_items,0)):\n\
      \        \tif datetime.now() - self.channels[i] > (i+1)*timedelta(days=0, seconds=1,\
      \ microseconds=0, milliseconds=0, minutes=0, hours=0, weeks=0):  #if time passed\n\
      \        \t\t#print(self.active)\n        \t\tif self.active[i] == True:\n \
      \       \t\t\tself.active[i] = False\n        \t\telse: \n\t        \t\tself.active[i]\
      \ = True\n        \t\tself.channels[i] = datetime.now() \n        \tif self.active[i]\
      \ == True:\n        \t\tself.turnon(i,input_items, output_items)\n        \t\
      else:\n        \t\tself.turnoff(i,input_items, output_items)\n        \t\t\n\
      \        \t\t\t\n        \t\n        return len(output_items[0])\n\t\n    def\
      \ turnon(self,i, input_items, output_items):\n    \toutput_items[i][:] = input_items[i]\
      \ \n    \t\n    def turnoff(self,i, input_items, output_items):\n    \toutput_items[i][:]\
      \ = input_items[i] * 0\n    \t\n    def run_continuously(interval=1):\n    \t\
      cease_continuous_run = threading.Event()\n\n    \tclass ScheduleThread(threading.Thread):\n\
      \    \t    def run(cls):\n\t            while not cease_continuous_run.is_set():\n\
      \                        schedule.run_pending()\n                        time.sleep(interval)\n\
      \    \tcontinuous_thread = ScheduleThread()\n    \tcontinuous_thread.start()\n\
      \    \treturn cease_continuous_run\n"
    affinity: ''
    alias: ''
    comment: ''
    example_param: '1.0'
    interval: samp_rate
    maxoutbuf: '0'
    minoutbuf: '0'
    poisson_lambda: '1.0'
  states:
    _io_cache: ('Channels emulator', 'blk', [('example_param', '1.0'), ('poisson_lambda',
      '5.0'), ('interval', '32000')], [('0', 'complex', 1), ('1', 'complex', 1), ('2',
      'complex', 1), ('3', 'complex', 1), ('4', 'complex', 1), ('5', 'complex', 1),
      ('6', 'complex', 1)], [('0', 'complex', 1), ('1', 'complex', 1), ('2', 'complex',
      1), ('3', 'complex', 1), ('4', 'complex', 1), ('5', 'complex', 1), ('6', 'complex',
      1)], 'Embedded Python Block example - a simple multiply const', ['example_param',
      'interval', 'poisson_lambda'])
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [477, 315]
    rotation: 0
    state: disabled
- name: epy_block_1_0
  id: epy_block
  parameters:
    _source_code: "\"\"\"\nEmbedded Python Blocks:\n\nEach time this file is saved,\
      \ GRC will instantiate the first class it finds\nto get ports and parameters\
      \ of your block. The arguments to __init__  will\nbe the parameters. All of\
      \ them are required to have default values!\n\"\"\"\n\n\nimport numpy as np\n\
      import time\nimport schedule\nimport threading\nfrom datetime import datetime,\
      \ timedelta\nfrom timeslot import Timeslot\nfrom gnuradio import gr\n\nclass\
      \ blk(gr.sync_block):  # other base classes are basic_block, decim_block, interp_block\n\
      \    \"\"\"Embedded Python Block example - a simple multiply const\"\"\"\n \
      \   def __init__(self, example_param=1.0, poisson_lambda=5.0, interval = 32000):\
      \  #change to numpy array size of inputs. only default arguments here\n    \
      \    \"\"\"arguments to this function show up as parameters in GRC\"\"\"\n \
      \       gr.sync_block.__init__(\n            self,\n            name='Channels\
      \ emulator',   # will show up in GRC\n            in_sig=[np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64],\n\
      \            out_sig=[np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64]\n\
      \        )\n        # if an attribute with the same name as a parameter is found,\n\
      \        # a callback is registered (properties work, too).\n        self.example_param\
      \ = example_param\n        self.poisson_lambda = poisson_lambda\n        stop_run_continuously\
      \ = self.run_continuously() # enable therding for scheduler\n        self.interval\
      \ = interval\n        self.channels = [0,0,0,0,0,0,0]#cahnge to size of inputs\n\
      \        for j in range(0,len(self.channels)): #cahnge to synchronus\n     \
      \   \tself.channels[j] = datetime.now()\n\t\n    def work(self, input_items,\
      \ output_items):\n        \"\"\"example: multiply with constant\"\"\"\n    \
      \    for i in range(0,np.size(input_items,0)):\n        \tactive = np.random.poisson(self.poisson_lambda,\
      \ 1) > 5.0\n        \tif datetime.now() - self.channels[i] > timedelta(days=0,\
      \ seconds=1, microseconds=self.interval, milliseconds=0, minutes=0, hours=0,\
      \ weeks=0):\n        \t\tself.turnoff(i,input_items, output_items)\n       \
      \ \t\tif active == True:\n        \t\t\tself.channels[i] = datetime.now()\n\
      \        \telse:\n        \t\tself.turnon(i,input_items, output_items)\n   \
      \     return len(output_items[0])\n\t\n    def turnon(self,i, input_items, output_items):\n\
      \    \toutput_items[i][:] = input_items[i] * self.example_param\n    \t\n  \
      \  def turnoff(self,i, input_items, output_items):\n    \toutput_items[i][:]\
      \ = input_items[i] * 0\n    \t\n    def run_continuously(interval=1):\n    \t\
      cease_continuous_run = threading.Event()\n\n    \tclass ScheduleThread(threading.Thread):\n\
      \    \t    def run(cls):\n\t            while not cease_continuous_run.is_set():\n\
      \                        schedule.run_pending()\n                        time.sleep(interval)\n\
      \    \tcontinuous_thread = ScheduleThread()\n    \tcontinuous_thread.start()\n\
      \    \treturn cease_continuous_run\n"
    affinity: ''
    alias: ''
    comment: ''
    example_param: '1.0'
    interval: samp_rate
    maxoutbuf: '0'
    minoutbuf: '0'
    poisson_lambda: '1.0'
  states:
    _io_cache: ('Channels emulator', 'blk', [('example_param', '1.0'), ('poisson_lambda',
      '5.0'), ('interval', '32000')], [('0', 'complex', 1), ('1', 'complex', 1), ('2',
      'complex', 1), ('3', 'complex', 1), ('4', 'complex', 1), ('5', 'complex', 1),
      ('6', 'complex', 1)], [('0', 'complex', 1), ('1', 'complex', 1), ('2', 'complex',
      1), ('3', 'complex', 1), ('4', 'complex', 1), ('5', 'complex', 1), ('6', 'complex',
      1)], 'Embedded Python Block example - a simple multiply const', ['example_param',
      'interval', 'poisson_lambda'])
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [478, 608]
    rotation: 0
    state: disabled
- name: epy_block_1_1
  id: epy_block
  parameters:
    _source_code: "\"\"\"\nEmbedded Python Blocks:\n\nEach time this file is saved,\
      \ GRC will instantiate the first class it finds\nto get ports and parameters\
      \ of your block. The arguments to __init__  will\nbe the parameters. All of\
      \ them are required to have default values!\n\"\"\"\n\n\nimport numpy as np\n\
      import time\nimport schedule\nimport threading\nfrom datetime import datetime,\
      \ timedelta\nfrom timeslot import Timeslot\nfrom gnuradio import gr\n\nclass\
      \ blk(gr.sync_block):  # other base classes are basic_block, decim_block, interp_block\n\
      \    \"\"\"Embedded Python Block example - a simple multiply const\"\"\"\n \
      \   def __init__(self, example_param=1.0, poisson_lambda=5.0, interval = 32000):\
      \  #change to numpy array size of inputs. only default arguments here\n    \
      \    \"\"\"arguments to this function show up as parameters in GRC\"\"\"\n \
      \       gr.sync_block.__init__(\n            self,\n            name='Channels\
      \ emulator',   # will show up in GRC\n            in_sig=[np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64],\n\
      \            out_sig=[np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64]\n\
      \        )\n        # if an attribute with the same name as a parameter is found,\n\
      \        # a callback is registered (properties work, too).\n        self.mus\
      \ = []\n        self.example_param = example_param\n        self.poisson_lambda\
      \ = poisson_lambda\n        stop_run_continuously = self.run_continuously()\
      \ # enable therding for scheduler\n        self.interval = interval\n      \
      \  self.active = [False,False,False,False,False,False,False]\n        self.channels\
      \ = [0,0,0,0,0,0,0]#cahnge to size of inputs\n        for j in range(0,len(self.channels)):\
      \ #cahnge to synchronus\n        \tself.channels[j] = datetime.now()\n     \
      \   \tself.mus = np.append(self.mus, 1+int(np.random.exponential(scale = j)))\n\
      \        print (self.mus)        \t\n        \n    def work(self, input_items,\
      \ output_items):\n        \"\"\"example: multiply with constant\"\"\"\n    \
      \    for i in range(0,np.size(input_items,0)):\n        \t\n        \tif datetime.now()\
      \ - self.channels[i] > timedelta(days=0, seconds=self.mus[i], microseconds=0,\
      \ milliseconds=0, minutes=0, hours=0, weeks=0):  #if time passed\n        \t\
      \t#print(self.active)\n        \t\tif self.active[i] == True:\n        \t\t\t\
      if 1 + np.random.exponential(scale = i) > i:\n        \t\t\t\tself.active[i]\
      \ = False\n        \t\telse: \n\t        \t\tself.active[i] = True\n       \
      \ \t\tself.channels[i] = datetime.now() \n        \tif self.active[i] == True:\n\
      \        \t\tself.turnon(i,input_items, output_items)\n        \telse:\n   \
      \     \t\tself.turnoff(i,input_items, output_items)\n        \t\t\n        \t\
      \t\t\n        \t\n        return len(output_items[0])\n\t\n    def turnon(self,i,\
      \ input_items, output_items):\n    \toutput_items[i][:] = input_items[i] \n\
      \    \t\n    def turnoff(self,i, input_items, output_items):\n    \toutput_items[i][:]\
      \ = input_items[i] * 0\n    \t\n    def run_continuously(interval=1):\n    \t\
      cease_continuous_run = threading.Event()\n\n    \tclass ScheduleThread(threading.Thread):\n\
      \    \t    def run(cls):\n\t            while not cease_continuous_run.is_set():\n\
      \                        schedule.run_pending()\n                        time.sleep(interval)\n\
      \    \tcontinuous_thread = ScheduleThread()\n    \tcontinuous_thread.start()\n\
      \    \treturn cease_continuous_run\n"
    affinity: ''
    alias: ''
    comment: ''
    example_param: '1.0'
    interval: samp_rate
    maxoutbuf: '0'
    minoutbuf: '0'
    poisson_lambda: '1.0'
  states:
    _io_cache: ('Channels emulator', 'blk', [('example_param', '1.0'), ('poisson_lambda',
      '5.0'), ('interval', '32000')], [('0', 'complex', 1), ('1', 'complex', 1), ('2',
      'complex', 1), ('3', 'complex', 1), ('4', 'complex', 1), ('5', 'complex', 1),
      ('6', 'complex', 1)], [('0', 'complex', 1), ('1', 'complex', 1), ('2', 'complex',
      1), ('3', 'complex', 1), ('4', 'complex', 1), ('5', 'complex', 1), ('6', 'complex',
      1)], 'Embedded Python Block example - a simple multiply const', ['example_param',
      'interval', 'poisson_lambda'])
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [476, 23]
    rotation: 0
    state: enabled
- name: epy_block_3_0
  id: epy_block
  parameters:
    _source_code: "\"\"\"\nEmbedded Python Blocks:\n\nEach time this file is saved,\
      \ GRC will instantiate the first class it finds\nto get ports and parameters\
      \ of your block. The arguments to __init__  will\nbe the parameters. All of\
      \ them are required to have default values!\n\"\"\"\n\nimport os\nimport copy\n\
      import sys      \nimport time\nimport schedule\nimport threading\nfrom datetime\
      \ import datetime, timedelta\nfrom timeslot import Timeslot  \nimport numpy\
      \ as np\nimport matplotlib.pyplot as plt\nimport torch\nimport tensorflow as\
      \ tf\nfrom gnuradio import gr\nimport pmt\nfrom tensorflow.keras import Model\n\
      from tensorflow.keras.layers import Layer, Dense, Softmax\nfrom tensorflow.keras.layers\
      \ import Dense\n\nfrom gym import Env\nfrom gym.spaces import Discrete, Box\n\
      \n\nfrom matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg\nfrom matplotlib.figure\
      \ import Figure\nfrom PyQt5 import QtCore #conda install pyqt\nfrom PyQt5 import\
      \ QtWidgets\nHISTORY_BUFFER_LEN = 2000\n\nclass blk(gr.sync_block):  # other\
      \ base classes are basic_block, decim_block, interp_block\n    \"\"\"Embedded\
      \ Python Block example - a simple multiply const\"\"\"\n\n    def __init__(self,number_of_channels=7,seed\
      \ = 0, num_GPU=0.0, active_threshold= 0.9, slot_time = 1, window_size = 5, initial_channel\
      \ = 0, gamma = 1, learning_rate = 0.01, epsilon = 1):  # only default arguments\
      \ here\n        \"\"\"arguments to this function show up as parameters in GRC\"\
      \"\"\n        gr.sync_block.__init__(\n            self,\n            name='RL\
      \ Channels select self agent',   # will show up in GRC\n            in_sig=[np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64],\n\
      \            out_sig=[np.uint16]\n        )\n        # if an attribute with\
      \ the same name as a parameter is found,\n        # a callback is registered\
      \ (properties work, too).\n        \n        #initailize tensorflow and GPU\
      \ usage\n        print('Tensorflow version: ', tf.__version__)\n        gpus\
      \ = tf.config.experimental.list_physical_devices(\"GPU\")\n        print('Number\
      \ of GPUs available :', len(gpus))\n        \n        if num_GPU < len(gpus):\n\
      \        \ttf.config.experimental.set_visible_devices(gpus[num_GPU], 'GPU')\n\
      \        \ttf.config.experimental.set_memory_growth(gpus[num_GPU], True)\n \
      \       \tprint('Only GPU number', num_GPU, 'used')\n        tf.random.set_seed(seed)\n\
      \t\n        #initialize Channel sensing\n        self.active_threshold = active_threshold\n\
      \        self.number_of_channels = number_of_channels\n        if (self.number_of_channels+1)%2!=0:\n\
      \        \tprint('/!\\ Please enter an even number of channels')\n        self.channels\
      \ = np.zeros(self.number_of_channels)\n        self.slot_time = slot_time\n\
      \        self.last_datetime = datetime.now()\n        self.window_size = window_size\n\
      \        self.grid = np.zeros((self.window_size, self.number_of_channels))\n\
      \        self.initial_channel = initial_channel #change to random\n        self.grid_flag\
      \ = 0\n        self.channel_decision = self.initial_channel\n        \n    \
      \    self.collision = 0\n        self.total_error = 0 \n        self.total_count\
      \ = 1 \n        #initialize DQN env\n        #self.dq_env = self.DQEnv(self.number_of_channels,\
      \ self.window_size, False, self.grid.T)\n\n        #initialize DQN Neural Network\n\
      \        self.gamma = gamma\n        self.learning_rate = learning_rate\n  \
      \      self.actions_number = self.number_of_channels\n        self.epsilon =\
      \ epsilon\n        self.nb_trainings = 300\n        self.loss = []  # Keep trak\
      \ of the losses\n        self.channel_decision = self.initial_channel\n    \
      \    self.train_flag = False\n        self.done = False\n        \n        self.agent_dq\
      \ = self.DQAgent(self.number_of_channels, self.learning_rate, self.gamma)\n\
      \        #sensed_ch, curr_ch, end = self.dq_env.get_init_state()\n        #print('##################################')\n\
      \        #print('ENV initial state:')\n        #print('State: ', sensed_ch,',\
      \ Current Channel: ', curr_ch)\n        #print('##################################')\n\
      \      \n\n    def work(self, input_items, output_items):       \t\n       \
      \ #Check slot time\n        if (datetime.now() - self.last_datetime) > timedelta(days=0,\
      \ seconds=self.slot_time, microseconds=0, milliseconds=0, minutes=0, hours=0,\
      \ weeks=0):\n            #Sample and mark channels activity\n            for\
      \ i in range(0,np.size(input_items,0)):\n            \t#Energy detected during\
      \ time slot, mark as occupied \n            \tif (np.mean(abs(input_items[i][:]))>self.active_threshold):\n\
      \            \t\tself.channels[i] = 1\n            for j in range (np.size(self.channels)):\
      \ self.channels[j] = np.abs(self.channels[j]-1)#flip ones and zeros\n      \
      \      print(self.channels,'    ', datetime.now())   \n            #when training\
      \ is done, make a decision\n            if self.done == True:\n            \t\
      state = [self.channels.astype(np.float32), self.dq_env.one_hot(self.channel_decision,\
      \ self.number_of_channels), 0]\n            \tself.channel_decision = self.agent_dq.choose_action(state,\
      \ 0)# epsilon = 0 only expoiltation wehn not training\n            \tself.total_count\
      \ += 1\n            \tself.total_ratio = (self.total_error)/self.total_count\n\
      \            \tprint('CH: ',self.channels)\n            \ttemp = np.zeros(np.size(self.channels))\n\
      \            \ttemp[self.channel_decision]=1\n            \tprint('DC: ',temp)\n\
      \            \tprint('count: ', self.total_count, ' Errors: ', self.total_error,\
      \ ' ratio: ',self.total_ratio)\n         \t\n            \tif self.channels[self.channel_decision]\
      \ == 0:\n            \t\tself.collision += 1             \n            self.grid\
      \ = self.make_grid(self.grid,(self.channels))\n            self.last_datetime\
      \ = datetime.now()\n            self.channels = np.zeros(self.number_of_channels)\n\
      \            #print(self.grid)\n            \n            self.grid_flag +=\
      \ 1\n        output_items[0][:]= self.channel_decision      \n        if self.collision\
      \ == 1:\n        \tself.total_error += 1\n        \tself.collision = 0\n   \
      \     \t#self.grid_flag = self.window_size+1\n        \t\n\n        \t\n   \
      \     #Full window sized grid\n        if (self.grid_flag == self.window_size+1):\
      \ \n\t\t\n        \t#initialize DQN env\n        \tself.dq_env = self.DQEnv(self.number_of_channels,\
      \ self.window_size, self.initial_channel, self.grid.T)\n\t\n\t\t# Run 200 episodes\n\
      \        \tfor i in range(HISTORY_BUFFER_LEN):\n                    \n     \
      \               self.dq_env.initialize()\n                    \n           \
      \         state = self.dq_env.get_init_state()\n                    action =\
      \ self.agent_dq.choose_action(state, self.epsilon)\n                    reward,\
      \ new_state = self.dq_env.run(action)\n\t    \n                    self.agent_dq.insert_history(state,\
      \ action, reward, new_state) # Insert state, action, reward, new state in history\
      \ \n\t    \n                    # While it's not the end of the episode\n  \
      \                  while new_state[2]==0 :\n                    \tstate = new_state\n\
      \                    \taction = self.agent_dq.choose_action(state, self.epsilon)\n\
      \                    \treward, new_state = self.dq_env.run(action)\n\t\t\n \
      \                   \tself.agent_dq.insert_history(state, action, reward, new_state)\
      \ # Insert state, action, reward, new state in history\n        \t\n       \
      \ \tself.train_flag = True                      \t\n        \tprint('Replay\
      \ buffer History populated')\n        \t                 \t \n\n        \t\n\
      \        \t\n\n        if self.train_flag == True:\n\t        print('Training...')\n\
      \        \tself.train_flag = False\n        \t\n        \tfor i in range(self.nb_trainings):\n\
      \        \t\tself.dq_env.initialize()\n        \t\tstate = self.dq_env.get_init_state()\n\
      \    \n        \t\taction = self.agent_dq.choose_action(state, self.epsilon)\n\
      \        \t\treward, new_state = self.dq_env.run(action)\n        \t\tself.agent_dq.insert_history(state,\
      \ action, reward, new_state) # Insert state, action, reward, new state in history\
      \ \n        \t\tself.agent_dq.learn(64) # Each time we store a new history,\
      \ we perform a training on random data\n    \n        \t\t# While it's not the\
      \ end of the episode\n        \t\twhile new_state[2]==0 :\n        \t\t\tstate\
      \ = new_state\n        \t\t\taction = self.agent_dq.choose_action(state, self.epsilon)\n\
      \        \t\t\treward, new_state = self.dq_env.run(action)\n        \t\t\tself.agent_dq.insert_history(state,\
      \ action, reward, new_state) # Insert state, action, reward, new state in history\
      \ \n        \t\t\tself.agent_dq.learn(64) # Each time we store a new history,\
      \ we perform a training on random data\n    \n        \t\tself.loss.append(tf.reduce_mean(self.agent_dq.loss).numpy())\
      \ #Save the losses for future visualization\n        \t\t\t#Every 10 iterations\
      \ we copy the parameters of the online DQN to the offline DQN\n        \t\t\
      if (i+1)%10 == 0:\n        \t\t\tself.agent_dq.copy_parameters()\n        \t\
      \t\tprint((i+1), end=', ')\n        \tself.done = True\n        \t#plt.semilogy(np.arange(len(self.loss)),\
      \ self.loss)\n        \t#plt.show()\n        \tself.evaluate_dq_agent(self.agent_dq\
      \ , self.dq_env.grid)\n        return len(output_items[0])\n        \n\n   \
      \ \n    \n    class DQEnv():\n    \tdef __init__(self, nb_channels = 7, nb_states=5,\
      \ initial_state = 0 , grid = []):\n        \n\t    \tif (nb_channels+1)%2!=0:\n\
      \t    \t\tprint('/!\\ Please enter an even number of channels')\n          \
      \  \n\t    \tself.nb_ch = nb_channels\n\t    \tself.nb_states = nb_states\n\
      \        \n\t    \tself.grid = grid\n\t    \tprint ('env grid:')\n\t    \tprint(self.grid)\n\
      \t    \tself.init_state = [int(initial_state), int(self.nb_states-1)]\n\t  \
      \  \tself.ch_state = self.init_state[1]\n\t    \tself.curr_ch = self.init_state[0]\n\
      \t    \tself.sent_mess = 0\n        \n\t    \tprint('This environment has '\
      \ + str(self.nb_ch) +' different channels')\n        \n    \tdef run(self, action):\n\
      \        \n\t    \tself.sent_mess += 1\n        \n\t    \tself.ch_state  = (self.ch_state\
      \ + 1)%self.nb_states\n        \n\t    \tself.curr_ch = action\n\t    \t\n\t\
      \    \treward = np.abs(self.grid[self.curr_ch, self.ch_state])\n        \n\t\
      \    \tif self.sent_mess != self.nb_states: \n\t    \t\tend = 0\n\t    \telse\
      \ :\n\t    \t\tend = 1\n        \n\t    \treturn(reward, [self.grid[:, self.ch_state],\
      \ self.one_hot(self.curr_ch, self.nb_ch), end])\n    \n    \tdef get_init_state(self):\n\
      \t    \treturn [self.grid[:, self.init_state[1]], self.one_hot(self.init_state[0],\
      \ self.nb_ch), 0]\n    \n    \tdef initialize(self):\n\t    \tself.ch_state\
      \ = self.init_state[1]\n\t    \tself.curr_ch = self.init_state[0]\n\t    \t\
      self.sent_mess = 0\n        \n    \tdef one_hot(self, index, depth):\n\t   \
      \ \toh = np.zeros(depth, dtype=np.float32)\n\t    \toh[index] = 1\n\t    \t\
      return oh\n    \n    \n    class DQAgent():\n    \tdef __init__(self, nb_channels,\
      \ learning_rate, gamma):\n    \t\tself.nb_ch = nb_channels\n    \t\tself.nb_actions\
      \ = nb_channels\n    \t\tself.gamma = gamma\n    \t\tself.learning_rate = learning_rate\n\
      \    \t\t\n    \t\tself.history_length = HISTORY_BUFFER_LEN\n    \t\tself.history\
      \ = [[]for i in range(self.history_length)]\n    \t\tself.history_idx = 0\n\
      \    \t\t\n    \t\t#Create and initialize the online DQN\n    \t\tself.DQN_online\
      \ = tf.keras.models.Sequential([Dense(2*self.nb_ch, activation='relu'), Dense(self.nb_actions,\
      \ activation='softplus') #Outputs positive values \n    \t\t])\n    \t\tself.DQN_online.build(input_shape=(None,\
      \ self.nb_ch*2)) #Build the model to create the weights\n    \t\t\n    \t\t\
      #Create and initialize the offline DQN\n    \t\tself.DQN_offline = tf.keras.models.Sequential([Dense(2*self.nb_ch,\
      \ activation='relu'), Dense(self.nb_actions, activation='softplus') #Outputs\
      \ positive values\n    \t\t])\n    \t\tself.DQN_offline.build(input_shape=(None,\
      \ self.nb_ch*2)) #Build the model to create the weights\n    \t\t\n    \t\t\
      self.copy_parameters() #Copy the weights of the online network to the offline\
      \ network\n    \t\t\n    \t\tself.loss_func = tf.keras.losses.MSE\n    \t\t\
      self.optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n        \n  \
      \      \n    \tdef choose_action(self, state, epsilon):\n    \t\t\"\"\"Implements\
      \ an epsilon-greedy policy\"\"\"\n    \t\tsensed_ch, curr_ch, end = state\n\
      \    \t\t# Explore ?\n    \t\tif np.random.uniform(size=1) < epsilon :\n   \
      \ \t\t\taction =  np.random.randint(low = 0, high = np.size(sensed_ch)) #######\
      \ Take one action randomly {0, 1, 2, 3, 4, 5, 6} with probability epsilon\n\
      \    \t\t\t\n        \n    \t\t#Choose the best action\n    \t\telse:    \n\
      \    \t\t\t#sensed_ch, curr_ch, end = state #Decomposes the state\n    \t\t\t\
      '''\n    \t\t\t* The shapes are :\n    \t\t\t- curr_ch : [self.nb_ch]\n    \t\
      \t\t- sensed_ch : [self.nb_ch]\n    \t\t\t- end : one integer\n    \t\t\t'''\n\
      \    \t\t\tDQN_input = tf.concat([sensed_ch, curr_ch], axis=0)[tf.newaxis, :]\
      \ #Create a state vector, which is the DQN input. Shape : [1, 2*self.nb_ch]\n\
      \    \t\t\toutputs = self.DQN_online(DQN_input).numpy() #Get the predicted Q\
      \ values corresponding to the 3 actions\n    \t\t\taction = np.argmax(outputs)#-1\
      \ #Take the action that has the highest predicted Q value (0, 1, 2, 3, 4, 5,\
      \ 6)\t\t\t\n    \t\t\t#print(outputs)\n    \t\treturn action\n    \n    \tdef\
      \ learn(self, batch_size):\n    \t\t\"\"\"Sample experiences from the history\
      \ and performs SGD\"\"\"\n    \t\t\n    \t\t# Samples random experiences from\
      \ the history\n    \t\tidx = np.random.choice(range(self.history_length), batch_size,\
      \ replace=False) # Create random indexes \n    \t\trdm_exp =  [self.history[i]\
      \ for i in idx] # Take experiences corresponding to the random indexes\n   \
      \ \t\t\n    \t\t# Each experience is written in this format : [state_vec, end,\
      \ action, reward, n_state_vec, n_end] (see insert_history method)\n    \t\t\n\
      \    \t\t# Create 6 batches : states_vec, end_boolean, actions, rewards, new\
      \ states_vec, new_end_boolean\n    \t\tstates_vec = np.array([rdm_exp[i][0]\
      \ for i in range(batch_size)]) # Shape : [Bs, 2*self.nb_ch]\n    \t\tend = np.array([rdm_exp[i][1]\
      \ for i in range(batch_size)]) # Shape : [Bs]\n    \t\tactions = np.array([rdm_exp[i][2]\
      \ for i in range(batch_size)]) # Shape : [BS]\n    \t\trewards = np.array([rdm_exp[i][3]\
      \ for i in range(batch_size)]) # Shape : [BS]\n    \t\tn_states_vec = np.array([rdm_exp[i][4]\
      \ for i in range(batch_size)]) # Shape : [Bs, 2*self.nb_ch]\n    \t\tn_end =\
      \ np.array([rdm_exp[i][5] for i in range(batch_size)]) # Shape : [BS]\n    \t\
      \t\n    \t\t#Compute the best q_value for the new states\n    \t\tmax_n_q_values\
      \ = tf.reduce_max(self.DQN_offline(n_states_vec), axis=1).numpy()\n\n    \t\t\
      with tf.GradientTape() as tape:\n    \t\t\t#Forward pass through the online\
      \ network to predict the q_values\n    \t\t\tpred_q_values = self.DQN_online(states_vec)\n\
      \    \t\t\t\n    \t\t\t# Compute targets\n    \t\t\ttargets = pred_q_values.numpy()\n\
      \    \t\t\ttargets[np.arange(targets.shape[0]), actions]= rewards + (1-n_end)\
      \ * self.gamma * max_n_q_values\n    \t\t\t\n    \t\t\t# Evaluate the loss\n\
      \    \t\t\tself.loss = self.loss_func(pred_q_values, targets)\n        \n  \
      \  \t\t# Compute gradients and perform the gradient descent\n    \t\tgradients\
      \ = tape.gradient(self.loss, self.DQN_online.trainable_weights)\n    \t\tself.optimizer.apply_gradients(zip(gradients,\
      \ self.DQN_online.trainable_weights))  \n    \n    \tdef insert_history(self,\
      \ state, action, reward, n_state):\n    \t\t\"\"\"Insert experience in history\"\
      \"\"\n        \n    \t\tsensed_ch, curr_ch, end = state\n    \t\tstate_vec =\
      \ np.concatenate([sensed_ch, curr_ch], axis=0) # Create the state vector for\
      \ the state\n        \n    \t\tn_sensed_ch, n_curr_ch, n_end = n_state\n   \
      \ \t\tn_state_vec = np.concatenate([n_sensed_ch, n_curr_ch], axis=0) # Create\
      \ the state vector for the new state\n\n    \t\tself.history[self.history_idx]\
      \ = [state_vec, end, action, reward, n_state_vec, n_end] # Insert everything\
      \ in the history\n        \n    \t\tself.history_idx = (self.history_idx+1)%self.history_length\
      \ # Move the history_idx by one\n    \n    \tdef copy_parameters(self):\n  \
      \  \t\t\"\"\"Copy the parameters of the online network to the offline network\"\
      \"\"\n\n    \t\tweights = self.DQN_online.get_weights()\n    \t\tself.DQN_offline.set_weights(weights)\n\
      \n\t\t\n\n    def make_grid(self,grid,channels):\n        grid = np.append(grid,\
      \ channels).reshape(self.window_size+1,7)\n        grid = np.delete(grid, 0,\
      \ 0)\n        grid = grid.reshape(self.window_size,self.number_of_channels)\n\
      \        return grid.astype(np.float32)\n    \n    def evaluate_dq_agent(self,\
      \ agent, grid):\n\n        action_history=[]\n        tot_reward = 0\n    \n\
      \        self.dq_env.initialize()\n        first_state = self.dq_env.get_init_state()\n\
      \        action = agent.choose_action(first_state, epsilon = 0)\n        action_history.append(action)\n\
      \        reward, new_state = self.dq_env.run(action)\n        tot_reward +=\
      \ reward\n    \n        for j in range(grid.shape[1]-1):\n            state\
      \ = new_state\n            action = agent.choose_action(state, epsilon = 0)\n\
      \            action_history.append(action)\n            reward, new_state =\
      \ self.dq_env.run(action)\n            tot_reward += reward\n            \n\
      \        choosen_channels = [(grid.shape[0]-1)/2]\n        for i in range(len(action_history)):\n\
      \        \tchoosen_channels.append((choosen_channels[i]+action_history[i])%grid.shape[0])\n\
      \        choosen_channels = choosen_channels[1:]\n        #plt.imshow(np.flip(grid,\
      \ axis=0), origin=\"lower\", cmap='gray', vmin=0, vmax=1)\n        #for i in\
      \ range(len(choosen_channels)):\n        #\tplt.scatter(i, grid.shape[0]-1-choosen_channels[i],\
      \ color='r')\n        #plt.show()\n        #print(str(int(tot_reward))+'/'+str(grid.shape[1])+'\
      \ packets have been transmitted')\n        return tot_reward\n"
    active_threshold: '0.9'
    affinity: ''
    alias: ''
    comment: ''
    epsilon: '0.25'
    gamma: '0.6'
    initial_channel: '0'
    learning_rate: '0.01'
    maxoutbuf: '0'
    minoutbuf: '0'
    num_GPU: '1'
    number_of_channels: '7'
    seed: '1'
    slot_time: '1'
    window_size: '128'
  states:
    _io_cache: ('RL Channels select self agent', 'blk', [('number_of_channels', '7'),
      ('seed', '0'), ('num_GPU', '0.0'), ('active_threshold', '0.9'), ('slot_time',
      '1'), ('window_size', '5'), ('initial_channel', '0'), ('gamma', '1'), ('learning_rate',
      '0.01'), ('epsilon', '1')], [('0', 'complex', 1), ('1', 'complex', 1), ('2',
      'complex', 1), ('3', 'complex', 1), ('4', 'complex', 1), ('5', 'complex', 1),
      ('6', 'complex', 1)], [('0', 'short', 1)], 'Embedded Python Block example -
      a simple multiply const', ['active_threshold', 'epsilon', 'gamma', 'initial_channel',
      'learning_rate', 'number_of_channels', 'slot_time', 'window_size'])
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [1041, 1261]
    rotation: 0
    state: disabled
- name: epy_block_3_0_0
  id: epy_block
  parameters:
    _source_code: "\"\"\"\nEmbedded Python Blocks:\n\nEach time this file is saved,\
      \ GRC will instantiate the first class it finds\nto get ports and parameters\
      \ of your block. The arguments to __init__  will\nbe the parameters. All of\
      \ them are required to have default values!\n\"\"\"\n\nimport os\nimport copy\n\
      import sys      \nimport time\nimport schedule\nimport threading\nfrom datetime\
      \ import datetime, timedelta\nfrom timeslot import Timeslot  \nimport numpy\
      \ as np\nimport matplotlib.pyplot as plt\nimport torch\nimport tensorflow as\
      \ tf\nfrom gnuradio import gr\nimport pmt\nfrom tensorflow.keras import Model\n\
      from tensorflow.keras.layers import Layer, Dense, Softmax\nfrom tensorflow.keras.layers\
      \ import Dense\n\nfrom gym import Env\nfrom gym.spaces import Discrete, Box\n\
      \n\nfrom matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg\nfrom matplotlib.figure\
      \ import Figure\nfrom PyQt5 import QtCore #conda install pyqt\nfrom PyQt5 import\
      \ QtWidgets\nHISTORY_BUFFER_LEN = 2000\n\nclass blk(gr.sync_block):  # other\
      \ base classes are basic_block, decim_block, interp_block\n    \"\"\"Embedded\
      \ Python Block example - a simple multiply const\"\"\"\n\n    def __init__(self,number_of_channels=7,seed\
      \ = 0, num_GPU=0.0, active_threshold= 0.9, slot_time = 1, window_size = 5, initial_channel\
      \ = 0, gamma = 1, learning_rate = 0.01, epsilon = 1):  # only default arguments\
      \ here\n        \"\"\"arguments to this function show up as parameters in GRC\"\
      \"\"\n        gr.sync_block.__init__(\n            self,\n            name='RL\
      \ Channels select self agent',   # will show up in GRC\n            in_sig=[np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64],\n\
      \            out_sig=[np.uint16]\n        )\n        # if an attribute with\
      \ the same name as a parameter is found,\n        # a callback is registered\
      \ (properties work, too).\n        \n        #initailize tensorflow and GPU\
      \ usage\n        print('Tensorflow version: ', tf.__version__)\n        gpus\
      \ = tf.config.experimental.list_physical_devices(\"GPU\")\n        print('Number\
      \ of GPUs available :', len(gpus))\n        \n        if num_GPU < len(gpus):\n\
      \        \ttf.config.experimental.set_visible_devices(gpus[num_GPU], 'GPU')\n\
      \        \ttf.config.experimental.set_memory_growth(gpus[num_GPU], True)\n \
      \       \tprint('Only GPU number', num_GPU, 'used')\n        tf.random.set_seed(seed)\n\
      \t\n        #initialize Channel sensing\n        self.active_threshold = active_threshold\n\
      \        self.number_of_channels = number_of_channels\n        if (self.number_of_channels+1)%2!=0:\n\
      \        \tprint('/!\\ Please enter an even number of channels')\n        self.channels\
      \ = np.zeros(self.number_of_channels)\n        self.slot_time = slot_time\n\
      \        self.last_datetime = datetime.now()\n        self.window_size = window_size\n\
      \        self.grid = np.zeros((self.window_size, self.number_of_channels))\n\
      \        self.initial_channel = initial_channel #change to random\n        self.grid_flag\
      \ = 0\n        self.channel_decision = self.initial_channel\n        \n    \
      \    self.collision = 0\n        self.total_error = 0 \n        self.total_count\
      \ = 1 \n        #initialize DQN env\n        #self.dq_env = self.DQEnv(self.number_of_channels,\
      \ self.window_size, False, self.grid.T)\n\n        #initialize DQN Neural Network\n\
      \        self.gamma = gamma\n        self.learning_rate = learning_rate\n  \
      \      self.actions_number = self.number_of_channels\n        self.epsilon =\
      \ epsilon\n        self.nb_trainings = 300\n        self.loss = []  # Keep trak\
      \ of the losses\n        self.channel_decision = self.initial_channel\n    \
      \    self.train_flag = False\n        self.done = False\n        \n        self.agent_dq\
      \ = self.DQAgent(self.number_of_channels, self.learning_rate, self.gamma)\n\
      \        #sensed_ch, curr_ch, end = self.dq_env.get_init_state()\n        #print('##################################')\n\
      \        #print('ENV initial state:')\n        #print('State: ', sensed_ch,',\
      \ Current Channel: ', curr_ch)\n        #print('##################################')\n\
      \      \n\n    def work(self, input_items, output_items):       \t\n       \
      \ #Check slot time\n        if (datetime.now() - self.last_datetime) > timedelta(days=0,\
      \ seconds=self.slot_time, microseconds=0, milliseconds=0, minutes=0, hours=0,\
      \ weeks=0):\n            #Sample and mark channels activity\n            for\
      \ i in range(0,np.size(input_items,0)):\n            \t#Energy detected during\
      \ time slot, mark as occupied \n            \tif (np.mean(abs(input_items[i][:]))>self.active_threshold):\n\
      \            \t\tself.channels[i] = 1\n            for j in range (np.size(self.channels)):\
      \ self.channels[j] = np.abs(self.channels[j]-1)#flip ones and zeros\n      \
      \      print(self.channels,'    ', datetime.now())   \n            #when training\
      \ is done, make a decision\n            if self.done == True:\n            \t\
      state = [self.channels.astype(np.float32), self.dq_env.one_hot(self.channel_decision,\
      \ self.number_of_channels), 0]\n            \tself.channel_decision = self.agent_dq.choose_action(state,\
      \ 0)# epsilon = 0 only expoiltation wehn not training\n            \tself.total_count\
      \ += 1\n            \tself.total_ratio = (self.total_error)/self.total_count\n\
      \            \tprint('CH: ',self.channels)\n            \ttemp = np.zeros(np.size(self.channels))\n\
      \            \ttemp[self.channel_decision]=1\n            \tprint('DC: ',temp)\n\
      \            \tprint('count: ', self.total_count, ' Errors: ', self.total_error,\
      \ ' ratio: ',self.total_ratio)\n         \t\n            \tif self.channels[self.channel_decision]\
      \ == 0:\n            \t\tself.collision += 1             \n            self.grid\
      \ = self.make_grid(self.grid,(self.channels))\n            self.last_datetime\
      \ = datetime.now()\n            self.channels = np.zeros(self.number_of_channels)\n\
      \            #print(self.grid)\n            \n            self.grid_flag +=\
      \ 1\n        output_items[0][:]= self.channel_decision      \n        if self.collision\
      \ == 1:\n        \tself.total_error += 1\n        \tself.collision = 0\n   \
      \     \t#self.grid_flag = self.window_size+1\n        \t\n\n        \t\n   \
      \     #Full window sized grid\n        if (self.grid_flag == self.window_size+1):\
      \ \n\t\t\n        \t#initialize DQN env\n        \tself.dq_env = self.DQEnv(self.number_of_channels,\
      \ self.window_size, self.initial_channel, self.grid.T)\n\t\n\t\t# Run 200 episodes\n\
      \        \tfor i in range(HISTORY_BUFFER_LEN):\n                    \n     \
      \               self.dq_env.initialize()\n                    \n           \
      \         state = self.dq_env.get_init_state()\n                    action =\
      \ self.agent_dq.choose_action(state, self.epsilon)\n                    reward,\
      \ new_state = self.dq_env.run(action)\n\t    \n                    self.agent_dq.insert_history(state,\
      \ action, reward, new_state) # Insert state, action, reward, new state in history\
      \ \n\t    \n                    # While it's not the end of the episode\n  \
      \                  while new_state[2]==0 :\n                    \tstate = new_state\n\
      \                    \taction = self.agent_dq.choose_action(state, self.epsilon)\n\
      \                    \treward, new_state = self.dq_env.run(action)\n\t\t\n \
      \                   \tself.agent_dq.insert_history(state, action, reward, new_state)\
      \ # Insert state, action, reward, new state in history\n        \t\n       \
      \ \tself.train_flag = True                      \t\n        \tprint('Replay\
      \ buffer History populated')\n        \t                 \t \n\n        \t\n\
      \        \t\n\n        if self.train_flag == True:\n\t        print('Training...')\n\
      \        \tself.train_flag = False\n        \t\n        \tfor i in range(self.nb_trainings):\n\
      \        \t\tself.dq_env.initialize()\n        \t\tstate = self.dq_env.get_init_state()\n\
      \    \n        \t\taction = self.agent_dq.choose_action(state, self.epsilon)\n\
      \        \t\treward, new_state = self.dq_env.run(action)\n        \t\tself.agent_dq.insert_history(state,\
      \ action, reward, new_state) # Insert state, action, reward, new state in history\
      \ \n        \t\tself.agent_dq.learn(64) # Each time we store a new history,\
      \ we perform a training on random data\n    \n        \t\t# While it's not the\
      \ end of the episode\n        \t\twhile new_state[2]==0 :\n        \t\t\tstate\
      \ = new_state\n        \t\t\taction = self.agent_dq.choose_action(state, self.epsilon)\n\
      \        \t\t\treward, new_state = self.dq_env.run(action)\n        \t\t\tself.agent_dq.insert_history(state,\
      \ action, reward, new_state) # Insert state, action, reward, new state in history\
      \ \n        \t\t\tself.agent_dq.learn(64) # Each time we store a new history,\
      \ we perform a training on random data\n    \n        \t\tself.loss.append(tf.reduce_mean(self.agent_dq.loss).numpy())\
      \ #Save the losses for future visualization\n        \t\t\t#Every 10 iterations\
      \ we copy the parameters of the online DQN to the offline DQN\n        \t\t\
      if (i+1)%10 == 0:\n        \t\t\tself.agent_dq.copy_parameters()\n        \t\
      \t\tprint((i+1), end=', ')\n        \tself.done = True\n        \t#plt.semilogy(np.arange(len(self.loss)),\
      \ self.loss)\n        \t#plt.show()\n        \tself.evaluate_dq_agent(self.agent_dq\
      \ , self.dq_env.grid)\n        return len(output_items[0])\n        \n\n   \
      \ \n    \n    class DQEnv():\n    \tdef __init__(self, nb_channels = 7, nb_states=5,\
      \ initial_state = 0 , grid = []):\n        \n\t    \tif (nb_channels+1)%2!=0:\n\
      \t    \t\tprint('/!\\ Please enter an even number of channels')\n          \
      \  \n\t    \tself.nb_ch = nb_channels\n\t    \tself.nb_states = nb_states\n\
      \        \n\t    \tself.grid = grid\n\t    \tprint ('env grid:')\n\t    \tprint(self.grid)\n\
      \t    \tself.init_state = [int(initial_state), int(self.nb_states-1)]\n\t  \
      \  \tself.ch_state = self.init_state[1]\n\t    \tself.curr_ch = self.init_state[0]\n\
      \t    \tself.sent_mess = 0\n        \n\t    \tprint('This environment has '\
      \ + str(self.nb_ch) +' different channels')\n        \n    \tdef run(self, action):\n\
      \        \n\t    \tself.sent_mess += 1\n        \n\t    \tself.ch_state  = (self.ch_state\
      \ + 1)%self.nb_states\n        \n\t    \tself.curr_ch = action\n\t    \t\n\t\
      \    \treward = np.abs(self.grid[self.curr_ch, self.ch_state])\n        \n\t\
      \    \tif self.sent_mess != self.nb_states: \n\t    \t\tend = 0\n\t    \telse\
      \ :\n\t    \t\tend = 1\n        \n\t    \treturn(reward, [self.grid[:, self.ch_state],\
      \ self.one_hot(self.curr_ch, self.nb_ch), end])\n    \n    \tdef get_init_state(self):\n\
      \t    \treturn [self.grid[:, self.init_state[1]], self.one_hot(self.init_state[0],\
      \ self.nb_ch), 0]\n    \n    \tdef initialize(self):\n\t    \tself.ch_state\
      \ = self.init_state[1]\n\t    \tself.curr_ch = self.init_state[0]\n\t    \t\
      self.sent_mess = 0\n        \n    \tdef one_hot(self, index, depth):\n\t   \
      \ \toh = np.zeros(depth, dtype=np.float32)\n\t    \toh[index] = 1\n\t    \t\
      return oh\n    \n    \n    class DQAgent():\n    \tdef __init__(self, nb_channels,\
      \ learning_rate, gamma):\n    \t\tself.nb_ch = nb_channels\n    \t\tself.nb_actions\
      \ = nb_channels\n    \t\tself.gamma = gamma\n    \t\tself.learning_rate = learning_rate\n\
      \    \t\t\n    \t\tself.history_length = HISTORY_BUFFER_LEN\n    \t\tself.history\
      \ = [[]for i in range(self.history_length)]\n    \t\tself.history_idx = 0\n\
      \    \t\t\n    \t\t#Create and initialize the online DQN\n    \t\tself.DQN_online\
      \ = tf.keras.models.Sequential([Dense(2*self.nb_ch, activation='relu'), Dense(self.nb_actions,\
      \ activation='softplus') #Outputs positive values \n    \t\t])\n    \t\tself.DQN_online.build(input_shape=(None,\
      \ self.nb_ch*2)) #Build the model to create the weights\n    \t\t\n    \t\t\
      #Create and initialize the offline DQN\n    \t\tself.DQN_offline = tf.keras.models.Sequential([Dense(2*self.nb_ch,\
      \ activation='relu'), Dense(self.nb_actions, activation='softplus') #Outputs\
      \ positive values\n    \t\t])\n    \t\tself.DQN_offline.build(input_shape=(None,\
      \ self.nb_ch*2)) #Build the model to create the weights\n    \t\t\n    \t\t\
      self.copy_parameters() #Copy the weights of the online network to the offline\
      \ network\n    \t\t\n    \t\tself.loss_func = tf.keras.losses.MSE\n    \t\t\
      self.optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n        \n  \
      \      \n    \tdef choose_action(self, state, epsilon):\n    \t\t\"\"\"Implements\
      \ an epsilon-greedy policy\"\"\"\n    \t\tsensed_ch, curr_ch, end = state\n\
      \    \t\t# Explore ?\n    \t\tif np.random.uniform(size=1) < epsilon :\n   \
      \ \t\t\taction =  np.random.randint(low = 0, high = np.size(sensed_ch)) #######\
      \ Take one action randomly {0, 1, 2, 3, 4, 5, 6} with probability epsilon\n\
      \    \t\t\t\n        \n    \t\t#Choose the best action\n    \t\telse:    \n\
      \    \t\t\t#sensed_ch, curr_ch, end = state #Decomposes the state\n    \t\t\t\
      '''\n    \t\t\t* The shapes are :\n    \t\t\t- curr_ch : [self.nb_ch]\n    \t\
      \t\t- sensed_ch : [self.nb_ch]\n    \t\t\t- end : one integer\n    \t\t\t'''\n\
      \    \t\t\tDQN_input = tf.concat([sensed_ch, curr_ch], axis=0)[tf.newaxis, :]\
      \ #Create a state vector, which is the DQN input. Shape : [1, 2*self.nb_ch]\n\
      \    \t\t\toutputs = self.DQN_online(DQN_input).numpy() #Get the predicted Q\
      \ values corresponding to the 3 actions\n    \t\t\taction = np.argmax(outputs)#-1\
      \ #Take the action that has the highest predicted Q value (0, 1, 2, 3, 4, 5,\
      \ 6)\t\t\t\n    \t\t\t#print(outputs)\n    \t\treturn action\n    \n    \tdef\
      \ learn(self, batch_size):\n    \t\t\"\"\"Sample experiences from the history\
      \ and performs SGD\"\"\"\n    \t\t\n    \t\t# Samples random experiences from\
      \ the history\n    \t\tidx = np.random.choice(range(self.history_length), batch_size,\
      \ replace=False) # Create random indexes \n    \t\trdm_exp =  [self.history[i]\
      \ for i in idx] # Take experiences corresponding to the random indexes\n   \
      \ \t\t\n    \t\t# Each experience is written in this format : [state_vec, end,\
      \ action, reward, n_state_vec, n_end] (see insert_history method)\n    \t\t\n\
      \    \t\t# Create 6 batches : states_vec, end_boolean, actions, rewards, new\
      \ states_vec, new_end_boolean\n    \t\tstates_vec = np.array([rdm_exp[i][0]\
      \ for i in range(batch_size)]) # Shape : [Bs, 2*self.nb_ch]\n    \t\tend = np.array([rdm_exp[i][1]\
      \ for i in range(batch_size)]) # Shape : [Bs]\n    \t\tactions = np.array([rdm_exp[i][2]\
      \ for i in range(batch_size)]) # Shape : [BS]\n    \t\trewards = np.array([rdm_exp[i][3]\
      \ for i in range(batch_size)]) # Shape : [BS]\n    \t\tn_states_vec = np.array([rdm_exp[i][4]\
      \ for i in range(batch_size)]) # Shape : [Bs, 2*self.nb_ch]\n    \t\tn_end =\
      \ np.array([rdm_exp[i][5] for i in range(batch_size)]) # Shape : [BS]\n    \t\
      \t\n    \t\t#Compute the best q_value for the new states\n    \t\tmax_n_q_values\
      \ = tf.reduce_max(self.DQN_offline(n_states_vec), axis=1).numpy()\n\n    \t\t\
      with tf.GradientTape() as tape:\n    \t\t\t#Forward pass through the online\
      \ network to predict the q_values\n    \t\t\tpred_q_values = self.DQN_online(states_vec)\n\
      \    \t\t\t\n    \t\t\t# Compute targets\n    \t\t\ttargets = pred_q_values.numpy()\n\
      \    \t\t\ttargets[np.arange(targets.shape[0]), actions]= rewards + (1-n_end)\
      \ * self.gamma * max_n_q_values\n    \t\t\t\n    \t\t\t# Evaluate the loss\n\
      \    \t\t\tself.loss = self.loss_func(pred_q_values, targets)\n        \n  \
      \  \t\t# Compute gradients and perform the gradient descent\n    \t\tgradients\
      \ = tape.gradient(self.loss, self.DQN_online.trainable_weights)\n    \t\tself.optimizer.apply_gradients(zip(gradients,\
      \ self.DQN_online.trainable_weights))  \n    \n    \tdef insert_history(self,\
      \ state, action, reward, n_state):\n    \t\t\"\"\"Insert experience in history\"\
      \"\"\n        \n    \t\tsensed_ch, curr_ch, end = state\n    \t\tstate_vec =\
      \ np.concatenate([sensed_ch, curr_ch], axis=0) # Create the state vector for\
      \ the state\n        \n    \t\tn_sensed_ch, n_curr_ch, n_end = n_state\n   \
      \ \t\tn_state_vec = np.concatenate([n_sensed_ch, n_curr_ch], axis=0) # Create\
      \ the state vector for the new state\n\n    \t\tself.history[self.history_idx]\
      \ = [state_vec, end, action, reward, n_state_vec, n_end] # Insert everything\
      \ in the history\n        \n    \t\tself.history_idx = (self.history_idx+1)%self.history_length\
      \ # Move the history_idx by one\n    \n    \tdef copy_parameters(self):\n  \
      \  \t\t\"\"\"Copy the parameters of the online network to the offline network\"\
      \"\"\n\n    \t\tweights = self.DQN_online.get_weights()\n    \t\tself.DQN_offline.set_weights(weights)\n\
      \n\t\t\n\n    def make_grid(self,grid,channels):\n        grid = np.append(grid,\
      \ channels).reshape(self.window_size+1,self.number_of_channels)\n        grid\
      \ = np.delete(grid, 0, 0)\n        grid = grid.reshape(self.window_size,self.number_of_channels)\n\
      \        return grid.astype(np.float32)\n    \n    def evaluate_dq_agent(self,\
      \ agent, grid):\n\n        action_history=[]\n        tot_reward = 0\n    \n\
      \        self.dq_env.initialize()\n        first_state = self.dq_env.get_init_state()\n\
      \        action = agent.choose_action(first_state, epsilon = 0)\n        action_history.append(action)\n\
      \        reward, new_state = self.dq_env.run(action)\n        tot_reward +=\
      \ reward\n    \n        for j in range(grid.shape[1]-1):\n            state\
      \ = new_state\n            action = agent.choose_action(state, epsilon = 0)\n\
      \            action_history.append(action)\n            reward, new_state =\
      \ self.dq_env.run(action)\n            tot_reward += reward\n            \n\
      \        choosen_channels = [(grid.shape[0]-1)/2]\n        for i in range(len(action_history)):\n\
      \        \tchoosen_channels.append((choosen_channels[i]+action_history[i])%grid.shape[0])\n\
      \        choosen_channels = choosen_channels[1:]\n        #plt.imshow(np.flip(grid,\
      \ axis=0), origin=\"lower\", cmap='gray', vmin=0, vmax=1)\n        #for i in\
      \ range(len(choosen_channels)):\n        #\tplt.scatter(i, grid.shape[0]-1-choosen_channels[i],\
      \ color='r')\n        #plt.show()\n        #print(str(int(tot_reward))+'/'+str(grid.shape[1])+'\
      \ packets have been transmitted')\n        return tot_reward\n"
    active_threshold: '0.9'
    affinity: ''
    alias: ''
    comment: ''
    epsilon: '0.25'
    gamma: '0.6'
    initial_channel: '0'
    learning_rate: '0.01'
    maxoutbuf: '0'
    minoutbuf: '0'
    num_GPU: '1'
    number_of_channels: '7'
    seed: '1'
    slot_time: '1'
    window_size: '128'
  states:
    _io_cache: ('RL Channels select self agent', 'blk', [('number_of_channels', '7'),
      ('seed', '0'), ('num_GPU', '0.0'), ('active_threshold', '0.9'), ('slot_time',
      '1'), ('window_size', '5'), ('initial_channel', '0'), ('gamma', '1'), ('learning_rate',
      '0.01'), ('epsilon', '1')], [('0', 'complex', 1), ('1', 'complex', 1), ('2',
      'complex', 1), ('3', 'complex', 1), ('4', 'complex', 1), ('5', 'complex', 1),
      ('6', 'complex', 1)], [('0', 'short', 1)], 'Embedded Python Block example -
      a simple multiply const', ['active_threshold', 'epsilon', 'gamma', 'initial_channel',
      'learning_rate', 'number_of_channels', 'slot_time', 'window_size'])
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [1181, 910]
    rotation: 0
    state: disabled
- name: epy_block_3_0_0_0
  id: epy_block
  parameters:
    _source_code: "\"\"\"\nEmbedded Python Blocks:\n\nEach time this file is saved,\
      \ GRC will instantiate the first class it finds\nto get ports and parameters\
      \ of your block. The arguments to __init__  will\nbe the parameters. All of\
      \ them are required to have default values!\n\"\"\"\nfrom __future__ import\
      \ absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\
      from tf_agents.environments import py_environment\nfrom tf_agents.environments\
      \ import tf_environment\nfrom tf_agents.environments import tf_py_environment\n\
      from tf_agents.environments import utils\nfrom tf_agents.specs import array_spec\n\
      from tf_agents.environments import wrappers\nfrom tf_agents.environments import\
      \ suite_gym\nfrom tf_agents.trajectories import time_step as ts\nimport os\n\
      import copy\nimport sys      \nimport time\nimport schedule\nimport threading\n\
      from datetime import datetime, timedelta\nfrom timeslot import Timeslot  \n\
      import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport tensorflow\
      \ as tf\nfrom gnuradio import gr\nimport pmt\nfrom tensorflow.keras import Model\n\
      from tensorflow.keras.layers import Layer, Dense, Softmax\nfrom tensorflow.keras.layers\
      \ import Dense\n\nfrom gym import Env\nfrom gym.spaces import Discrete, Box\n\
      \n\nfrom matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg\nfrom matplotlib.figure\
      \ import Figure\nfrom PyQt5 import QtCore #conda install pyqt\nfrom PyQt5 import\
      \ QtWidgets\n\nimport abc\nfrom tf_agents.networks import q_network\nfrom tf_agents.agents.dqn\
      \ import dqn_agent\n\n\nHISTORY_BUFFER_LEN = 2000\nDEFAULT_WINDOW_SIZE = 32\n\
      \nclass blk(gr.sync_block):  # other base classes are basic_block, decim_block,\
      \ interp_block\n    \"\"\"Embedded Python Block example - a simple multiply\
      \ const\"\"\"\n\n    def __init__(self,number_of_channels=7,seed = 0, num_GPU=0.0,\
      \ active_threshold= 0.9, slot_time = 1, window_size = 5, initial_channel = 0,\
      \ gamma = 1, learning_rate = 0.01, epsilon = 1):  # only default arguments here\n\
      \        \"\"\"arguments to this function show up as parameters in GRC\"\"\"\
      \n        gr.sync_block.__init__(\n            self,\n            name='RL TF\
      \ Transmit agent',   # will show up in GRC\n            in_sig=[np.complex64],\n\
      \            out_sig=[np.uint16]\n        )\n        # if an attribute with\
      \ the same name as a parameter is found,\n        # a callback is registered\
      \ (properties work, too).\n        \n        #initailize tensorflow and GPU\
      \ usage\n        print('Tensorflow version: ', tf.__version__)\n        gpus\
      \ = tf.config.experimental.list_physical_devices(\"GPU\")\n        print('Number\
      \ of GPUs available :', len(gpus))\n        \n        if num_GPU < len(gpus):\n\
      \        \ttf.config.experimental.set_visible_devices(gpus[num_GPU], 'GPU')\n\
      \        \ttf.config.experimental.set_memory_growth(gpus[num_GPU], True)\n \
      \       \tprint('Only GPU number', num_GPU, 'used')\n        tf.random.set_seed(seed)\n\
      \t\n        #initialize Channel sensing\n        self.active_threshold = active_threshold\n\
      \        self.number_of_channels = number_of_channels\n        if (self.number_of_channels+1)%2!=0:\n\
      \        \tprint('/!\\ Please enter an even number of channels')\n        self.channels\
      \ = np.zeros(self.number_of_channels)\n        self.slot_time = slot_time\n\
      \        self.last_datetime = datetime.now()\n        self.window_size = window_size\n\
      \        self.grid = np.zeros((self.window_size, self.number_of_channels))\n\
      \        self.initial_channel = initial_channel #change to random\n        self.grid_flag\
      \ = 0\n        self.channel_decision = self.initial_channel\n        \n    \
      \    self.collision = 0\n        self.total_error = 0 \n        self.total_count\
      \ = 1 \n        #initialize DQN env\n        #self.dq_env = self.DQEnv(self.number_of_channels,\
      \ self.window_size, False, self.grid.T)\n\n        #initialize DQN Neural Network\n\
      \        self.gamma = gamma\n        self.learning_rate = learning_rate\n  \
      \      self.actions_number = self.number_of_channels\n        self.epsilon =\
      \ epsilon\n        self.nb_trainings = 300\n        self.loss = []  # Keep trak\
      \ of the losses\n        self.channel_decision = self.initial_channel\n    \
      \    self.train_flag = False\n        self.done = False\n        '''\n     \
      \   self.agent_dq = self.DQAgent(self.number_of_channels, self.learning_rate,\
      \ self.gamma)\n        #sensed_ch, curr_ch, end = self.dq_env.get_init_state()\n\
      \        #print('##################################')\n        #print('ENV initial\
      \ state:')\n        #print('State: ', sensed_ch,', Current Channel: ', curr_ch)\n\
      \        #print('##################################')\n        '''\n       \
      \ \n        \n        #############################################################\n\
      \        self.channel = []\n        \n        self.environment = self.transmit_wait()\n\
      \        utils.validate_py_environment(self.environment, episodes=5)\n     \
      \   \n        try_to_transmit_action = np.array(0, dtype=np.int32)\n       \
      \ end_round_action = np.array(1, dtype=np.int32)\n        \n        self.environment\
      \ = self.transmit_wait(np.random.randint(self.window_size), self.window_size)\n\
      \        print('states:            ',self.environment.observation_spec())\n\
      \        time_step = self.environment.reset()\n        print(time_step)\n  \
      \      cumulative_reward = time_step.reward\n        \n        \n\n      \n\n\
      \    def work(self, input_items, output_items):       \t\n        #Check slot\
      \ time\n        if (datetime.now() - self.last_datetime) > timedelta(days=0,\
      \ seconds=self.slot_time, microseconds=0, milliseconds=0, minutes=0, hours=0,\
      \ weeks=0):\n            #Sample and mark channel activity\n            \tif\
      \ (np.mean(abs(input_items[0][:]))>self.active_threshold):\n            \t\t\
      self.channel.append(1)\n            \telse:\n            \t\tself.channel.append(0)\n\
      \            \t\n            \tself.last_datetime = datetime.now()       \n\
      \            \tself.grid_flag += 1\n            \t\n            \t#Full window\
      \ sized grid\n            \tif (self.grid_flag == self.window_size+1): \n  \
      \          \t\tself.train_flag = True\n            \t\tself.channel.pop(0)\n\
      \            \t\tprint('history: ',self.channel)\n            \t\t\t\n     \
      \   if self.train_flag == True:\t\n        \t#Feed env with time grid\n    \
      \    \tself.environment = self.transmit_wait(self.channel)\n        \t#start\
      \ traning  \n        \tfor _ in range(100):\n            \t\ttime_step = self.environment.step(try_to_transmit_action)\n\
      \            \t\tprint(time_step)\n            \t\tcumulative_reward += time_step.reward\n\
      \        \ttime_step = self.environment.step(end_round_action)\n        \tprint(time_step)\n\
      \        \tcumulative_reward += time_step.reward\n        \tprint('Final Reward\
      \ = ', cumulative_reward)           \n         ## random decision at first (choose\
      \ threshhold)\n            \n        '''\n            for i in range(0,np.size(input_items,0)):\n\
      \            \t#Energy detected during time slot, mark as occupied \n      \
      \      \tif (np.mean(abs(input_items[i][:]))>self.active_threshold):\n     \
      \       \t\tself.channels[i] = 1\n            for j in range (np.size(self.channels)):\
      \ self.channels[j] = np.abs(self.channels[j]-1)#flip ones and zeros\n      \
      \      print(self.channels,'    ', datetime.now())   \n            #when training\
      \ is done, make a decision\n            if self.done == True:\n            \t\
      state = [self.channels.astype(np.float32), self.dq_env.one_hot(self.channel_decision,\
      \ self.number_of_channels), 0]\n            \tself.channel_decision = self.agent_dq.choose_action(state,\
      \ 0)# epsilon = 0 only expoiltation wehn not training\n            \tself.total_count\
      \ += 1\n            \tself.total_ratio = (self.total_error)/self.total_count\n\
      \            \tprint('CH: ',self.channels)\n            \ttemp = np.zeros(np.size(self.channels))\n\
      \            \ttemp[self.channel_decision]=1\n            \tprint('DC: ',temp)\n\
      \            \tprint('count: ', self.total_count, ' Errors: ', self.total_error,\
      \ ' ratio: ',self.total_ratio)\n         \t\n            \tif self.channels[self.channel_decision]\
      \ == 0:\n            \t\tself.collision += 1             \n            self.grid\
      \ = self.make_grid(self.grid,(self.channels))\n            self.last_datetime\
      \ = datetime.now()\n            self.channels = np.zeros(self.number_of_channels)\n\
      \            #print(self.grid)\n            \n            self.grid_flag +=\
      \ 1\n        output_items[0][:]= self.channel_decision      \n        if self.collision\
      \ == 1:\n        \tself.total_error += 1\n        \tself.collision = 0\n   \
      \     \t#self.grid_flag = self.window_size+1\n        \t'''\n\n        \t\n\
      \        '''#Full window sized grid\n        if (self.grid_flag == self.window_size+1):\
      \ \n\t\t\n        \t#initialize DQN env\n        \tself.dq_env = self.DQEnv(self.number_of_channels,\
      \ self.window_size, self.initial_channel, self.grid.T)\n\t\n\t\t# Run 200 episodes\n\
      \        \tfor i in range(HISTORY_BUFFER_LEN):\n                    \n     \
      \               self.dq_env.initialize()\n                    \n           \
      \         state = self.dq_env.get_init_state()\n                    action =\
      \ self.agent_dq.choose_action(state, self.epsilon)\n                    reward,\
      \ new_state = self.dq_env.run(action)\n\t    \n                    self.agent_dq.insert_history(state,\
      \ action, reward, new_state) # Insert state, action, reward, new state in history\
      \ \n\t    \n                    # While it's not the end of the episode\n  \
      \                  while new_state[2]==0 :\n                    \tstate = new_state\n\
      \                    \taction = self.agent_dq.choose_action(state, self.epsilon)\n\
      \                    \treward, new_state = self.dq_env.run(action)\n\t\t\n \
      \                   \tself.agent_dq.insert_history(state, action, reward, new_state)\
      \ # Insert state, action, reward, new state in history\n        \t\n       \
      \ \tself.train_flag = True                      \t\n        \tprint('Replay\
      \ buffer History populated')\n        \t                 \t \n\n        \t\n\
      \        \t\n\n        if self.train_flag == True:\n\t        print('Training...')\n\
      \        \tself.train_flag = False\n        \t\n        \tfor i in range(self.nb_trainings):\n\
      \        \t\tself.dq_env.initialize()\n        \t\tstate = self.dq_env.get_init_state()\n\
      \    \n        \t\taction = self.agent_dq.choose_action(state, self.epsilon)\n\
      \        \t\treward, new_state = self.dq_env.run(action)\n        \t\tself.agent_dq.insert_history(state,\
      \ action, reward, new_state) # Insert state, action, reward, new state in history\
      \ \n        \t\tself.agent_dq.learn(64) # Each time we store a new history,\
      \ we perform a training on random data\n    \n        \t\t# While it's not the\
      \ end of the episode\n        \t\twhile new_state[2]==0 :\n        \t\t\tstate\
      \ = new_state\n        \t\t\taction = self.agent_dq.choose_action(state, self.epsilon)\n\
      \        \t\t\treward, new_state = self.dq_env.run(action)\n        \t\t\tself.agent_dq.insert_history(state,\
      \ action, reward, new_state) # Insert state, action, reward, new state in history\
      \ \n        \t\t\tself.agent_dq.learn(64) # Each time we store a new history,\
      \ we perform a training on random data\n    \n        \t\tself.loss.append(tf.reduce_mean(self.agent_dq.loss).numpy())\
      \ #Save the losses for future visualization\n        \t\t\t#Every 10 iterations\
      \ we copy the parameters of the online DQN to the offline DQN\n        \t\t\
      if (i+1)%10 == 0:\n        \t\t\tself.agent_dq.copy_parameters()\n        \t\
      \t\tprint((i+1), end=', ')\n        \tself.done = True\n        \t#plt.semilogy(np.arange(len(self.loss)),\
      \ self.loss)\n        \t#plt.show()\n        \tself.evaluate_dq_agent(self.agent_dq\
      \ , self.dq_env.grid)\n        '''\n        return len(output_items[0])\n  \
      \      \n\n\n    class transmit_wait(py_environment.PyEnvironment):\n\n    \t\
      def __init__(self, grid = [] , window_size = DEFAULT_WINDOW_SIZE ):\n    \t\t\
      self._action_spec = array_spec.BoundedArraySpec(shape=(), dtype=np.int32, minimum=0,\
      \ maximum=1, name='action') #transmit or not\n    \t\tself._observation_spec\
      \ = array_spec.BoundedArraySpec(shape=(1,), dtype=np.int32, minimum=0, maximum=window_size\
      \ , name='observation')#time and ocupency\n    \t\tself._state = 0\n    \t\t\
      self._episode_ended = False\n\n    \tdef action_spec(self):\n    \t\treturn\
      \ self._action_spec\n\n    \tdef observation_spec(self):\n    \t\treturn self._observation_spec\n\
      \n    \tdef _reset(self):\n    \t\tself._state = 0\n    \t\tself._episode_ended\
      \ = False\n    \t\treturn ts.restart(np.array([self._state], dtype=np.int32))\n\
      \n    \tdef _step(self, action):\n\n    \t\tif self._episode_ended:\n    \t\t\
      \t# The last action ended the episode. Ignore the current action and start\n\
      \    \t\t\t# a new episode.\n    \t\t\treturn self.reset()\n\n    \t\t# Make\
      \ sure episodes don't go on forever.\n    \t\tif action == 1:\n    \t\t\tself._episode_ended\
      \ = True\n    \t\telif action == 0:\n    \t\t\tnew_card = np.random.randint(1,\
      \ 11)\n    \t\t\tself._state += new_card\n    \t\telse:\n    \t\t\traise ValueError('`action`\
      \ should be 0 (wait) or 1 (transmit).')\n\n    \t\tif self._episode_ended or\
      \ self._state >= 21:\n    \t\t\treward = self._state - 21 if self._state <=\
      \ 21 else -21\n    \t\t\treturn ts.termination(np.array([self._state], dtype=np.int32),\
      \ reward)\n    \t\telse:\n    \t\t\treturn ts.transition(np.array([self._state],\
      \ dtype=np.int32), reward=0.0, discount=1.0)\n\n\n\n    class PyEnvironment(object):\n\
      \    \n    \tdef reset(self):\n    \t\t\"\"\"Return initial_time_step.\"\"\"\
      \n    \t\tself._current_time_step = self._reset()\n    \t\treturn self._current_time_step\n\
      \    \t\t\n    \tdef step(self, action):\n    \t\t\"\"\"Apply action and return\
      \ new time_step.\"\"\"\n    \t\tif self._current_time_step is None:\n    \t\t\
      \treturn self.reset()\n    \t\tself._current_time_step = self._step(action)\n\
      \    \t\treturn self._current_time_step\n\n    \tdef current_time_step(self):\n\
      \    \t\treturn self._current_time_step\n\n    \tdef time_step_spec(self):\n\
      \    \t\t\"\"\"Return time_step_spec.\"\"\"\n\n    \t@abc.abstractmethod\n \
      \   \tdef observation_spec(self):\n    \t\t\"\"\"Return observation_spec.\"\"\
      \"\n\n    \t@abc.abstractmethod\n    \tdef action_spec(self):\n    \t\t\"\"\"\
      Return action_spec.\"\"\"\n\n    \t@abc.abstractmethod\n    \tdef _reset(self):\n\
      \    \t\t\"\"\"Return initial_time_step.\"\"\"\n\n    \t@abc.abstractmethod\n\
      \    \tdef _step(self, action):\n    \t\t\"\"\"Apply action and return new time_step.\"\
      \"\"\n    \n    class DQEnv():\n    \tdef __init__(self, nb_channels = 7, nb_states=5,\
      \ initial_state = 0 , grid = []):\n        \n\t    \tif (nb_channels+1)%2!=0:\n\
      \t    \t\tprint('/!\\ Please enter an even number of channels')\n          \
      \  \n\t    \tself.nb_ch = nb_channels\n\t    \tself.nb_states = nb_states\n\
      \        \n\t    \tself.grid = grid\n\t    \tprint ('env grid:')\n\t    \tprint(self.grid)\n\
      \t    \tself.init_state = [int(initial_state), int(self.nb_states-1)]\n\t  \
      \  \tself.ch_state = self.init_state[1]\n\t    \tself.curr_ch = self.init_state[0]\n\
      \t    \tself.sent_mess = 0\n        \n\t    \tprint('This environment has '\
      \ + str(self.nb_ch) +' different channels')\n        \n    \tdef run(self, action):\n\
      \        \n\t    \tself.sent_mess += 1\n        \n\t    \tself.ch_state  = (self.ch_state\
      \ + 1)%self.nb_states\n        \n\t    \tself.curr_ch = action\n\t    \t\n\t\
      \    \treward = np.abs(self.grid[self.curr_ch, self.ch_state])\n        \n\t\
      \    \tif self.sent_mess != self.nb_states: \n\t    \t\tend = 0\n\t    \telse\
      \ :\n\t    \t\tend = 1\n        \n\t    \treturn(reward, [self.grid[:, self.ch_state],\
      \ self.one_hot(self.curr_ch, self.nb_ch), end])\n    \n    \tdef get_init_state(self):\n\
      \t    \treturn [self.grid[:, self.init_state[1]], self.one_hot(self.init_state[0],\
      \ self.nb_ch), 0]\n    \n    \tdef initialize(self):\n\t    \tself.ch_state\
      \ = self.init_state[1]\n\t    \tself.curr_ch = self.init_state[0]\n\t    \t\
      self.sent_mess = 0\n        \n    \tdef one_hot(self, index, depth):\n\t   \
      \ \toh = np.zeros(depth, dtype=np.float32)\n\t    \toh[index] = 1\n\t    \t\
      return oh\n    \n    \n    class DQAgent():\n    \tdef __init__(self, nb_channels,\
      \ learning_rate, gamma):\n    \t\tself.nb_ch = nb_channels\n    \t\tself.nb_actions\
      \ = nb_channels\n    \t\tself.gamma = gamma\n    \t\tself.learning_rate = learning_rate\n\
      \    \t\t\n    \t\tself.history_length = HISTORY_BUFFER_LEN\n    \t\tself.history\
      \ = [[]for i in range(self.history_length)]\n    \t\tself.history_idx = 0\n\
      \    \t\t\n    \t\t#Create and initialize the online DQN\n    \t\tself.DQN_online\
      \ = tf.keras.models.Sequential([Dense(2*self.nb_ch, activation='relu'), Dense(self.nb_actions,\
      \ activation='softplus') #Outputs positive values \n    \t\t])\n    \t\tself.DQN_online.build(input_shape=(None,\
      \ self.nb_ch*2)) #Build the model to create the weights\n    \t\t\n    \t\t\
      #Create and initialize the offline DQN\n    \t\tself.DQN_offline = tf.keras.models.Sequential([Dense(2*self.nb_ch,\
      \ activation='relu'), Dense(self.nb_actions, activation='softplus') #Outputs\
      \ positive values\n    \t\t])\n    \t\tself.DQN_offline.build(input_shape=(None,\
      \ self.nb_ch*2)) #Build the model to create the weights\n    \t\t\n    \t\t\
      self.copy_parameters() #Copy the weights of the online network to the offline\
      \ network\n    \t\t\n    \t\tself.loss_func = tf.keras.losses.MSE\n    \t\t\
      self.optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n        \n  \
      \      \n    \tdef choose_action(self, state, epsilon):\n    \t\t\"\"\"Implements\
      \ an epsilon-greedy policy\"\"\"\n    \t\tsensed_ch, curr_ch, end = state\n\
      \    \t\t# Explore ?\n    \t\tif np.random.uniform(size=1) < epsilon :\n   \
      \ \t\t\taction =  np.random.randint(low = 0, high = np.size(sensed_ch)) #######\
      \ Take one action randomly {0, 1, 2, 3, 4, 5, 6} with probability epsilon\n\
      \    \t\t\t\n        \n    \t\t#Choose the best action\n    \t\telse:    \n\
      \    \t\t\t#sensed_ch, curr_ch, end = state #Decomposes the state\n    \t\t\t\
      '''\n    \t\t\t* The shapes are :\n    \t\t\t- curr_ch : [self.nb_ch]\n    \t\
      \t\t- sensed_ch : [self.nb_ch]\n    \t\t\t- end : one integer\n    \t\t\t'''\n\
      \    \t\t\tDQN_input = tf.concat([sensed_ch, curr_ch], axis=0)[tf.newaxis, :]\
      \ #Create a state vector, which is the DQN input. Shape : [1, 2*self.nb_ch]\n\
      \    \t\t\toutputs = self.DQN_online(DQN_input).numpy() #Get the predicted Q\
      \ values corresponding to the 3 actions\n    \t\t\taction = np.argmax(outputs)#-1\
      \ #Take the action that has the highest predicted Q value (0, 1, 2, 3, 4, 5,\
      \ 6)\t\t\t\n    \t\t\t#print(outputs)\n    \t\treturn action\n    \n    \tdef\
      \ learn(self, batch_size):\n    \t\t\"\"\"Sample experiences from the history\
      \ and performs SGD\"\"\"\n    \t\t\n    \t\t# Samples random experiences from\
      \ the history\n    \t\tidx = np.random.choice(range(self.history_length), batch_size,\
      \ replace=False) # Create random indexes \n    \t\trdm_exp =  [self.history[i]\
      \ for i in idx] # Take experiences corresponding to the random indexes\n   \
      \ \t\t\n    \t\t# Each experience is written in this format : [state_vec, end,\
      \ action, reward, n_state_vec, n_end] (see insert_history method)\n    \t\t\n\
      \    \t\t# Create 6 batches : states_vec, end_boolean, actions, rewards, new\
      \ states_vec, new_end_boolean\n    \t\tstates_vec = np.array([rdm_exp[i][0]\
      \ for i in range(batch_size)]) # Shape : [Bs, 2*self.nb_ch]\n    \t\tend = np.array([rdm_exp[i][1]\
      \ for i in range(batch_size)]) # Shape : [Bs]\n    \t\tactions = np.array([rdm_exp[i][2]\
      \ for i in range(batch_size)]) # Shape : [BS]\n    \t\trewards = np.array([rdm_exp[i][3]\
      \ for i in range(batch_size)]) # Shape : [BS]\n    \t\tn_states_vec = np.array([rdm_exp[i][4]\
      \ for i in range(batch_size)]) # Shape : [Bs, 2*self.nb_ch]\n    \t\tn_end =\
      \ np.array([rdm_exp[i][5] for i in range(batch_size)]) # Shape : [BS]\n    \t\
      \t\n    \t\t#Compute the best q_value for the new states\n    \t\tmax_n_q_values\
      \ = tf.reduce_max(self.DQN_offline(n_states_vec), axis=1).numpy()\n\n    \t\t\
      with tf.GradientTape() as tape:\n    \t\t\t#Forward pass through the online\
      \ network to predict the q_values\n    \t\t\tpred_q_values = self.DQN_online(states_vec)\n\
      \    \t\t\t\n    \t\t\t# Compute targets\n    \t\t\ttargets = pred_q_values.numpy()\n\
      \    \t\t\ttargets[np.arange(targets.shape[0]), actions]= rewards + (1-n_end)\
      \ * self.gamma * max_n_q_values\n    \t\t\t\n    \t\t\t# Evaluate the loss\n\
      \    \t\t\tself.loss = self.loss_func(pred_q_values, targets)\n        \n  \
      \  \t\t# Compute gradients and perform the gradient descent\n    \t\tgradients\
      \ = tape.gradient(self.loss, self.DQN_online.trainable_weights)\n    \t\tself.optimizer.apply_gradients(zip(gradients,\
      \ self.DQN_online.trainable_weights))  \n    \n    \tdef insert_history(self,\
      \ state, action, reward, n_state):\n    \t\t\"\"\"Insert experience in history\"\
      \"\"\n        \n    \t\tsensed_ch, curr_ch, end = state\n    \t\tstate_vec =\
      \ np.concatenate([sensed_ch, curr_ch], axis=0) # Create the state vector for\
      \ the state\n        \n    \t\tn_sensed_ch, n_curr_ch, n_end = n_state\n   \
      \ \t\tn_state_vec = np.concatenate([n_sensed_ch, n_curr_ch], axis=0) # Create\
      \ the state vector for the new state\n\n    \t\tself.history[self.history_idx]\
      \ = [state_vec, end, action, reward, n_state_vec, n_end] # Insert everything\
      \ in the history\n        \n    \t\tself.history_idx = (self.history_idx+1)%self.history_length\
      \ # Move the history_idx by one\n    \n    \tdef copy_parameters(self):\n  \
      \  \t\t\"\"\"Copy the parameters of the online network to the offline network\"\
      \"\"\n\n    \t\tweights = self.DQN_online.get_weights()\n    \t\tself.DQN_offline.set_weights(weights)\n\
      \n\t\t\n\n    def make_grid(self,grid,channels):\n        grid = np.append(grid,\
      \ channels).reshape(self.window_size+1,7)\n        grid = np.delete(grid, 0,\
      \ 0)\n        grid = grid.reshape(self.window_size,self.number_of_channels)\n\
      \        return grid.astype(np.float32)\n    \n    def evaluate_dq_agent(self,\
      \ agent, grid):\n\n        action_history=[]\n        tot_reward = 0\n    \n\
      \        self.dq_env.initialize()\n        first_state = self.dq_env.get_init_state()\n\
      \        action = agent.choose_action(first_state, epsilon = 0)\n        action_history.append(action)\n\
      \        reward, new_state = self.dq_env.run(action)\n        tot_reward +=\
      \ reward\n    \n        for j in range(grid.shape[1]-1):\n            state\
      \ = new_state\n            action = agent.choose_action(state, epsilon = 0)\n\
      \            action_history.append(action)\n            reward, new_state =\
      \ self.dq_env.run(action)\n            tot_reward += reward\n            \n\
      \        choosen_channels = [(grid.shape[0]-1)/2]\n        for i in range(len(action_history)):\n\
      \        \tchoosen_channels.append((choosen_channels[i]+action_history[i])%grid.shape[0])\n\
      \        choosen_channels = choosen_channels[1:]\n        #plt.imshow(np.flip(grid,\
      \ axis=0), origin=\"lower\", cmap='gray', vmin=0, vmax=1)\n        #for i in\
      \ range(len(choosen_channels)):\n        #\tplt.scatter(i, grid.shape[0]-1-choosen_channels[i],\
      \ color='r')\n        #plt.show()\n        #print(str(int(tot_reward))+'/'+str(grid.shape[1])+'\
      \ packets have been transmitted')\n        return tot_reward\n"
    active_threshold: '0.9'
    affinity: ''
    alias: ''
    comment: ''
    epsilon: '0.25'
    gamma: '0.6'
    initial_channel: '0'
    learning_rate: '0.01'
    maxoutbuf: '0'
    minoutbuf: '0'
    num_GPU: '1'
    number_of_channels: '1'
    seed: '1'
    slot_time: '1'
    window_size: '16'
  states:
    _io_cache: ('RL TF Transmit agent', 'blk', [('number_of_channels', '7'), ('seed',
      '0'), ('num_GPU', '0.0'), ('active_threshold', '0.9'), ('slot_time', '1'), ('window_size',
      '5'), ('initial_channel', '0'), ('gamma', '1'), ('learning_rate', '0.01'), ('epsilon',
      '1')], [('0', 'complex', 1)], [('0', 'short', 1)], 'Embedded Python Block example
      - a simple multiply const', ['active_threshold', 'epsilon', 'gamma', 'initial_channel',
      'learning_rate', 'number_of_channels', 'slot_time', 'window_size'])
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [1297, 347]
    rotation: 0
    state: disabled
- name: epy_block_3_0_0_0_0
  id: epy_block
  parameters:
    _source_code: "\"\"\"\nEmbedded Python Blocks:\n\nEach time this file is saved,\
      \ GRC will instantiate the first class it finds\nto get ports and parameters\
      \ of your block. The arguments to __init__  will\nbe the parameters. All of\
      \ them are required to have default values!\n\"\"\"\nfrom __future__ import\
      \ absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\
      from tf_agents.environments import py_environment\nfrom tf_agents.environments\
      \ import tf_environment\nfrom tf_agents.environments import tf_py_environment\n\
      from tf_agents.environments import utils\nfrom tf_agents.specs import array_spec\n\
      from tf_agents.environments import wrappers\nfrom tf_agents.environments import\
      \ suite_gym\nfrom tf_agents.trajectories import time_step as ts\nimport os\n\
      import copy\nimport sys      \nimport time\nimport schedule\nimport threading\n\
      from datetime import datetime, timedelta\nfrom timeslot import Timeslot  \n\
      import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport tensorflow\
      \ as tf\nfrom gnuradio import gr\nimport pmt\nfrom tensorflow.keras import Model\n\
      from tensorflow.keras.layers import Layer, Dense, Softmax\nfrom tensorflow.keras.layers\
      \ import Dense\n\nfrom gym import Env\nfrom gym.spaces import Discrete, Box\n\
      \n\nfrom matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg\nfrom matplotlib.figure\
      \ import Figure\nfrom PyQt5 import QtCore #conda install pyqt\nfrom PyQt5 import\
      \ QtWidgets\n\nimport abc\nfrom tf_agents.networks import q_network\nfrom tf_agents.agents.dqn\
      \ import dqn_agent\n\n\nHISTORY_BUFFER_LEN = 20\nDEFAULT_WINDOW_SIZE = 32\n\
      EPISODE_LENGTH = 100\nNUMBER_OF_EPISODES = 10\n\nclass blk(gr.sync_block): \
      \ # other base classes are basic_block, decim_block, interp_block\n    \"\"\"\
      Embedded Python Block example - a simple multiply const\"\"\"\n\n    def __init__(self,number_of_channels=7,seed\
      \ = 0, num_GPU=0.0, active_threshold= 0.9, slot_time = 1, window_size = 5, initial_channel\
      \ = 0, gamma = 1, learning_rate = 0.01, epsilon = 1):  # only default arguments\
      \ here\n        \"\"\"arguments to this function show up as parameters in GRC\"\
      \"\"\n        gr.sync_block.__init__(\n            self,\n            name='RL\
      \ Transmit agent',   # will show up in GRC\n            in_sig=[np.complex64],\n\
      \            out_sig=[np.uint16]\n        )\n        # if an attribute with\
      \ the same name as a parameter is found,\n        # a callback is registered\
      \ (properties work, too).\n        \n        #initailize tensorflow and GPU\
      \ usage\n        print('Tensorflow version: ', tf.__version__)\n        gpus\
      \ = tf.config.experimental.list_physical_devices(\"GPU\")\n        print('Number\
      \ of GPUs available :', len(gpus))\n        \n        if num_GPU < len(gpus):\n\
      \        \ttf.config.experimental.set_visible_devices(gpus[num_GPU], 'GPU')\n\
      \        \ttf.config.experimental.set_memory_growth(gpus[num_GPU], True)\n \
      \       \tprint('Only GPU number', num_GPU, 'used')\n        tf.random.set_seed(seed)\n\
      \t\n        #initialize Channel sensing\n        self.active_threshold = active_threshold\n\
      \        self.number_of_channels = number_of_channels\n        if (self.number_of_channels+1)%2!=0:\n\
      \        \tprint('/!\\ Please enter an even number of channels')\n        self.channels\
      \ = np.zeros(self.number_of_channels)\n        self.slot_time = slot_time\n\
      \        self.last_datetime = datetime.now()\n        self.window_size = window_size\n\
      \        self.grid = np.zeros((self.window_size, self.number_of_channels))\n\
      \        self.initial_channel = initial_channel #change to random\n        self.grid_flag\
      \ = 0\n        self.channel_decision = self.initial_channel\n        \n    \
      \    self.collision = 0\n        self.total_error = 0 \n        self.total_count\
      \ = 1 \n        #initialize DQN env\n        #self.dq_env = self.DQEnv(self.number_of_channels,\
      \ self.window_size, False, self.grid.T)\n\n        #initialize DQN Neural Network\n\
      \        self.gamma = gamma\n        self.learning_rate = learning_rate\n  \
      \      self.actions_number = 2\n        self.epsilon = epsilon\n        self.nb_trainings\
      \ = 300\n        #############################################################\
      \        \n        self.num_iterations  = 300\n        self.initial_collect_steps\
      \ = 100\n        self.replay_buffer_max_length = HISTORY_BUFFER_LEN\n      \
      \  self.batch_size = 64\n        self.log_interval = 200\n        self.num_eval_episodes\
      \ = 10\n        self.eval_interval = 1000\n        #############################################################\
      \        \n        self.loss = []  # Keep trak of the losses\n        self.channel_decision\
      \ = self.initial_channel\n        self.train_flag = False\n        self.done\
      \ = False\n        self.hist =0\n        '''\n        self.agent_dq = self.DQAgent(self.number_of_channels,\
      \ self.learning_rate, self.gamma)\n        #sensed_ch, curr_ch, end = self.dq_env.get_init_state()\n\
      \        #print('##################################')\n        #print('ENV initial\
      \ state:')\n        #print('State: ', sensed_ch,', Current Channel: ', curr_ch)\n\
      \        #print('##################################')\n        '''\n       \
      \ \n        \n        #############################################################\n\
      \        self.channel = []\n        \n        self.environment = self.transmit_wait(0,self.window_size,0)\n\
      \        self.agent = self.DQN_agent(self.learning_rate, self.gamma)\n\n   \
      \ def work(self, input_items, output_items):       \t\n        #Check slot time\n\
      \        if (datetime.now() - self.last_datetime) > timedelta(days=0, seconds=self.slot_time,\
      \ microseconds=0, milliseconds=0, minutes=0, hours=0, weeks=0):\n          \
      \  \t#Sample and mark channel activity\n            \tactive = np.mean(abs(input_items[0][:]))>self.active_threshold\n\
      \            \tif active:\n            \t\tself.channel.append(1)\n        \
      \    \telse:\n            \t\tself.channel.append(0)\n            \t\n     \
      \       \tself.last_datetime = datetime.now()       \n            \tself.grid_flag\
      \ += 1\n            \t\n            \t#Full window sized grid\n            \t\
      #if (self.grid_flag == self.window_size+1): \n            \t#\tself.train_flag\
      \ = True\n            \t#\tself.channel.pop(0)\n            \t#\tprint('history:\
      \ ',self.channel)\n       \n\t\t# Run 200 episodes\n            \t\n       \
      \     \tself.environment.reset()\n            \tprint('insert history ', self.hist)\n\
      \            \tstate = [0,0,0]\n            \taction = self.agent.choose_action(state,\
      \ self.epsilon)\n            \treward, new_state, end = self.environment.step(action,\
      \ int(active))\n            \tend = new_state[2]\n    \n            \tself.agent.insert_history(state,\
      \ action, reward, new_state) # Insert state, action, reward, new state in history\
      \ \n    \n            \t# While it's not the end of the episode\n          \
      \  \twhile end==0 : #atache to time###################################################################\
      \ once a slot\n            \t\tstate = new_state\n            \t\taction = self.agent.choose_action(state,\
      \ self.epsilon)\n            \t\treward, new_state, end = self.environment.step(action,\
      \ int(active))\n            \t\tend = new_state[2]\n\t\n            \tself.agent.insert_history(state,\
      \ action, reward, new_state) # Insert state, action, reward, new state in history\n\
      \            \tself.hist += 1\n            \tif self.hist > HISTORY_BUFFER_LEN:\n\
      \            \t\tself.train_flag = True                      \t\n          \
      \  \t\tprint('Replay buffer History populated')       \n       \n       \n \
      \           \t\t\t\n        if self.train_flag == True:\t\n        \t#Feed env\
      \ with initial schannel state\n        \tself.environment = self.transmit_wait(int(active),self.window_size,0)\n\
      \        \t#start traning  \n        \tfor episode in range(NUMBER_OF_EPISODES):\n\
      \            \t\tstate = self.environment.reset()[1]\n            \t\tdone =\
      \ False\n            \t\tscore = 0\n            \t\t\n            \t\twhile\
      \ not done:\n            \t\t\taction = self.agent.choose_action(state, self.epsilon)\n\
      \            \t\t\treward, new_state , end = self.environment.step(action, int(active))\n\
      \            \t\t\t\n            \t\t\tself.agent.insert_history(state, action,\
      \ reward, new_state) # Insert S,A,R,S' history \n            \t\t\tself.agent.learn(10)\
      \ # Each time we store a new history, we perform a training on random data\n\
      \            \t\t\t\n            \t\t\tif new_state[2] == 1:\n            \t\
      \t\t\tdone = True\n            \t\tprint(' Episode:{} Score:{}' .format(episode,\
      \ score))\n        \tself.train_flag = False\n        \t         \n        \
      \ ## random decision at first (choose threshhold)\n            \n        '''\n\
      \            for i in range(0,np.size(input_items,0)):\n            \t#Energy\
      \ detected during time slot, mark as occupied \n            \tif (np.mean(abs(input_items[i][:]))>self.active_threshold):\n\
      \            \t\tself.channels[i] = 1\n            for j in range (np.size(self.channels)):\
      \ self.channels[j] = np.abs(self.channels[j]-1)#flip ones and zeros\n      \
      \      print(self.channels,'    ', datetime.now())   \n            #when training\
      \ is done, make a decision\n            if self.done == True:\n            \t\
      state = [self.channels.astype(np.float32), self.dq_env.one_hot(self.channel_decision,\
      \ self.number_of_channels), 0]\n            \tself.channel_decision = self.agent_dq.choose_action(state,\
      \ 0)# epsilon = 0 only expoiltation wehn not training\n            \tself.total_count\
      \ += 1\n            \tself.total_ratio = (self.total_error)/self.total_count\n\
      \            \tprint('CH: ',self.channels)\n            \ttemp = np.zeros(np.size(self.channels))\n\
      \            \ttemp[self.channel_decision]=1\n            \tprint('DC: ',temp)\n\
      \            \tprint('count: ', self.total_count, ' Errors: ', self.total_error,\
      \ ' ratio: ',self.total_ratio)\n         \t\n            \tif self.channels[self.channel_decision]\
      \ == 0:\n            \t\tself.collision += 1             \n            self.grid\
      \ = self.make_grid(self.grid,(self.channels))\n            self.last_datetime\
      \ = datetime.now()\n            self.channels = np.zeros(self.number_of_channels)\n\
      \            #print(self.grid)\n            \n            self.grid_flag +=\
      \ 1\n        output_items[0][:]= self.channel_decision      \n        if self.collision\
      \ == 1:\n        \tself.total_error += 1\n        \tself.collision = 0\n   \
      \     \t#self.grid_flag = self.window_size+1\n        \t'''\n\n        \t\n\
      \        '''#Full window sized grid\n        if (self.grid_flag == self.window_size+1):\
      \ \n\t\t\n        \t#initialize DQN env\n        \tself.dq_env = self.DQEnv(self.number_of_channels,\
      \ self.window_size, self.initial_channel, self.grid.T)\n\t\n\t\t# Run 200 episodes\n\
      \        \tfor i in range(HISTORY_BUFFER_LEN):\n                    \n     \
      \               self.dq_env.initialize()\n                    \n           \
      \         state = self.dq_env.get_init_state()\n                    action =\
      \ self.agent_dq.choose_action(state, self.epsilon)\n                    reward,\
      \ new_state = self.dq_env.run(action)\n\t    \n                    self.agent_dq.insert_history(state,\
      \ action, reward, new_state) # Insert state, action, reward, new state in history\
      \ \n\t    \n                    # While it's not the end of the episode\n  \
      \                  while new_state[2]==0 :\n                    \tstate = new_state\n\
      \                    \taction = self.agent_dq.choose_action(state, self.epsilon)\n\
      \                    \treward, new_state = self.dq_env.run(action)\n\t\t\n \
      \                   \tself.agent_dq.insert_history(state, action, reward, new_state)\
      \ # Insert state, action, reward, new state in history\n        \t\n       \
      \ \tself.train_flag = True                      \t\n        \tprint('Replay\
      \ buffer History populated')\n        \t                 \t \n\n        \t\n\
      \        \t\n\n        if self.train_flag == True:\n\t        print('Training...')\n\
      \        \tself.train_flag = False\n        \t\n        \tfor i in range(self.nb_trainings):\n\
      \        \t\tself.dq_env.initialize()\n        \t\tstate = self.dq_env.get_init_state()\n\
      \    \n        \t\taction = self.agent_dq.choose_action(state, self.epsilon)\n\
      \        \t\treward, new_state = self.dq_env.run(action)\n        \t\tself.agent_dq.insert_history(state,\
      \ action, reward, new_state) # Insert state, action, reward, new state in history\
      \ \n        \t\tself.agent_dq.learn(64) # Each time we store a new history,\
      \ we perform a training on random data\n    \n        \t\t# While it's not the\
      \ end of the episode\n        \t\twhile new_state[2]==0 :\n        \t\t\tstate\
      \ = new_state\n        \t\t\taction = self.agent_dq.choose_action(state, self.epsilon)\n\
      \        \t\t\treward, new_state = self.dq_env.run(action)\n        \t\t\tself.agent_dq.insert_history(state,\
      \ action, reward, new_state) # Insert state, action, reward, new state in history\
      \ \n        \t\t\tself.agent_dq.learn(64) # Each time we store a new history,\
      \ we perform a training on random data\n    \n        \t\tself.loss.append(tf.reduce_mean(self.agent_dq.loss).numpy())\
      \ #Save the losses for future visualization\n        \t\t\t#Every 10 iterations\
      \ we copy the parameters of the online DQN to the offline DQN\n        \t\t\
      if (i+1)%10 == 0:\n        \t\t\tself.agent_dq.copy_parameters()\n        \t\
      \t\tprint((i+1), end=', ')\n        \tself.done = True\n        \t#plt.semilogy(np.arange(len(self.loss)),\
      \ self.loss)\n        \t#plt.show()\n        \tself.evaluate_dq_agent(self.agent_dq\
      \ , self.dq_env.grid)\n        '''\n        return len(output_items[0])\n  \
      \      \n\n    class transmit_wait():\n\n    \tdef __init__(self, sensed = 0\
      \ , window_size = DEFAULT_WINDOW_SIZE, initial_state = 0 ):\n    \t\tself._action_spec\
      \ = Discrete(2) #transmit or not {0,1}\n    \t\tself._observation_spec = Discrete(2)\
      \ #occupied or not {0,1}\n    \t\tself.initial_state = initial_state #no transmision\n\
      \    \t\t#self.window_size = window_size\n    \t\tself.sensed = sensed\n   \
      \ \t\tself._state = [initial_state , sensed , 0]\n    \t\tself.episode_length\
      \ = EPISODE_LENGTH\n    \t\tself.reward = 0\n    \t\t\n    \tdef action_spec(self):\n\
      \    \t\treturn self._action_spec\n\n    \tdef observation_spec(self):\n   \
      \ \t\treturn self._observation_spec\n\n    \tdef reset(self):\n    \t\tself._state\
      \ = [self.initial_state , self.sensed , 0]\n    \t\tself._episode_ended = False\n\
      \    \t\tself.episode_length = EPISODE_LENGTH\n    \t\tself.reward = 0\n   \
      \ \t\treturn (self.reward , self._state , 0)\n\n    \tdef step(self, action,\
      \ sensed):\n    \t\tprv_action, prv_sensed, self._episode_ended = self._state\n\
      \    \t\treward = self.reward\n    \t\tif self._episode_ended:\n    \t\t\t#\
      \ The last action ended the episode. Ignore the current action and start\n \
      \   \t\t\t# a new episode.\n    \t\t\tself.reset()\n    \t\t\t\n    \t\tif action\
      \ == 1 or action == 0:\n    \t\t\tself._state = [action , sensed , 0] #new state\n\
      \    \t\telse:\n    \t\t\traise ValueError('`action` should be 0 (wait) or 1\
      \ (transmit).')\n    \t\tif prv_sensed == 1 and prv_action == 1: #collision\n\
      \    \t\t\tself.reward = self.reward-2\n    \t\telif prv_sensed == 0 and prv_action\
      \ == 1: #clean transmit\n    \t\t\tself.reward = self.reward+2\n    \t\telif\
      \ prv_sensed == 1 and prv_action == 0: #avoided collision\n    \t\t\tself.reward\
      \ = self.reward+1\n    \t\telif prv_sensed == 0 and prv_action == 0: #wasted\
      \ slot\n    \t\t\tself.reward = self.reward-1\t\n    \t\t\t\n    \t\t#episode\
      \ done?\t\n    \t\tself.episode_length -=1\n    \t\tif self.episode_length <=\
      \ 0:\n    \t\t\tself._state = [action , sensed , 1]\n    \t\t\n    \t\t# Make\
      \ sure episodes don't go on forever.\n    \t\tif self._episode_ended:\n    \t\
      \t\treturn(reward, self._state , 1)\n    \t\telse:\n    \t\t\treturn(reward,\
      \ self._state , 0)\n\t\t\t    \t\t\t\n    class DQN_agent():\n    \tdef __init__(self,\
      \ learning_rate, gamma):\n    \t\tself.nb_actions = 2\n    \t\tself.gamma =\
      \ gamma\n    \t\tself.learning_rate = learning_rate\n    \t\t\n    \t\tself.history_length\
      \ = HISTORY_BUFFER_LEN\n    \t\tself.history = [[]for i in range(self.history_length)]\n\
      \    \t\tself.history_idx = 0\n    \t\t\n    \t\t#Create and initialize the\
      \ online DQN\n    \t\tself.DQN_online = tf.keras.models.Sequential([Dense(2,\
      \ activation='relu'), Dense(self.nb_actions, activation='softplus') #Outputs\
      \ positive values \n    \t\t])\n    \t\tself.DQN_online.build(input_shape=(None,\
      \ 2)) #Build the model to create the weights\n    \t\t\n    \t\t#Create and\
      \ initialize the offline DQN\n    \t\tself.DQN_offline = tf.keras.models.Sequential([Dense(2,\
      \ activation='relu'), Dense(self.nb_actions, activation='softplus') #Outputs\
      \ positive values\n    \t\t])\n    \t\tself.DQN_offline.build(input_shape=(None,\
      \ 2)) #Build the model to create the weights\n    \t\t\n    \t\tself.copy_parameters()\
      \ #Copy the weights of the online network to the offline network\n    \t\t\n\
      \    \t\tself.loss_func = tf.keras.losses.MSE\n    \t\tself.optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n\
      \        \n        \n    \tdef choose_action(self, state, epsilon):\n    \t\t\
      \"\"\"Implements an epsilon-greedy policy\"\"\"\n    \t\t#sensed_ch, curr_ch,\
      \ end = state\n    \t\ttransmit , sensed , end = state\n    \t\t# Explore ?\n\
      \    \t\tif np.random.uniform(size=1) < epsilon :\n    \t\t\taction =  np.random.randint(low\
      \ = 0, high = 1) ####### Transmit with probability epsilon\n\n    \t\t#Exploite\
      \ - Choose the current best action\n    \t\telse:    \n    \t\t\tDQN_input =\
      \ tf.concat([[int(transmit)], [int(sensed)]], axis=0)[tf.newaxis, :] #Create\
      \ a state vector, which is the DQN input. Shape : [1, 1]\n    \t\t\toutputs\
      \ = self.DQN_online(DQN_input).numpy() #Get the predicted Q values corresponding\
      \ to the 2 actions\n    \t\t\taction = np.argmax(outputs) #Take the action that\
      \ has the highest predicted Q value (0, 1)\t\t\t\n    \t\treturn action\n  \
      \  \n    \tdef learn(self, batch_size):\n    \t\t\"\"\"Sample experiences from\
      \ the history and performs SGD\"\"\"\n    \t\t\n    \t\t# Samples random experiences\
      \ from the history\n    \t\tidx = np.random.choice(range(self.history_length),\
      \ batch_size, replace=False) # Create random indexes \n    \t\trdm_exp =  [self.history[i]\
      \ for i in idx] # Take experiences corresponding to the random indexes\n   \
      \ \t\t\n    \t\t# Each experience is written in this format : [state_vec, end,\
      \ action, reward, n_state_vec, n_end] (see insert_history method)\n    \t\t\n\
      \    \t\t# Create 6 batches : states_vec, end_boolean, actions, rewards, new\
      \ states_vec, new_end_boolean\n    \t\tstates_vec = np.array([rdm_exp[i][0]\
      \ for i in range(batch_size)]) # Shape : [Bs, 2*self.nb_ch]\n    \t\tend = np.array([rdm_exp[i][1]\
      \ for i in range(batch_size)]) # Shape : [Bs]\n    \t\tactions = np.array([rdm_exp[i][2]\
      \ for i in range(batch_size)]) # Shape : [BS]\n    \t\trewards = np.array([rdm_exp[i][3]\
      \ for i in range(batch_size)]) # Shape : [BS]\n    \t\tn_states_vec = np.array([rdm_exp[i][4]\
      \ for i in range(batch_size)]) # Shape : [Bs, 2*self.nb_ch]\n    \t\tn_end =\
      \ np.array([rdm_exp[i][5] for i in range(batch_size)]) # Shape : [BS]\n    \t\
      \t\n    \t\t#Compute the best q_value for the new states\n    \t\tmax_n_q_values\
      \ = tf.reduce_max(self.DQN_offline(n_states_vec), axis=1).numpy()\n\n    \t\t\
      with tf.GradientTape() as tape:\n    \t\t\t#Forward pass through the online\
      \ network to predict the q_values\n    \t\t\tpred_q_values = self.DQN_online(states_vec)\n\
      \    \t\t\t\n    \t\t\t# Compute targets\n    \t\t\ttargets = pred_q_values.numpy()\n\
      \    \t\t\ttargets[np.arange(targets.shape[0]), actions]= rewards + (1-n_end)\
      \ * self.gamma * max_n_q_values\n    \t\t\t\n    \t\t\t# Evaluate the loss\n\
      \    \t\t\tself.loss = self.loss_func(pred_q_values, targets)\n        \n  \
      \  \t\t# Compute gradients and perform the gradient descent\n    \t\tgradients\
      \ = tape.gradient(self.loss, self.DQN_online.trainable_weights)\n    \t\tself.optimizer.apply_gradients(zip(gradients,\
      \ self.DQN_online.trainable_weights))  \n    \n    \tdef insert_history(self,\
      \ state, action, reward, n_state):\n    \t\t\"\"\"Insert experience in history\"\
      \"\"\n        \n    \t\t#sensed_ch, curr_ch, end = state\n    \t\ttransmit ,\
      \ sensed , end = state\n    \t\tstate_vec = np.array([int(transmit),int(sensed)])#\
      \ Create the state vector for the state\n        \n    \t\tn_transmit, n_sensed,\
      \ n_end = n_state\n    \t\tn_state_vec = np.array([n_transmit, n_sensed]) #\
      \ Create the state vector for the new state\n\n    \t\tself.history[self.history_idx]\
      \ = [state_vec, end, action, reward, n_state_vec, n_end] # Insert everything\
      \ in the history\n        \n    \t\tself.history_idx = (self.history_idx+1)%self.history_length\
      \ # Move the history_idx by one\n    \n    \tdef copy_parameters(self):\n  \
      \  \t\t\"\"\"Copy the parameters of the online network to the offline network\"\
      \"\"\n\n    \t\tweights = self.DQN_online.get_weights()\n    \t\tself.DQN_offline.set_weights(weights)\t\
      \t\t\n''' \n    class DQEnv():\n    \tdef __init__(self, nb_channels = 7, nb_states=5,\
      \ initial_state = 0 , grid = []):\n        \n\t    \tif (nb_channels+1)%2!=0:\n\
      \t    \t\tprint('/!\\ Please enter an even number of channels')\n          \
      \  \n\t    \tself.nb_ch = nb_channels\n\t    \tself.nb_states = nb_states\n\
      \        \n\t    \tself.grid = grid\n\t    \tprint ('env grid:')\n\t    \tprint(self.grid)\n\
      \t    \tself.init_state = [int(initial_state), int(self.nb_states-1)]\n\t  \
      \  \tself.ch_state = self.init_state[1]\n\t    \tself.curr_ch = self.init_state[0]\n\
      \t    \tself.sent_mess = 0\n        \n\t    \tprint('This environment has '\
      \ + str(self.nb_ch) +' different channels')\n        \n    \tdef run(self, action):\n\
      \        \n\t    \tself.sent_mess += 1\n        \n\t    \tself.ch_state  = (self.ch_state\
      \ + 1)%self.nb_states\n        \n\t    \tself.curr_ch = action\n\t    \t\n\t\
      \    \treward = np.abs(self.grid[self.curr_ch, self.ch_state])\n        \n\t\
      \    \tif self.sent_mess != self.nb_states: \n\t    \t\tend = 0\n\t    \telse\
      \ :\n\t    \t\tend = 1\n        \n\t    \treturn(reward, [self.grid[:, self.ch_state],\
      \ self.one_hot(self.curr_ch, self.nb_ch), end])\n    \n    \tdef get_init_state(self):\n\
      \t    \treturn [self.grid[:, self.init_state[1]], self.one_hot(self.init_state[0],\
      \ self.nb_ch), 0]\n    \n    \tdef initialize(self):\n\t    \tself.ch_state\
      \ = self.init_state[1]\n\t    \tself.curr_ch = self.init_state[0]\n\t    \t\
      self.sent_mess = 0\n        \n    \tdef one_hot(self, index, depth):\n\t   \
      \ \toh = np.zeros(depth, dtype=np.float32)\n\t    \toh[index] = 1\n\t    \t\
      return oh\n    \n    \n    class DQAgent():\n    \tdef __init__(self, nb_channels,\
      \ learning_rate, gamma):\n    \t\tself.nb_ch = nb_channels\n    \t\tself.nb_actions\
      \ = nb_channels\n    \t\tself.gamma = gamma\n    \t\tself.learning_rate = learning_rate\n\
      \    \t\t\n    \t\tself.history_length = HISTORY_BUFFER_LEN\n    \t\tself.history\
      \ = [[]for i in range(self.history_length)]\n    \t\tself.history_idx = 0\n\
      \    \t\t\n    \t\t#Create and initialize the online DQN\n    \t\tself.DQN_online\
      \ = tf.keras.models.Sequential([Dense(2*self.nb_ch, activation='relu'), Dense(self.nb_actions,\
      \ activation='softplus') #Outputs positive values \n    \t\t])\n    \t\tself.DQN_online.build(input_shape=(None,\
      \ self.nb_ch*2)) #Build the model to create the weights\n    \t\t\n    \t\t\
      #Create and initialize the offline DQN\n    \t\tself.DQN_offline = tf.keras.models.Sequential([Dense(2*self.nb_ch,\
      \ activation='relu'), Dense(self.nb_actions, activation='softplus') #Outputs\
      \ positive values\n    \t\t])\n    \t\tself.DQN_offline.build(input_shape=(None,\
      \ self.nb_ch*2)) #Build the model to create the weights\n    \t\t\n    \t\t\
      self.copy_parameters() #Copy the weights of the online network to the offline\
      \ network\n    \t\t\n    \t\tself.loss_func = tf.keras.losses.MSE\n    \t\t\
      self.optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n        \n  \
      \      \n    \tdef choose_action(self, state, epsilon):\n    \t\t\"\"\"Implements\
      \ an epsilon-greedy policy\"\"\"\n    \t\tsensed_ch, curr_ch, end = state\n\
      \    \t\t# Explore ?\n    \t\tif np.random.uniform(size=1) < epsilon :\n   \
      \ \t\t\taction =  np.random.randint(low = 0, high = np.size(sensed_ch)) #######\
      \ Take one action randomly {0, 1, 2, 3, 4, 5, 6} with probability epsilon\n\
      \    \t\t\t\n        \n    \t\t#Choose the best action\n    \t\telse:    \n\
      \    \t\t\t#sensed_ch, curr_ch, end = state #Decomposes the state\n\n    \t\t\
      \tDQN_input = tf.concat([sensed_ch, curr_ch], axis=0)[tf.newaxis, :] #Create\
      \ a state vector, which is the DQN input. Shape : [1, 2*self.nb_ch]\n    \t\t\
      \toutputs = self.DQN_online(DQN_input).numpy() #Get the predicted Q values corresponding\
      \ to the 3 actions\n    \t\t\taction = np.argmax(outputs)#-1 #Take the action\
      \ that has the highest predicted Q value (0, 1, 2, 3, 4, 5, 6)\t\t\t\n    \t\
      \t\t#print(outputs)\n    \t\treturn action\n    \n    \tdef learn(self, batch_size):\n\
      \    \t\t\"\"\"Sample experiences from the history and performs SGD\"\"\"\n\
      \    \t\t\n    \t\t# Samples random experiences from the history\n    \t\tidx\
      \ = np.random.choice(range(self.history_length), batch_size, replace=False)\
      \ # Create random indexes \n    \t\trdm_exp =  [self.history[i] for i in idx]\
      \ # Take experiences corresponding to the random indexes\n    \t\t\n    \t\t\
      # Each experience is written in this format : [state_vec, end, action, reward,\
      \ n_state_vec, n_end] (see insert_history method)\n    \t\t\n    \t\t# Create\
      \ 6 batches : states_vec, end_boolean, actions, rewards, new states_vec, new_end_boolean\n\
      \    \t\tstates_vec = np.array([rdm_exp[i][0] for i in range(batch_size)]) #\
      \ Shape : [Bs, 2*self.nb_ch]\n    \t\tend = np.array([rdm_exp[i][1] for i in\
      \ range(batch_size)]) # Shape : [Bs]\n    \t\tactions = np.array([rdm_exp[i][2]\
      \ for i in range(batch_size)]) # Shape : [BS]\n    \t\trewards = np.array([rdm_exp[i][3]\
      \ for i in range(batch_size)]) # Shape : [BS]\n    \t\tn_states_vec = np.array([rdm_exp[i][4]\
      \ for i in range(batch_size)]) # Shape : [Bs, 2*self.nb_ch]\n    \t\tn_end =\
      \ np.array([rdm_exp[i][5] for i in range(batch_size)]) # Shape : [BS]\n    \t\
      \t\n    \t\t#Compute the best q_value for the new states\n    \t\tmax_n_q_values\
      \ = tf.reduce_max(self.DQN_offline(n_states_vec), axis=1).numpy()\n\n    \t\t\
      with tf.GradientTape() as tape:\n    \t\t\t#Forward pass through the online\
      \ network to predict the q_values\n    \t\t\tpred_q_values = self.DQN_online(states_vec)\n\
      \    \t\t\t\n    \t\t\t# Compute targets\n    \t\t\ttargets = pred_q_values.numpy()\n\
      \    \t\t\ttargets[np.arange(targets.shape[0]), actions]= rewards + (1-n_end)\
      \ * self.gamma * max_n_q_values\n    \t\t\t\n    \t\t\t# Evaluate the loss\n\
      \    \t\t\tself.loss = self.loss_func(pred_q_values, targets)\n        \n  \
      \  \t\t# Compute gradients and perform the gradient descent\n    \t\tgradients\
      \ = tape.gradient(self.loss, self.DQN_online.trainable_weights)\n    \t\tself.optimizer.apply_gradients(zip(gradients,\
      \ self.DQN_online.trainable_weights))  \n    \n    \tdef insert_history(self,\
      \ state, action, reward, n_state):\n    \t\t\"\"\"Insert experience in history\"\
      \"\"\n        \n    \t\tsensed_ch, curr_ch, end = state\n    \t\tstate_vec =\
      \ np.concatenate([sensed_ch, curr_ch], axis=0) # Create the state vector for\
      \ the state\n        \n    \t\tn_sensed_ch, n_curr_ch, n_end = n_state\n   \
      \ \t\tn_state_vec = np.concatenate([n_sensed_ch, n_curr_ch], axis=0) # Create\
      \ the state vector for the new state\n\n    \t\tself.history[self.history_idx]\
      \ = [state_vec, end, action, reward, n_state_vec, n_end] # Insert everything\
      \ in the history\n        \n    \t\tself.history_idx = (self.history_idx+1)%self.history_length\
      \ # Move the history_idx by one\n    \n    \tdef copy_parameters(self):\n  \
      \  \t\t\"\"\"Copy the parameters of the online network to the offline network\"\
      \"\"\n\n    \t\tweights = self.DQN_online.get_weights()\n    \t\tself.DQN_offline.set_weights(weights)\n\
      \n\t\t\n\n    def make_grid(self,grid,channels):\n        grid = np.append(grid,\
      \ channels).reshape(self.window_size+1,7)\n        grid = np.delete(grid, 0,\
      \ 0)\n        grid = grid.reshape(self.window_size,self.number_of_channels)\n\
      \        return grid.astype(np.float32)\n  \n    def evaluate_dq_agent(self,\
      \ agent, grid):\n\n        action_history=[]\n        tot_reward = 0\n    \n\
      \        self.dq_env.initialize()\n        first_state = self.dq_env.get_init_state()\n\
      \        action = agent.choose_action(first_state, epsilon = 0)\n        action_history.append(action)\n\
      \        reward, new_state = self.dq_env.run(action)\n        tot_reward +=\
      \ reward\n    \n        for j in range(grid.shape[1]-1):\n            state\
      \ = new_state\n            action = agent.choose_action(state, epsilon = 0)\n\
      \            action_history.append(action)\n            reward, new_state =\
      \ self.dq_env.run(action)\n            tot_reward += reward\n            \n\
      \        choosen_channels = [(grid.shape[0]-1)/2]\n        for i in range(len(action_history)):\n\
      \        \tchoosen_channels.append((choosen_channels[i]+action_history[i])%grid.shape[0])\n\
      \        choosen_channels = choosen_channels[1:]\n        #plt.imshow(np.flip(grid,\
      \ axis=0), origin=\"lower\", cmap='gray', vmin=0, vmax=1)\n        #for i in\
      \ range(len(choosen_channels)):\n        #\tplt.scatter(i, grid.shape[0]-1-choosen_channels[i],\
      \ color='r')\n        #plt.show()\n        #print(str(int(tot_reward))+'/'+str(grid.shape[1])+'\
      \ packets have been transmitted')\n        return tot_reward\n        '''  \n"
    active_threshold: '0.9'
    affinity: ''
    alias: ''
    comment: ''
    epsilon: '0.25'
    gamma: '0.6'
    initial_channel: '0'
    learning_rate: '0.01'
    maxoutbuf: '0'
    minoutbuf: '0'
    num_GPU: '1'
    number_of_channels: '1'
    seed: '1'
    slot_time: '1'
    window_size: '16'
  states:
    _io_cache: ('RL Transmit agent', 'blk', [('number_of_channels', '7'), ('seed',
      '0'), ('num_GPU', '0.0'), ('active_threshold', '0.9'), ('slot_time', '1'), ('window_size',
      '5'), ('initial_channel', '0'), ('gamma', '1'), ('learning_rate', '0.01'), ('epsilon',
      '1')], [('0', 'complex', 1)], [('0', 'short', 1)], 'Embedded Python Block example
      - a simple multiply const', ['active_threshold', 'epsilon', 'gamma', 'initial_channel',
      'learning_rate', 'number_of_channels', 'slot_time', 'window_size'])
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [1297, 42]
    rotation: 0
    state: enabled
- name: epy_block_3_0_0_1
  id: epy_block
  parameters:
    _source_code: "\"\"\"\nEmbedded Python Blocks:\n\nEach time this file is saved,\
      \ GRC will instantiate the first class it finds\nto get ports and parameters\
      \ of your block. The arguments to __init__  will\nbe the parameters. All of\
      \ them are required to have default values!\n\"\"\"\n\nimport os\nimport copy\n\
      import sys      \nimport time\nimport schedule\nimport threading\nfrom datetime\
      \ import datetime, timedelta\nfrom timeslot import Timeslot  \nimport numpy\
      \ as np\nimport matplotlib.pyplot as plt\nimport torch\nimport tensorflow as\
      \ tf\nfrom gnuradio import gr\nimport pmt\nfrom tensorflow.keras import Model\n\
      from tensorflow.keras.layers import Layer, Dense, Softmax\nfrom tensorflow.keras.layers\
      \ import Dense\n\nfrom gym import Env\nfrom gym.spaces import Discrete, Box\n\
      \n\nfrom matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg\nfrom matplotlib.figure\
      \ import Figure\nfrom PyQt5 import QtCore #conda install pyqt\nfrom PyQt5 import\
      \ QtWidgets\nHISTORY_BUFFER_LEN = 2000\n\nclass blk(gr.sync_block):  # other\
      \ base classes are basic_block, decim_block, interp_block\n    \"\"\"Embedded\
      \ Python Block example - a simple multiply const\"\"\"\n\n    def __init__(self,seed\
      \ = 0, num_GPU=0.0, active_threshold= 0.9, slot_time = 1, window_size = 5, initial_channel\
      \ = 0, gamma = 1, learning_rate = 0.01, epsilon = 1):  # only default arguments\
      \ here\n        \"\"\"arguments to this function show up as parameters in GRC\"\
      \"\"\n        gr.sync_block.__init__(\n            self,\n            name='RL\
      \ 1 channel agent',   # will show up in GRC\n            in_sig=[np.complex64],\n\
      \            out_sig=[np.uint16]\n        )\n        # if an attribute with\
      \ the same name as a parameter is found,\n        # a callback is registered\
      \ (properties work, too).\n        \n        #initailize tensorflow and GPU\
      \ usage\n        print('Tensorflow version: ', tf.__version__)\n        gpus\
      \ = tf.config.experimental.list_physical_devices(\"GPU\")\n        print('Number\
      \ of GPUs available :', len(gpus))\n        \n        if num_GPU < len(gpus):\n\
      \        \ttf.config.experimental.set_visible_devices(gpus[num_GPU], 'GPU')\n\
      \        \ttf.config.experimental.set_memory_growth(gpus[num_GPU], True)\n \
      \       \tprint('Only GPU number', num_GPU, 'used')\n        tf.random.set_seed(seed)\n\
      \        \n        number_of_channels=1\n        #initialize Channel sensing\n\
      \        self.active_threshold = active_threshold\n        self.number_of_channels\
      \ = number_of_channels\n        if (self.number_of_channels+1)%2!=0:\n     \
      \   \tprint('/!\\ Please enter an even number of channels')\n        self.channels\
      \ = np.zeros(self.number_of_channels)\n        self.slot_time = slot_time\n\
      \        self.last_datetime = datetime.now()\n        self.window_size = window_size\n\
      \        self.grid = np.zeros((self.window_size, self.number_of_channels))\n\
      \        self.initial_channel = initial_channel #change to random\n        self.grid_flag\
      \ = 0\n        self.channel_decision = self.initial_channel\n        \n    \
      \    self.collision = 0\n        self.total_error = 0 \n        self.total_count\
      \ = 1 \n        #initialize DQN env\n        #self.dq_env = self.DQEnv(self.number_of_channels,\
      \ self.window_size, False, self.grid.T)\n\n        #initialize DQN Neural Network\n\
      \        self.gamma = gamma\n        self.learning_rate = learning_rate\n  \
      \      self.actions_number = self.number_of_channels\n        self.epsilon =\
      \ epsilon\n        self.nb_trainings = 300\n        self.loss = []  # Keep trak\
      \ of the losses\n        self.channel_decision = self.initial_channel\n    \
      \    self.train_flag = False\n        self.done = False\n        \n        self.agent_dq\
      \ = self.DQAgent(self.number_of_channels, self.learning_rate, self.gamma)\n\
      \        #sensed_ch, curr_ch, end = self.dq_env.get_init_state()\n        #print('##################################')\n\
      \        #print('ENV initial state:')\n        #print('State: ', sensed_ch,',\
      \ Current Channel: ', curr_ch)\n        #print('##################################')\n\
      \      \n\n    def work(self, input_items, output_items):       \t\n       \
      \ #Check slot time\n        if (datetime.now() - self.last_datetime) > timedelta(days=0,\
      \ seconds=self.slot_time, microseconds=0, milliseconds=0, minutes=0, hours=0,\
      \ weeks=0):\n            #Sample and mark channels activity\n            for\
      \ i in range(0,np.size(input_items,0)):\n            \t#Energy detected during\
      \ time slot, mark as occupied \n            \tif (np.mean(abs(input_items[i][:]))>self.active_threshold):\n\
      \            \t\tself.channels[i] = 1\n            for j in range (np.size(self.channels)):\
      \ self.channels[j] = np.abs(self.channels[j]-1)#flip ones and zeros\n      \
      \      print(self.channels,'    ', datetime.now())   \n            \n      \
      \      \n            #when training is done, make a decision\n            if\
      \ self.done == True:\n            \tstate = [self.channels.astype(np.float32),\
      \ self.dq_env.one_hot(self.channel_decision, self.number_of_channels), 0]\n\
      \            \tself.channel_decision = self.agent_dq.choose_action(state, 0)#\
      \ epsilon = 0 only expoiltation wehn not training\n            \tself.total_count\
      \ += 1\n            \tself.total_ratio = (self.total_error)/self.total_count\n\
      \            \tprint('CH: ',self.channels)\n            \ttemp = np.zeros(np.size(self.channels))\n\
      \            \ttemp[self.channel_decision]=1\n            \tprint('DC: ',temp)\n\
      \            \tprint('count: ', self.total_count, ' Errors: ', self.total_error,\
      \ ' ratio: ',self.total_ratio)\n         \t\n            \tif self.channels[self.channel_decision]\
      \ == 0:\n            \t\tself.collision += 1             \n            self.grid\
      \ = self.make_grid(self.grid,(self.channels))\n            self.last_datetime\
      \ = datetime.now()\n            self.channels = np.zeros(self.number_of_channels)\n\
      \            #print(self.grid)\n            \n            self.grid_flag +=\
      \ 1\n        output_items[0][:]= self.channel_decision      \n        if self.collision\
      \ == 1:\n        \tself.total_error += 1\n        \tself.collision = 0\n   \
      \     \t#self.grid_flag = self.window_size+1\n        \t\n\n        \t\n   \
      \     #Full window sized grid\n        if (self.grid_flag == self.window_size+1):\
      \ \n\t\t\n        \t#initialize DQN env\n        \tself.dq_env = self.DQEnv(self.number_of_channels,\
      \ self.window_size, self.initial_channel, self.grid.T)\n\t\n\t\t# Run 200 episodes\n\
      \        \tfor i in range(HISTORY_BUFFER_LEN):\n                    \n     \
      \               self.dq_env.initialize()\n                    \n           \
      \         state = self.dq_env.get_init_state()\n                    action =\
      \ self.agent_dq.choose_action(state, self.epsilon)\n                    reward,\
      \ new_state = self.dq_env.run(action)\n\t    \n                    self.agent_dq.insert_history(state,\
      \ action, reward, new_state) # Insert state, action, reward, new state in history\
      \ \n\t    \n                    # While it's not the end of the episode\n  \
      \                  while new_state[2]==0 :\n                    \tstate = new_state\n\
      \                    \taction = self.agent_dq.choose_action(state, self.epsilon)\n\
      \                    \treward, new_state = self.dq_env.run(action)\n\t\t\n \
      \                   \tself.agent_dq.insert_history(state, action, reward, new_state)\
      \ # Insert state, action, reward, new state in history\n        \t\n       \
      \ \tself.train_flag = True                      \t\n        \tprint('Replay\
      \ buffer History populated')\n        \t                 \t \n\n        \t\n\
      \        \t\n\n        if self.train_flag == True:\n\t        print('Training...')\n\
      \        \tself.train_flag = False\n        \t\n        \tfor i in range(self.nb_trainings):\n\
      \        \t\tself.dq_env.initialize()\n        \t\tstate = self.dq_env.get_init_state()\n\
      \    \n        \t\taction = self.agent_dq.choose_action(state, self.epsilon)\n\
      \        \t\treward, new_state = self.dq_env.run(action)\n        \t\tself.agent_dq.insert_history(state,\
      \ action, reward, new_state) # Insert state, action, reward, new state in history\
      \ \n        \t\tself.agent_dq.learn(64) # Each time we store a new history,\
      \ we perform a training on random data\n    \n        \t\t# While it's not the\
      \ end of the episode\n        \t\twhile new_state[2]==0 :\n        \t\t\tstate\
      \ = new_state\n        \t\t\taction = self.agent_dq.choose_action(state, self.epsilon)\n\
      \        \t\t\treward, new_state = self.dq_env.run(action)\n        \t\t\tself.agent_dq.insert_history(state,\
      \ action, reward, new_state) # Insert state, action, reward, new state in history\
      \ \n        \t\t\tself.agent_dq.learn(64) # Each time we store a new history,\
      \ we perform a training on random data\n    \n        \t\tself.loss.append(tf.reduce_mean(self.agent_dq.loss).numpy())\
      \ #Save the losses for future visualization\n        \t\t\t#Every 10 iterations\
      \ we copy the parameters of the online DQN to the offline DQN\n        \t\t\
      if (i+1)%10 == 0:\n        \t\t\tself.agent_dq.copy_parameters()\n        \t\
      \t\tprint((i+1), end=', ')\n        \tself.done = True\n        \t#plt.semilogy(np.arange(len(self.loss)),\
      \ self.loss)\n        \t#plt.show()\n        \tself.evaluate_dq_agent(self.agent_dq\
      \ , self.dq_env.grid)\n        return len(output_items[0])\n        \n\n   \
      \ \n    \n    class DQEnv():\n    \tdef __init__(self, nb_channels = 1, nb_states=2,\
      \ initial_state = 0 , grid = []):\n        \n\t    \tself.nb_ch = nb_channels\n\
      \t    \tself.nb_states = nb_states\n        \n\t    \tself.grid = grid\n\t \
      \   \tprint ('env grid:')\n\t    \tprint(self.grid)\n\t    \tself.init_state\
      \ = [int(initial_state), int(self.nb_states-1)]\n\t    \tself.ch_state = self.init_state[1]\n\
      \t    \tself.curr_ch = self.init_state[0]\n\t    \tself.sent_mess = 0\n    \
      \    \n\t    \tprint('This environment has ' + str(self.nb_ch) +' different\
      \ channels')\n        \n    \tdef run(self, action):\n        \t#not occupied\n\
      \t    \t#if self.ch_state == 1:\n\t    \tself.sent_mess += 1\n        \n\t \
      \   \tself.ch_state  = (self.ch_state + 1)%self.nb_states\n        \n\t    \t\
      self.curr_ch = action\n\t    \t\n\t    \treward = np.abs(self.grid[self.curr_ch,\
      \ self.ch_state])\n        \n\t    \tif self.sent_mess != self.nb_states: \n\
      \t    \t\tend = 0\n\t    \telse :\n\t    \t\tend = 1\n        \n\t    \treturn(reward,\
      \ [self.grid[:, self.ch_state], self.one_hot(self.curr_ch, self.nb_ch), end])\n\
      \    \n    \tdef get_init_state(self):\n\t    \treturn [self.grid[:, self.init_state[1]],\
      \ self.one_hot(self.init_state[0], self.nb_ch), 0]\n    \n    \tdef initialize(self):\n\
      \t    \tself.ch_state = self.init_state[1]\n\t    \tself.curr_ch = self.init_state[0]\n\
      \t    \t#self.sent_mess = 0\n        \n    \tdef one_hot(self, index, depth):\n\
      \t    \toh = np.zeros(depth, dtype=np.float32)\n\t    \toh[index] = 1\n\t  \
      \  \treturn oh\n    \n    \n    class DQAgent():\n    \tdef __init__(self, nb_channels,\
      \ learning_rate, gamma):\n    \t\tself.nb_ch = nb_channels\n    \t\tself.nb_actions\
      \ = nb_channels+1\n    \t\tself.gamma = gamma\n    \t\tself.learning_rate =\
      \ learning_rate\n    \t\t\n    \t\tself.history_length = HISTORY_BUFFER_LEN\n\
      \    \t\tself.history = [[]for i in range(self.history_length)]\n    \t\tself.history_idx\
      \ = 0\n    \t\t\n    \t\t#Create and initialize the online DQN\n    \t\tself.DQN_online\
      \ = tf.keras.models.Sequential([Dense(2*self.nb_ch, activation='relu'), Dense(self.nb_actions,\
      \ activation='softplus') #Outputs positive values \n    \t\t])\n    \t\tself.DQN_online.build(input_shape=(None,\
      \ self.nb_ch*2)) #Build the model to create the weights\n    \t\t\n    \t\t\
      #Create and initialize the offline DQN\n    \t\tself.DQN_offline = tf.keras.models.Sequential([Dense(2*self.nb_ch,\
      \ activation='relu'), Dense(self.nb_actions, activation='softplus') #Outputs\
      \ positive values\n    \t\t])\n    \t\tself.DQN_offline.build(input_shape=(None,\
      \ self.nb_ch*2)) #Build the model to create the weights\n    \t\t\n    \t\t\
      self.copy_parameters() #Copy the weights of the online network to the offline\
      \ network\n    \t\t\n    \t\tself.loss_func = tf.keras.losses.MSE\n    \t\t\
      self.optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n        \n  \
      \      \n    \tdef choose_action(self, state, epsilon):\n    \t\t\"\"\"Implements\
      \ an epsilon-greedy policy\"\"\"\n    \t\tsensed_ch, curr_ch, end = state\n\
      \    \t\t# Explore ?\n    \t\tif np.random.uniform(size=1) < epsilon :\n   \
      \ \t\t\taction =  np.random.randint(low = 0, high = 1) ####### Take one action\
      \ randomly {0, 1, 2, 3, 4, 5, 6} with probability epsilon\n    \t\t\t\n    \
      \    \n    \t\t#Choose the best action\n    \t\telse:    \n    \t\t\t#sensed_ch,\
      \ curr_ch, end = state #Decomposes the state\n    \t\t\t'''\n    \t\t\t* The\
      \ shapes are :\n    \t\t\t- curr_ch : [self.nb_ch]\n    \t\t\t- sensed_ch :\
      \ [self.nb_ch]\n    \t\t\t- end : one integer\n    \t\t\t'''\n    \t\t\tDQN_input\
      \ = tf.concat([sensed_ch, curr_ch], axis=0)[tf.newaxis, :] #Create a state vector,\
      \ which is the DQN input. Shape : [1, 2*self.nb_ch]\n    \t\t\toutputs = self.DQN_online(DQN_input).numpy()\
      \ #Get the predicted Q values corresponding to the 3 actions\n    \t\t\taction\
      \ = np.argmax(outputs)#-1 #Take the action that has the highest predicted Q\
      \ value (0, 1, 2, 3, 4, 5, 6)\t\t\t\n    \t\t\t#print(outputs)\n    \t\treturn\
      \ action\n    \n    \tdef learn(self, batch_size):\n    \t\t\"\"\"Sample experiences\
      \ from the history and performs SGD\"\"\"\n    \t\t\n    \t\t# Samples random\
      \ experiences from the history\n    \t\tidx = np.random.choice(range(self.history_length),\
      \ batch_size, replace=False) # Create random indexes \n    \t\trdm_exp =  [self.history[i]\
      \ for i in idx] # Take experiences corresponding to the random indexes\n   \
      \ \t\t\n    \t\t# Each experience is written in this format : [state_vec, end,\
      \ action, reward, n_state_vec, n_end] (see insert_history method)\n    \t\t\n\
      \    \t\t# Create 6 batches : states_vec, end_boolean, actions, rewards, new\
      \ states_vec, new_end_boolean\n    \t\tstates_vec = np.array([rdm_exp[i][0]\
      \ for i in range(batch_size)]) # Shape : [Bs, 2*self.nb_ch]\n    \t\tend = np.array([rdm_exp[i][1]\
      \ for i in range(batch_size)]) # Shape : [Bs]\n    \t\tactions = np.array([rdm_exp[i][2]\
      \ for i in range(batch_size)]) # Shape : [BS]\n    \t\trewards = np.array([rdm_exp[i][3]\
      \ for i in range(batch_size)]) # Shape : [BS]\n    \t\tn_states_vec = np.array([rdm_exp[i][4]\
      \ for i in range(batch_size)]) # Shape : [Bs, 2*self.nb_ch]\n    \t\tn_end =\
      \ np.array([rdm_exp[i][5] for i in range(batch_size)]) # Shape : [BS]\n    \t\
      \t\n    \t\t#Compute the best q_value for the new states\n    \t\tmax_n_q_values\
      \ = tf.reduce_max(self.DQN_offline(n_states_vec), axis=1).numpy()\n\n    \t\t\
      with tf.GradientTape() as tape:\n    \t\t\t#Forward pass through the online\
      \ network to predict the q_values\n    \t\t\tpred_q_values = self.DQN_online(states_vec)\n\
      \    \t\t\t\n    \t\t\t# Compute targets\n    \t\t\ttargets = pred_q_values.numpy()\n\
      \    \t\t\ttargets[np.arange(targets.shape[0]), actions]= rewards + (1-n_end)\
      \ * self.gamma * max_n_q_values\n    \t\t\t\n    \t\t\t# Evaluate the loss\n\
      \    \t\t\tself.loss = self.loss_func(pred_q_values, targets)\n        \n  \
      \  \t\t# Compute gradients and perform the gradient descent\n    \t\tgradients\
      \ = tape.gradient(self.loss, self.DQN_online.trainable_weights)\n    \t\tself.optimizer.apply_gradients(zip(gradients,\
      \ self.DQN_online.trainable_weights))  \n    \n    \tdef insert_history(self,\
      \ state, action, reward, n_state):\n    \t\t\"\"\"Insert experience in history\"\
      \"\"\n        \n    \t\tsensed_ch, curr_ch, end = state\n    \t\tstate_vec =\
      \ np.concatenate([sensed_ch, curr_ch], axis=0) # Create the state vector for\
      \ the state\n        \n    \t\tn_sensed_ch, n_curr_ch, n_end = n_state\n   \
      \ \t\tn_state_vec = np.concatenate([n_sensed_ch, n_curr_ch], axis=0) # Create\
      \ the state vector for the new state\n\n    \t\tself.history[self.history_idx]\
      \ = [state_vec, end, action, reward, n_state_vec, n_end] # Insert everything\
      \ in the history\n        \n    \t\tself.history_idx = (self.history_idx+1)%self.history_length\
      \ # Move the history_idx by one\n    \n    \tdef copy_parameters(self):\n  \
      \  \t\t\"\"\"Copy the parameters of the online network to the offline network\"\
      \"\"\n\n    \t\tweights = self.DQN_online.get_weights()\n    \t\tself.DQN_offline.set_weights(weights)\n\
      \n\t\t\n\n    def make_grid(self,grid,channels):\n        grid = np.append(grid,\
      \ channels).reshape(self.window_size+1,self.number_of_channels)\n        grid\
      \ = np.delete(grid, 0, 0)\n        grid = grid.reshape(self.window_size,self.number_of_channels)\n\
      \        return grid.astype(np.float32)\n    \n    def evaluate_dq_agent(self,\
      \ agent, grid):\n\n        action_history=[]\n        tot_reward = 0\n    \n\
      \        self.dq_env.initialize()\n        first_state = self.dq_env.get_init_state()\n\
      \        action = agent.choose_action(first_state, epsilon = 0)\n        action_history.append(action)\n\
      \        reward, new_state = self.dq_env.run(action)\n        tot_reward +=\
      \ reward\n    \n        for j in range(grid.shape[1]-1):\n            state\
      \ = new_state\n            action = agent.choose_action(state, epsilon = 0)\n\
      \            action_history.append(action)\n            reward, new_state =\
      \ self.dq_env.run(action)\n            tot_reward += reward\n            \n\
      \        choosen_channels = [(grid.shape[0]-1)/2]\n        for i in range(len(action_history)):\n\
      \        \tchoosen_channels.append((choosen_channels[i]+action_history[i])%grid.shape[0])\n\
      \        choosen_channels = choosen_channels[1:]\n        #plt.imshow(np.flip(grid,\
      \ axis=0), origin=\"lower\", cmap='gray', vmin=0, vmax=1)\n        #for i in\
      \ range(len(choosen_channels)):\n        #\tplt.scatter(i, grid.shape[0]-1-choosen_channels[i],\
      \ color='r')\n        #plt.show()\n        #print(str(int(tot_reward))+'/'+str(grid.shape[1])+'\
      \ packets have been transmitted')\n        return tot_reward\n"
    active_threshold: '0.9'
    affinity: ''
    alias: ''
    comment: ''
    epsilon: '0.25'
    gamma: '0.6'
    initial_channel: '0'
    learning_rate: '0.01'
    maxoutbuf: '0'
    minoutbuf: '0'
    num_GPU: '1'
    seed: '1'
    slot_time: '1'
    window_size: '128'
  states:
    _io_cache: ('RL 1 channel agent', 'blk', [('seed', '0'), ('num_GPU', '0.0'), ('active_threshold',
      '0.9'), ('slot_time', '1'), ('window_size', '5'), ('initial_channel', '0'),
      ('gamma', '1'), ('learning_rate', '0.01'), ('epsilon', '1')], [('0', 'complex',
      1)], [('0', 'short', 1)], 'Embedded Python Block example - a simple multiply
      const', ['active_threshold', 'epsilon', 'gamma', 'initial_channel', 'learning_rate',
      'slot_time', 'window_size'])
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [1476, 1098]
    rotation: 0
    state: disabled
- name: epy_block_4
  id: epy_block
  parameters:
    _source_code: "\"\"\"\nEmbedded Python Blocks:\n\nEach time this file is saved,\
      \ GRC will instantiate the first class it finds\nto get ports and parameters\
      \ of your block. The arguments to __init__  will\nbe the parameters. All of\
      \ them are required to have default values!\n\"\"\"\n\nimport numpy as np\n\
      from gnuradio import gr\nimport time\nfrom datetime import datetime, timedelta\n\
      \nclass blk(gr.sync_block):  # other base classes are basic_block, decim_block,\
      \ interp_block\n    \"\"\"Embedded Python Block example - a simple multiply\
      \ const\"\"\"\n\n    def __init__(self, example_param=1.0):  # only default\
      \ arguments here\n        \"\"\"arguments to this function show up as parameters\
      \ in GRC\"\"\"\n        gr.sync_block.__init__(\n            self,\n       \
      \     name='Selected Channel',   # will show up in GRC\n            in_sig=[np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.uint16],\n\
      \            out_sig=[np.complex64]\n        )\n        # if an attribute with\
      \ the same name as a parameter is found,\n        # a callback is registered\
      \ (properties work, too).\n        self.example_param = example_param\n    \
      \    self.last_datetime = datetime.now()\n        self.index = 0\n\n    def\
      \ work(self, input_items, output_items):\n        \"\"\"example: multiply with\
      \ constant\"\"\"\n        if (datetime.now() - self.last_datetime) > timedelta(days=0,\
      \ seconds=1, microseconds=0, milliseconds=0, minutes=0, hours=0, weeks=0):\n\
      \            #when training is done, make a decision  \n            self.last_datetime\
      \ = datetime.now()\n            self.index = input_items[7][0]\n           \
      \ #print(self.index)\n            if self.index < 0 or self.index > 6:\n   \
      \         \tself.index = 0\n\n        output_items[0][:] = input_items[self.index][:]\n\
      \        return len(output_items[0])\n"
    affinity: ''
    alias: ''
    comment: ''
    example_param: '1.0'
    maxoutbuf: '0'
    minoutbuf: '0'
  states:
    _io_cache: ('Selected Channel', 'blk', [('example_param', '1.0')], [('0', 'complex',
      1), ('1', 'complex', 1), ('2', 'complex', 1), ('3', 'complex', 1), ('4', 'complex',
      1), ('5', 'complex', 1), ('6', 'complex', 1), ('7', 'short', 1)], [('0', 'complex',
      1)], 'Embedded Python Block example - a simple multiply const', ['example_param'])
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [1512, 666]
    rotation: 0
    state: true
- name: note_0
  id: note
  parameters:
    alias: ''
    comment: ''
    note: Deterministic channals
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [466, 558]
    rotation: 0
    state: enabled
- name: note_0_0
  id: note
  parameters:
    alias: ''
    comment: ''
    note: Probalistic channals
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [470, 838]
    rotation: 0
    state: enabled
- name: note_0_1
  id: note
  parameters:
    alias: ''
    comment: ''
    note: Exponential intervals
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [463, 262]
    rotation: 0
    state: enabled
- name: qtgui_freq_sink_x_0
  id: qtgui_freq_sink_x
  parameters:
    affinity: ''
    alias: ''
    alpha1: '1.0'
    alpha10: '1.0'
    alpha2: '1.0'
    alpha3: '1.0'
    alpha4: '1.0'
    alpha5: '1.0'
    alpha6: '1.0'
    alpha7: '1.0'
    alpha8: '10.0'
    alpha9: '1.0'
    autoscale: 'False'
    average: '1.0'
    axislabels: 'True'
    bw: samp_rate
    color1: '"blue"'
    color10: '"dark blue"'
    color2: '"red"'
    color3: '"green"'
    color4: '"black"'
    color5: '"cyan"'
    color6: '"magenta"'
    color7: '"yellow"'
    color8: '"dark blue"'
    color9: '"dark green"'
    comment: ''
    ctrlpanel: 'False'
    fc: '0'
    fftsize: '1024'
    freqhalf: 'True'
    grid: 'False'
    gui_hint: ''
    label: Relative Gain
    label1: CH1
    label10: ''''''
    label2: CH2
    label3: CH3
    label4: CH4
    label5: CH5
    label6: CH6
    label7: CH7
    label8: '''CHOISE'''
    label9: ''''''
    legend: 'True'
    maxoutbuf: '0'
    minoutbuf: '0'
    name: '""'
    nconnections: '8'
    showports: 'False'
    tr_chan: '0'
    tr_level: '0.0'
    tr_mode: qtgui.TRIG_MODE_FREE
    tr_tag: '""'
    type: complex
    units: dB
    update_time: '0.10'
    width1: '1'
    width10: '1'
    width2: '1'
    width3: '1'
    width4: '1'
    width5: '1'
    width6: '1'
    width7: '1'
    width8: '1'
    width9: '1'
    wintype: firdes.WIN_BLACKMAN_hARRIS
    ymax: '10'
    ymin: '-140'
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [1938, 101]
    rotation: 0
    state: true
- name: qtgui_number_sink_0
  id: qtgui_number_sink
  parameters:
    affinity: ''
    alias: ''
    autoscale: 'False'
    avg: '0'
    color1: ("black", "black")
    color10: ("black", "black")
    color2: ("black", "black")
    color3: ("black", "black")
    color4: ("black", "black")
    color5: ("black", "black")
    color6: ("black", "black")
    color7: ("black", "black")
    color8: ("black", "black")
    color9: ("black", "black")
    comment: ''
    factor1: '1'
    factor10: '1'
    factor2: '1'
    factor3: '1'
    factor4: '1'
    factor5: '1'
    factor6: '1'
    factor7: '1'
    factor8: '1'
    factor9: '1'
    graph_type: qtgui.NUM_GRAPH_HORIZ
    gui_hint: ''
    label1: ''
    label10: ''
    label2: ''
    label3: ''
    label4: ''
    label5: ''
    label6: ''
    label7: ''
    label8: ''
    label9: ''
    max: '7'
    min: '0'
    name: '""'
    nconnections: '1'
    type: short
    unit1: ''
    unit10: ''
    unit2: ''
    unit3: ''
    unit4: ''
    unit5: ''
    unit6: ''
    unit7: ''
    unit8: ''
    unit9: ''
    update_time: '0.10'
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [2114, 1329]
    rotation: 0
    state: true
- name: qtgui_waterfall_sink_x_0
  id: qtgui_waterfall_sink_x
  parameters:
    affinity: ''
    alias: ''
    alpha1: '0.5'
    alpha10: '1.0'
    alpha2: '0.5'
    alpha3: '1.0'
    alpha4: '1.0'
    alpha5: '1.0'
    alpha6: '1.0'
    alpha7: '1.0'
    alpha8: '1.0'
    alpha9: '1.0'
    axislabels: 'True'
    bw: samp_rate
    color1: '0'
    color10: '0'
    color2: '6'
    color3: '0'
    color4: '0'
    color5: '0'
    color6: '0'
    color7: '0'
    color8: '0'
    color9: '0'
    comment: ''
    fc: '0'
    fftsize: '1024'
    freqhalf: 'True'
    grid: 'True'
    gui_hint: ''
    int_max: '1'
    int_min: '-200'
    label1: ''
    label10: ''
    label2: ''
    label3: ''
    label4: ''
    label5: ''
    label6: ''
    label7: ''
    label8: ''
    label9: ''
    legend: 'True'
    maxoutbuf: '0'
    minoutbuf: '0'
    name: '""'
    nconnections: '2'
    showports: 'False'
    type: complex
    update_time: '0.10'
    wintype: firdes.WIN_BLACKMAN_hARRIS
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [1936, 746]
    rotation: 0
    state: true

connections:
- [analog_sig_source_x_0, '0', epy_block_1, '0']
- [analog_sig_source_x_0, '0', epy_block_1_0, '0']
- [analog_sig_source_x_0, '0', epy_block_1_1, '0']
- [analog_sig_source_x_0, '0', epy_block_4, '0']
- [analog_sig_source_x_0_0, '0', epy_block_1, '1']
- [analog_sig_source_x_0_0, '0', epy_block_1_0, '1']
- [analog_sig_source_x_0_0, '0', epy_block_1_1, '1']
- [analog_sig_source_x_0_0, '0', epy_block_4, '1']
- [analog_sig_source_x_0_0_0, '0', epy_block_1, '2']
- [analog_sig_source_x_0_0_0, '0', epy_block_1_0, '2']
- [analog_sig_source_x_0_0_0, '0', epy_block_1_1, '2']
- [analog_sig_source_x_0_0_0, '0', epy_block_4, '2']
- [analog_sig_source_x_0_0_0_0, '0', epy_block_1, '3']
- [analog_sig_source_x_0_0_0_0, '0', epy_block_1_0, '3']
- [analog_sig_source_x_0_0_0_0, '0', epy_block_1_1, '3']
- [analog_sig_source_x_0_0_0_0, '0', epy_block_4, '3']
- [analog_sig_source_x_0_0_0_0_0, '0', epy_block_1, '4']
- [analog_sig_source_x_0_0_0_0_0, '0', epy_block_1_0, '4']
- [analog_sig_source_x_0_0_0_0_0, '0', epy_block_1_1, '4']
- [analog_sig_source_x_0_0_0_0_0, '0', epy_block_4, '4']
- [analog_sig_source_x_0_0_0_0_0_0, '0', epy_block_1, '5']
- [analog_sig_source_x_0_0_0_0_0_0, '0', epy_block_1_0, '5']
- [analog_sig_source_x_0_0_0_0_0_0, '0', epy_block_1_1, '5']
- [analog_sig_source_x_0_0_0_0_0_0, '0', epy_block_4, '5']
- [analog_sig_source_x_0_0_0_0_0_0_0, '0', epy_block_1, '6']
- [analog_sig_source_x_0_0_0_0_0_0_0, '0', epy_block_1_0, '6']
- [analog_sig_source_x_0_0_0_0_0_0_0, '0', epy_block_1_1, '6']
- [analog_sig_source_x_0_0_0_0_0_0_0, '0', epy_block_4, '6']
- [blocks_add_xx_0_0_0_0_0_0_0_0, '0', qtgui_waterfall_sink_x_0, '0']
- [channels_channel_model_0, '0', blocks_add_xx_0_0_0_0_0_0_0_0, '0']
- [channels_channel_model_0, '0', epy_block_3_0, '0']
- [channels_channel_model_0, '0', epy_block_3_0_0, '0']
- [channels_channel_model_0, '0', epy_block_3_0_0_0, '0']
- [channels_channel_model_0, '0', epy_block_3_0_0_0_0, '0']
- [channels_channel_model_0, '0', qtgui_freq_sink_x_0, '0']
- [channels_channel_model_0_0, '0', blocks_add_xx_0_0_0_0_0_0_0_0, '1']
- [channels_channel_model_0_0, '0', epy_block_3_0, '1']
- [channels_channel_model_0_0, '0', epy_block_3_0_0, '1']
- [channels_channel_model_0_0, '0', qtgui_freq_sink_x_0, '1']
- [channels_channel_model_0_0_0, '0', blocks_add_xx_0_0_0_0_0_0_0_0, '2']
- [channels_channel_model_0_0_0, '0', epy_block_3_0, '2']
- [channels_channel_model_0_0_0, '0', epy_block_3_0_0, '2']
- [channels_channel_model_0_0_0, '0', qtgui_freq_sink_x_0, '2']
- [channels_channel_model_0_0_0_0, '0', blocks_add_xx_0_0_0_0_0_0_0_0, '3']
- [channels_channel_model_0_0_0_0, '0', epy_block_3_0, '3']
- [channels_channel_model_0_0_0_0, '0', epy_block_3_0_0, '3']
- [channels_channel_model_0_0_0_0, '0', qtgui_freq_sink_x_0, '3']
- [channels_channel_model_0_0_0_0_0, '0', blocks_add_xx_0_0_0_0_0_0_0_0, '4']
- [channels_channel_model_0_0_0_0_0, '0', epy_block_3_0, '4']
- [channels_channel_model_0_0_0_0_0, '0', epy_block_3_0_0, '4']
- [channels_channel_model_0_0_0_0_0, '0', qtgui_freq_sink_x_0, '4']
- [channels_channel_model_0_0_0_0_0_0, '0', blocks_add_xx_0_0_0_0_0_0_0_0, '5']
- [channels_channel_model_0_0_0_0_0_0, '0', epy_block_3_0, '5']
- [channels_channel_model_0_0_0_0_0_0, '0', epy_block_3_0_0, '5']
- [channels_channel_model_0_0_0_0_0_0, '0', qtgui_freq_sink_x_0, '5']
- [channels_channel_model_0_0_0_0_0_0_0, '0', blocks_add_xx_0_0_0_0_0_0_0_0, '6']
- [channels_channel_model_0_0_0_0_0_0_0, '0', epy_block_3_0, '6']
- [channels_channel_model_0_0_0_0_0_0_0, '0', epy_block_3_0_0, '6']
- [channels_channel_model_0_0_0_0_0_0_0, '0', epy_block_3_0_0_1, '0']
- [channels_channel_model_0_0_0_0_0_0_0, '0', qtgui_freq_sink_x_0, '6']
- [channels_channel_model_0_1, '0', qtgui_freq_sink_x_0, '7']
- [channels_channel_model_0_1, '0', qtgui_waterfall_sink_x_0, '1']
- [epy_block_1, '0', channels_channel_model_0, '0']
- [epy_block_1, '1', channels_channel_model_0_0, '0']
- [epy_block_1, '2', channels_channel_model_0_0_0, '0']
- [epy_block_1, '3', channels_channel_model_0_0_0_0, '0']
- [epy_block_1, '4', channels_channel_model_0_0_0_0_0, '0']
- [epy_block_1, '5', channels_channel_model_0_0_0_0_0_0, '0']
- [epy_block_1, '6', channels_channel_model_0_0_0_0_0_0_0, '0']
- [epy_block_1_0, '0', channels_channel_model_0, '0']
- [epy_block_1_0, '1', channels_channel_model_0_0, '0']
- [epy_block_1_0, '2', channels_channel_model_0_0_0, '0']
- [epy_block_1_0, '3', channels_channel_model_0_0_0_0, '0']
- [epy_block_1_0, '4', channels_channel_model_0_0_0_0_0, '0']
- [epy_block_1_0, '5', channels_channel_model_0_0_0_0_0_0, '0']
- [epy_block_1_0, '6', channels_channel_model_0_0_0_0_0_0_0, '0']
- [epy_block_1_1, '0', channels_channel_model_0, '0']
- [epy_block_1_1, '1', channels_channel_model_0_0, '0']
- [epy_block_1_1, '2', channels_channel_model_0_0_0, '0']
- [epy_block_1_1, '3', channels_channel_model_0_0_0_0, '0']
- [epy_block_1_1, '4', channels_channel_model_0_0_0_0_0, '0']
- [epy_block_1_1, '5', channels_channel_model_0_0_0_0_0_0, '0']
- [epy_block_1_1, '6', channels_channel_model_0_0_0_0_0_0_0, '0']
- [epy_block_3_0, '0', epy_block_4, '7']
- [epy_block_3_0, '0', qtgui_number_sink_0, '0']
- [epy_block_3_0_0, '0', epy_block_4, '7']
- [epy_block_3_0_0, '0', qtgui_number_sink_0, '0']
- [epy_block_3_0_0_0, '0', epy_block_4, '7']
- [epy_block_3_0_0_0, '0', qtgui_number_sink_0, '0']
- [epy_block_3_0_0_0_0, '0', epy_block_4, '7']
- [epy_block_3_0_0_0_0, '0', qtgui_number_sink_0, '0']
- [epy_block_3_0_0_1, '0', epy_block_4, '7']
- [epy_block_3_0_0_1, '0', qtgui_number_sink_0, '0']
- [epy_block_4, '0', channels_channel_model_0_1, '0']

metadata:
  file_format: 1
