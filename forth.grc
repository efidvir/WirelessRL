options:
  parameters:
    author: lab512
    category: '[GRC Hier Blocks]'
    cmake_opt: ''
    comment: ''
    copyright: ''
    description: ''
    gen_cmake: 'On'
    gen_linking: dynamic
    generate_options: qt_gui
    hier_block_src_path: '.:'
    id: forth
    max_nouts: '0'
    output_language: python
    placement: (0,0)
    qt_qss_theme: ''
    realtime_scheduling: ''
    run: 'True'
    run_command: '{python} -u {filename}'
    run_options: prompt
    sizing_mode: fixed
    thread_safe_setters: ''
    title: Tx RL Channel selector
    window_size: ''
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [8, 8]
    rotation: 0
    state: enabled

blocks:
- name: samp_rate
  id: variable
  parameters:
    comment: ''
    value: '32000'
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [9, 102]
    rotation: 0
    state: enabled
- name: analog_sig_source_x_0
  id: analog_sig_source_x
  parameters:
    affinity: ''
    alias: ''
    amp: '1'
    comment: ''
    freq: '1000'
    maxoutbuf: '0'
    minoutbuf: '0'
    offset: '0'
    phase: '0'
    samp_rate: samp_rate
    type: complex
    waveform: analog.GR_COS_WAVE
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [204, 9]
    rotation: 0
    state: true
- name: analog_sig_source_x_0_0
  id: analog_sig_source_x
  parameters:
    affinity: ''
    alias: ''
    amp: '1'
    comment: ''
    freq: '2000'
    maxoutbuf: '0'
    minoutbuf: '0'
    offset: '0'
    phase: '0'
    samp_rate: samp_rate
    type: complex
    waveform: analog.GR_COS_WAVE
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [206, 136]
    rotation: 0
    state: true
- name: analog_sig_source_x_0_0_0
  id: analog_sig_source_x
  parameters:
    affinity: ''
    alias: ''
    amp: '1'
    comment: ''
    freq: '3000'
    maxoutbuf: '0'
    minoutbuf: '0'
    offset: '0'
    phase: '0'
    samp_rate: samp_rate
    type: complex
    waveform: analog.GR_COS_WAVE
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [206, 263]
    rotation: 0
    state: true
- name: analog_sig_source_x_0_0_0_0
  id: analog_sig_source_x
  parameters:
    affinity: ''
    alias: ''
    amp: '1'
    comment: ''
    freq: '4000'
    maxoutbuf: '0'
    minoutbuf: '0'
    offset: '0'
    phase: '0'
    samp_rate: samp_rate
    type: complex
    waveform: analog.GR_COS_WAVE
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [206, 389]
    rotation: 0
    state: true
- name: analog_sig_source_x_0_0_0_0_0
  id: analog_sig_source_x
  parameters:
    affinity: ''
    alias: ''
    amp: '1'
    comment: ''
    freq: '5000'
    maxoutbuf: '0'
    minoutbuf: '0'
    offset: '0'
    phase: '0'
    samp_rate: samp_rate
    type: complex
    waveform: analog.GR_COS_WAVE
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [205, 513]
    rotation: 0
    state: true
- name: analog_sig_source_x_0_0_0_0_0_0
  id: analog_sig_source_x
  parameters:
    affinity: ''
    alias: ''
    amp: '1'
    comment: ''
    freq: '6000'
    maxoutbuf: '0'
    minoutbuf: '0'
    offset: '0'
    phase: '0'
    samp_rate: samp_rate
    type: complex
    waveform: analog.GR_COS_WAVE
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [205, 640]
    rotation: 0
    state: true
- name: analog_sig_source_x_0_0_0_0_0_0_0
  id: analog_sig_source_x
  parameters:
    affinity: ''
    alias: ''
    amp: '1'
    comment: ''
    freq: '7000'
    maxoutbuf: '0'
    minoutbuf: '0'
    offset: '0'
    phase: '0'
    samp_rate: samp_rate
    type: complex
    waveform: analog.GR_COS_WAVE
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [206, 765]
    rotation: 0
    state: true
- name: blocks_add_xx_0_0_0_0_0_0_0_0
  id: blocks_add_xx
  parameters:
    affinity: ''
    alias: ''
    comment: ''
    maxoutbuf: '0'
    minoutbuf: '0'
    num_inputs: '7'
    type: complex
    vlen: '1'
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [1265, 20]
    rotation: 90
    state: true
- name: blocks_null_sink_0
  id: blocks_null_sink
  parameters:
    affinity: ''
    alias: ''
    bus_structure_sink: '[[0,],]'
    comment: ''
    num_inputs: '1'
    type: short
    vlen: '1'
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [1873, 655]
    rotation: 0
    state: disabled
- name: channels_channel_model_0
  id: channels_channel_model
  parameters:
    affinity: ''
    alias: ''
    block_tags: 'False'
    comment: ''
    epsilon: '1.0'
    freq_offset: '0.001'
    maxoutbuf: '0'
    minoutbuf: '0'
    noise_voltage: '0.1'
    seed: '0'
    taps: 1.0 + 1.0j
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [720, 7]
    rotation: 0
    state: true
- name: channels_channel_model_0_0
  id: channels_channel_model
  parameters:
    affinity: ''
    alias: ''
    block_tags: 'False'
    comment: ''
    epsilon: '1.0'
    freq_offset: '0.001'
    maxoutbuf: '0'
    minoutbuf: '0'
    noise_voltage: '0.1'
    seed: '1'
    taps: 1.0 + 1.0j
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [720, 136]
    rotation: 0
    state: true
- name: channels_channel_model_0_0_0
  id: channels_channel_model
  parameters:
    affinity: ''
    alias: ''
    block_tags: 'False'
    comment: ''
    epsilon: '1.0'
    freq_offset: '0.001'
    maxoutbuf: '0'
    minoutbuf: '0'
    noise_voltage: '0.1'
    seed: '2'
    taps: 1.0 + 1.0j
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [719, 265]
    rotation: 0
    state: true
- name: channels_channel_model_0_0_0_0
  id: channels_channel_model
  parameters:
    affinity: ''
    alias: ''
    block_tags: 'False'
    comment: ''
    epsilon: '1.0'
    freq_offset: '0.001'
    maxoutbuf: '0'
    minoutbuf: '0'
    noise_voltage: '0.1'
    seed: '3'
    taps: 1.0 + 1.0j
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [720, 398]
    rotation: 0
    state: true
- name: channels_channel_model_0_0_0_0_0
  id: channels_channel_model
  parameters:
    affinity: ''
    alias: ''
    block_tags: 'False'
    comment: ''
    epsilon: '1.0'
    freq_offset: '0.001'
    maxoutbuf: '0'
    minoutbuf: '0'
    noise_voltage: '0.1'
    seed: '4'
    taps: 1.0 + 1.0j
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [721, 528]
    rotation: 0
    state: true
- name: channels_channel_model_0_0_0_0_0_0
  id: channels_channel_model
  parameters:
    affinity: ''
    alias: ''
    block_tags: 'False'
    comment: ''
    epsilon: '1.0'
    freq_offset: '0.001'
    maxoutbuf: '0'
    minoutbuf: '0'
    noise_voltage: '0.1'
    seed: '5'
    taps: 1.0 + 1.0j
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [723, 661]
    rotation: 0
    state: true
- name: channels_channel_model_0_0_0_0_0_0_0
  id: channels_channel_model
  parameters:
    affinity: ''
    alias: ''
    block_tags: 'False'
    comment: ''
    epsilon: '1.0'
    freq_offset: '0.001'
    maxoutbuf: '0'
    minoutbuf: '0'
    noise_voltage: '0.1'
    seed: '6'
    taps: 1.0 + 1.0j
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [725, 790]
    rotation: 0
    state: true
- name: channels_channel_model_0_1
  id: channels_channel_model
  parameters:
    affinity: ''
    alias: ''
    block_tags: 'False'
    comment: ''
    epsilon: '1.0'
    freq_offset: '0.001'
    maxoutbuf: '0'
    minoutbuf: '0'
    noise_voltage: '0.1'
    seed: '11'
    taps: 0.5 + 0.5j
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [1801, 244]
    rotation: 0
    state: true
- name: epy_block_0
  id: epy_block
  parameters:
    _source_code: "import os\nimport copy\nimport sys      \nimport time\nimport schedule\n\
      import threading\nfrom datetime import datetime, timedelta\nfrom timeslot import\
      \ Timeslot  \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\n\
      import tensorflow as tf\nfrom gnuradio import gr\nimport pmt\nfrom tensorflow.keras\
      \ import Model\nfrom tensorflow.keras.layers import Layer, Dense, Softmax\n\
      from tensorflow.keras.layers import Dense\n\nfrom gym import Env\n\n# Number\
      \ of messages - Dict from MAC\n#M = 8\n\n\nclass blk(gr.sync_block):  # other\
      \ base classes are basic_block, decim_block, interp_block\n\t\"\"\"Embedded\
      \ Python Block example - a simple multiply const\"\"\"\n\n\tdef __init__(self,number_of_channels=7,seed\
      \ = 0, num_GPU=0.0, active_threshold= 0.9, slot_time = 1, window_size = 5, gamma\
      \ = 1, learning_rate = 0.01):  # only default arguments here\n\t\t\"\"\"arguments\
      \ to this function show up as parameters in GRC\"\"\"\n\t\tgr.sync_block.__init__(\n\
      \t\t\tself,\n\t\t\tname='RL Select channel',   # will show up in GRC\n\t\t\t\
      in_sig=[np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64],\n\
      \t\t\tout_sig=[np.int16]\n\t\t)\n\t\t\n\n\t\t#initailize tensorflow and GPU\
      \ usage\n\t\tprint('Tensorflow version: ', tf.__version__)\n\n\t\tgpus = tf.config.experimental.list_physical_devices(\"\
      GPU\")\n\t\tprint('Number of GPUs available :', len(gpus))\n\t\t\n\t\tif num_GPU\
      \ < len(gpus):\n\t\t    tf.config.experimental.set_visible_devices(gpus[num_GPU],\
      \ 'GPU')\n\t\t    tf.config.experimental.set_memory_growth(gpus[num_GPU], True)\n\
      \t\t    print('Only GPU number', num_GPU, 'used')\n\n\t\t#initialize Channel\
      \ sensing\n\t\tself.active_threshold = active_threshold\n\t\tself.number_of_channels\
      \ = number_of_channels\n\t\tif (self.number_of_channels+1)%2!=0:\n         \
      \   \t\tprint('/!\\ Please enter an even number of channels')\n\t\tself.channels\
      \ = np.zeros(self.number_of_channels)\n\t\tself.slot_time = slot_time\n\t\t\
      self.last_datetime = datetime.now()\n\t\tself.window_size = window_size\n\t\t\
      self.grid = np.zeros((self.window_size, self.number_of_channels))\n\n\t\t\n\t\
      \t#initialize DQN env\n\t\t\n\t\t#initialize DQN Neural Network\n\t\tself.gamma\
      \ = gamma\n\t\tself.learning_rate = learning_rate\n\t\tself.actions_number =\
      \ self.number_of_channels\n\t\tself.history_length = 1000 # Initialise history\
      \ size\n\t\t# The history is a list of lists. Each inner list will contain an\
      \ experience\n\t\tself.history = [[]for i in range(self.history_length)] \n\t\
      \t# Keeps a history index : when we reach the end of the buffer, we satr to\
      \ refill from the beginning (FIFO)\n\t\tself.history_idx = 0 \n\t\t\n\t\t#Create\
      \ and initialize the online DQN\n\t\tself.DQN_online = tf.keras.models.Sequential([\n\
      \            \tDense(2*self.number_of_channels, activation='relu'),\n      \
      \      \tDense(self.actions_number, activation='softplus') #Outputs positive\
      \ values\n\t\t])\n\t\tself.DQN_online.build(input_shape=(None, self.number_of_channels*2))\
      \ #Build the model to create the weights\n\t\t\n\t\t#Create and initialize the\
      \ offline DQN\n\t\tself.DQN_offline = tf.keras.models.Sequential([\n\t\tDense(2*self.number_of_channels,\
      \ activation='relu'),\n\t\tDense(self.actions_number, activation='softplus')\
      \ #Outputs positive values\n\t\t])\n\t\tself.DQN_offline.build(input_shape=(None,\
      \ self.number_of_channels*2)) #Build the model to create the weights\n\t\t\n\
      \t\tself.copy_parameters() #Copy the weights of the online network to the offline\
      \ network\n\t\t\n\t\tself.loss_func = tf.keras.losses.MSE\n\t\tself.optimizer\
      \ = tf.keras.optimizers.Adam(self.learning_rate)\n\t\t\n\t\t\n\t\t#Get initial\
      \ state\n\t\tdq_env = DQEnv(random=False)\n\t\tdq_env.initialize()\n\t\t\n\t\
      \tprint(dq_env.get_init_state())\n\t\t\n\t\t#agent_dq = DQAgent(nb_channels=7,\
      \ learning_rate=0.01, gamma=1)\n\t\t\n\t\tself.init_DQN_flag = 0\n\t\tself.seed\
      \ = seed\n\t\tself.init_state = [int((self.number_of_channels-1)/2), int(self.window_size-1)]\n\
      \t\tself.ch_state = self.init_state[1]\n\t\tself.curr_ch = self.init_state[0]\n\
      \t\tself.sent_mess = 0\n\t\ttf.random.set_seed(seed)\n\t\t\n\t\t# init channels\
      \ states to 0\n\t\tfor i in range(0,self.number_of_channels): \n\t\t\tself.channels[i]\
      \ = 0\n\t\t\n\t\tself.initialize_DQN()\n\t\t\n\t\t\n\t#runtime code\n\tdef work(self,\
      \ input_items, output_items):\n\t\t\"\"\"example: multiply with constant\"\"\
      \"\n\t\t#sample channels\n\t\tfor i in range(0,np.size(input_items,0)):\n\t\t\
      \t\n\t\t\t#check slot time\n\t\t\tif datetime.now() - self.last_datetime > timedelta(days=0,\
      \ seconds=self.slot_time, microseconds=0, milliseconds=0, minutes=0, hours=0,\
      \ weeks=0):\n\t\t\t\tself.grid = self.make_grid(self.grid,self.channels)\n\t\
      \t\t\tself.last_datetime = datetime.now()\n\t\t\t\tself.channels = np.zeros(self.number_of_channels)\n\
      \t\t\t\tprint(self.grid.T)\n\t\t\tif (np.mean(input_items[i][:])>self.active_threshold):\n\
      \t\t\t\tself.channels[i] = 1\n\t\t\t\n\t\t\t#FIFO push channels experience to\
      \ replay buffer\t\n\t\t\t\t\t\t\n\t\t\toutput_items[0][:] = input_items[i]\n\
      \t\treturn len(output_items[0])\n\n\tdef copy_parameters(self):\n\t\t\"\"\"\
      Copy the parameters of the online network to the offline network\"\"\"\n\n\t\
      \tweights = self.DQN_online.get_weights()\n\t\tself.DQN_offline.set_weights(weights)\n\
      \n\tdef make_grid(self,grid,channels):\n\t\tgrid = np.append(grid, channels).reshape(self.window_size+1,7)\n\
      \t\tgrid = np.delete(grid, 0, 0)\n\t\tgrid = grid.reshape(self.window_size,self.number_of_channels)\n\
      \t\treturn grid.astype(np.float32)\n        \n\tdef run(self, action):\n\t\t\
      self.sent_mess += 1\n\t\tself.ch_state  = (self.ch_state + 1)%self.window_size\n\
      \t\tself.curr_ch = (self.curr_ch + action)%self.number_of_channels\n\t\treward\
      \ = self.grid.T[self.curr_ch, self.ch_state]\n\t\tif self.sent_mess != self.window_size:\
      \ \n\t\t    end = 0\n\t\telse :\n\t\t    end = 1\n\t\treturn(reward, [self.grid.T[:,\
      \ self.ch_state], self.one_hot(self.curr_ch, self.number_of_channels), end])\n\
      \t\t\n\tdef get_init_state(self):\n\t\treturn [self.grid.T[:, self.init_state[1]],\
      \ self.one_hot(self.init_state[0], self.number_of_channels), 0]\n\t\t\n\tdef\
      \ initialize_DQN(self):\n\t\tself.ch_state = self.init_state[1]\n\t\tself.curr_ch\
      \ = self.init_state[0]\n\t\tself.sent_mess = 0\n\t\t\n\tdef one_hot(self, index,\
      \ depth):\n\t\toh = np.zeros(depth, dtype=np.float32)\n\t\toh[index] = 1\n\t\
      \treturn oh\n\t\t\n\tdef evaluate_dq_agent(agent, grid, display = False):\n\n\
      \t    action_history=[]\n\t    tot_reward = 0\n    \n\t    dq_env.initialize()\n\
      \t    first_state = dq_env.get_init_state()\n\t    action = agent.choose_action(first_state,\
      \ epsilon = 0)\n\t    action_history.append(action)\n\t    reward, new_state\
      \ = dq_env.run(action)\n\t    tot_reward += reward\n\t    \n\t    for j in range(grid.T.shape[1]-1):\n\
      \t    \tstate = new_state\n\t    \taction = agent.choose_action(state, epsilon\
      \ = 0)\n\t    \taction_history.append(action)\n\t    \treward, new_state = dq_env.run(action)\n\
      \t    \ttot_reward += reward\n\n\t    choosen_channels = [(grid.shape[0]-1)/2]\n\
      \t    for idx in range(len(action_history)):\n\t    \tchoosen_channels.append((choosen_channels[idx]+action_history[idx])%grid.shape[0])\n\
      \t    choosen_channels = choosen_channels[1:]\n\t    return tot_reward\n\n\t\
      \nclass Normalization(Layer):\n\tdef __init__(self, **kwargs):\n\t\tsuper(Normalization,\
      \ self).__init__(**kwargs)\n\n\tdef call(self, symbols):        \n\t\t# Normalize\
      \ power per symbol to 1\n\t\ten_moy = tf.sqrt(2 * tf.reduce_mean(tf.square(symbols)))\n\
      \t\tsymbols_norm = tf.divide(symbols, en_moy)\n\t\t\n\t\treturn symbols_norm\n\
      class R2C(Layer):\n\n\tdef __init__(self, **kwargs):\n\t\tsuper(R2C, self).__init__(**kwargs)\n\
      \n\tdef call(self, x):\n\t\t# Converts 2Nc real numbers into Nc complex numbers\n\
      \t\tx_cplx = tf.complex(x[:, :1], x[:, 1:])            \n\t\treturn x_cplx\n\
      \nclass C2R(Layer):\n\n\tdef __init__(self, **kwargs):\n\t\tsuper(C2R, self).__init__(**kwargs)\n\
      \n\tdef call(self, y):\n\t\t#converts Nc complex numbers into 2Nc real numbers\n\
      \t\ty_real = tf.concat([tf.math.real(y), tf.math.imag(y)], axis=1)\n\t\treturn\
      \ y_real\n\nclass OneHot(Layer):\n\n\tdef __init__(self, M, **kwargs):\n\t\t\
      super(OneHot, self).__init__(**kwargs)\n\t\tself.M = M\n\n\tdef call(self, msg):\n\
      \n\t\tone_hot_msg = tf.one_hot(msg, depth = M)\n\n\t\treturn one_hot_msg\n\n\
      class OneHot(Layer):\n\n\tdef __init__(self, M, **kwargs):\n\t\tsuper(OneHot,\
      \ self).__init__(**kwargs)\n\t\tself.M = M\n\n\tdef call(self, msg):\n\n\t\t\
      one_hot_msg = tf.one_hot(msg, depth = M)\n\n\t\treturn one_hot_msg\n\nclass\
      \ Channel(Layer):\n\n\tdef __init__(self, **kwargs):\n\t\tsuper(Channel, self).__init__(**kwargs)\n\
      \            \n\tdef call(self, x, snr):\n\t\t# Adds random Gaussian noise to\
      \ the input\n\t\tnoise_stddev = tf.sqrt( 2 / tf.pow(10., snr/10.0))\n\t\tnoise_r\
      \ = tf.random.normal(shape = tf.shape(x), stddev = 1) * noise_stddev/tf.sqrt(2.)\n\
      \t\tnoise_i = tf.random.normal(shape = tf.shape(x), stddev = 1) * noise_stddev/tf.sqrt(2.)\n\
      \t\tnoise_cplx = tf.complex(noise_r, noise_i, name=\"noise\")\n\n\t\ty = x +\
      \ noise_cplx\n\n\t\treturn y\n\t\n\t\n\t\n\t\ndef generate_ds_msg(epoch_len,\
      \ batch_size, M):\n\n\t#Generate a dataset of 'epoch_len' batches of messages.\n\
      \t#Each batch have a size 'batch_size' and consists of messages (symbols) betwen\
      \ 0 and M-1\n\trand_msg = tf.random.uniform(shape=[epoch_len, batch_size], minval=0,\
      \ maxval=M, dtype=tf.int32)\n\tfeatures_ds = tf.data.Dataset.from_tensor_slices(rand_msg)\n\
      \n\t#The labels are the same as the features : \n\t#this is an autoencoder,\
      \ we want to predict the same message that has been sent.\n\tlabels_ds = features_ds\n\
      \treturn (features_ds, labels_ds)\n\t\nclass Autoencoder(Model):\n\n\tdef __init__(self,\
      \ M, **kwargs):\n\t\tsuper(Autoencoder, self).__init__(**kwargs)\t\t\t\n\t\t\
      #Set the class' attributes\n\t\tself.M = M\n\n\t\t# Initialise the transmitter\
      \ layers\n\t\tself.oh = OneHot(self.M)\n\t\tself.d1 = Dense(64, activation='elu')\n\
      \t\tself.d2 = Dense(2, activation=None)\n\t\tself.norm = Normalization()\n\t\
      \tself.r2c = R2C()\n\t\t\n\t\t# Initialise the channel\n\t\tself.ch = Channel()\n\
      \t\t\n\t\t# Initialise the receiver layers\n\t\tself.c2r = C2R()\n\t\tself.d3\
      \ = Dense(64, activation='elu')\n\t\tself.d4 = Dense(self.M, activation=None)\n\
      \t\tself.softm = Softmax()\n\n\tdef call(self, inputs, snr):\n\t\t\t\n\t\t#\
      \ Transmitter\n\t\tself.x1 = self.oh(inputs)\n\t\tself.x2 = self.d1(self.x1)\n\
      \t\tself.x3 = self.d2(self.x2)\n\t\tself.x4 = self.norm(self.x3)\n\t\tself.x\
      \ = self.r2c(self.x4)\n\t\t    \n\t\t#Channel\n\t\tself.y = self.ch(self.x,\
      \ snr)\n\t\t\t\n\t\t\t#Receiver\n\t\tself.pb1 = self.c2r(self.y)\n\t\tself.pb2\
      \ = self.d3(self.pb1)\n\t\tself.pb3 = self.d4(self.pb2)\n\t\tself.pb = self.softm(self.pb3)\n\
      \t\t    \n\t\treturn self.pb\n\n###################\n# DO NOT LOOK AT THAT CODE\n\
      \n        \ndef new_grid(random=False, nb_ch=7, nb_states=5, min_nb=1, max_nb=3,\
      \ display=False):\n    \n    if random == False :\n        grid = np.array([[1,\
      \ 1, 1, 1, 0],\n                         [0, 1, 0, 0, 0],\n                \
      \         [0, 0, 0, 0, 0],\n                         [0, 1, 0, 0, 0],\n    \
      \                     [1, 0, 0, 0, 0],\n                         [0, 0, 0, 0,\
      \ 0],\n                         [0, 0, 0, 1, 1]])\n    else:#sampled\n     \
      \   poss_ch = []\n        print(gr.sync_block.input_items[2][0])\n        nb_good_ch\
      \ = np.random.randint(min_nb, max_nb)\n        poss_ch.append(np.sort(np.random.choice(np.arange(nb_ch),\
      \ size=(nb_good_ch), replace=False)))\n\n        for i in range(nb_states-1):\n\
      \            new = 0\n            while new == 0:\n                nb_good_ch\
      \ = np.random.randint(min_nb, max_nb)\n                new_ch = np.sort(np.random.choice(np.arange(nb_ch),\
      \ size=(nb_good_ch), replace=False))\n                for i in range(len(poss_ch)):\n\
      \                    if np.array_equal(poss_ch[i], new_ch):\n              \
      \          new-=1\n                if new < 0:\n                    new = 0\n\
      \                else:\n                    new = 1\n\n            poss_ch.append(new_ch)\n\
      \n        grid = np.zeros((nb_ch, nb_states))\n\n        for i in range(nb_states):\n\
      \            for j in range(poss_ch[i].shape[0]):\n                grid[poss_ch[i][j],\
      \ i] = 1\n                \n    if display == True:\n        plt.figure()\n\
      \        plt.imshow(np.flip(grid, axis=0), cmap='gray',  origin=\"lower\", vmin=0,\
      \ vmax=1)#, extent=[0, grid.shape[1], 0, grid.shape[0]])\n        \n    return\
      \ grid.astype(np.float32)\n    \nclass DQEnv():\n    def __init__(self, nb_channels\
      \ = 7, nb_states=5, random=False):\n        \n        if (nb_channels+1)%2!=0:\n\
      \            print('/!\\ Please enter an even number of channels')\n       \
      \     \n        self.nb_ch = nb_channels\n        self.nb_states = nb_states\n\
      \        \n        self.grid = new_grid(random, self.nb_ch, self.nb_states,\
      \ 1, int((self.nb_ch+1)/2), display=True)\n        \n        self.init_state\
      \ = [int((self.nb_ch-1)/2), int(self.nb_states-1)]\n        self.ch_state =\
      \ self.init_state[1]\n        self.curr_ch = self.init_state[0]\n        self.sent_mess\
      \ = 0\n        \n        print('This environment has ' + str(self.nb_ch) +'\
      \ different channels')\n        \n    def run(self, action):\n        \n   \
      \     self.sent_mess += 1\n        \n        self.ch_state  = (self.ch_state\
      \ + 1)%self.nb_states\n        \n        self.curr_ch = (self.curr_ch + action)%self.nb_ch\n\
      \        \n        reward = self.grid[self.curr_ch, self.ch_state]\n       \
      \ \n        if self.sent_mess != self.nb_states: \n            end = 0\n   \
      \     else :\n            end = 1\n        \n        return(reward, [self.grid[:,\
      \ self.ch_state], self.one_hot(self.curr_ch, self.nb_ch), end])\n    \n    def\
      \ get_init_state(self):\n        return [self.grid[:, self.init_state[1]], self.one_hot(self.init_state[0],\
      \ self.nb_ch), 0]\n    \n    def initialize(self):\n        self.ch_state =\
      \ self.init_state[1]\n        self.curr_ch = self.init_state[0]\n        self.sent_mess\
      \ = 0\n        \n    def one_hot(self, index, depth):\n        oh = np.zeros(depth,\
      \ dtype=np.float32)\n        oh[index] = 1\n        return oh\n        \ndef\
      \ evaluate_dq_agent(agent, grid, display = False):\n\n    action_history=[]\n\
      \    tot_reward = 0\n    \n    dq_env.initialize()\n    first_state = dq_env.get_init_state()\n\
      \    action = agent.choose_action(first_state, epsilon = 0)\n    action_history.append(action)\n\
      \    reward, new_state = dq_env.run(action)\n    tot_reward += reward\n    \n\
      \    for j in range(grid.shape[1]-1):\n        state = new_state\n        action\
      \ = agent.choose_action(state, epsilon = 0)\n        action_history.append(action)\n\
      \        reward, new_state = dq_env.run(action)\n        tot_reward += reward\n\
      \n    choosen_channels = [(grid.shape[0]-1)/2]\n    for i in range(len(action_history)):\n\
      \        choosen_channels.append((choosen_channels[i]+action_history[i])%grid.shape[0])\n\
      \    choosen_channels = choosen_channels[1:]\n\n    if display == True :\n \
      \       plt.imshow(np.flip(grid, axis=0), origin=\"lower\", cmap='gray', vmin=0,\
      \ vmax=1)\n        for i in range(len(choosen_channels)):\n            plt.scatter(i,\
      \ grid.shape[0]-1-choosen_channels[i], color='r')\n        print(str(int(tot_reward))+'/'+str(grid.shape[1])+'\
      \ packets have been transmitted')\n    return tot_reward        \n"
    active_threshold: '0.3'
    affinity: ''
    alias: ''
    comment: ''
    gamma: '1'
    learning_rate: '0.01'
    maxoutbuf: '0'
    minoutbuf: '0'
    num_GPU: '0.0'
    number_of_channels: '7'
    seed: '0'
    slot_time: '1'
    window_size: '5'
  states:
    _io_cache: ('RL Select channel', 'blk', [('number_of_channels', '7'), ('seed',
      '0'), ('num_GPU', '0.0'), ('active_threshold', '0.9'), ('slot_time', '1'), ('window_size',
      '5'), ('gamma', '1'), ('learning_rate', '0.01')], [('0', 'complex', 1), ('1',
      'complex', 1), ('2', 'complex', 1), ('3', 'complex', 1), ('4', 'complex', 1),
      ('5', 'complex', 1), ('6', 'complex', 1)], [('0', 'short', 1)], 'Embedded Python
      Block example - a simple multiply const', ['active_threshold', 'gamma', 'learning_rate',
      'number_of_channels', 'seed', 'slot_time', 'window_size'])
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [1376, 292]
    rotation: 0
    state: disabled
- name: epy_block_1
  id: epy_block
  parameters:
    _source_code: "\"\"\"\nEmbedded Python Blocks:\n\nEach time this file is saved,\
      \ GRC will instantiate the first class it finds\nto get ports and parameters\
      \ of your block. The arguments to __init__  will\nbe the parameters. All of\
      \ them are required to have default values!\n\"\"\"\n\n\nimport numpy as np\n\
      import time\nimport schedule\nimport threading\nfrom datetime import datetime,\
      \ timedelta\nfrom timeslot import Timeslot\nfrom gnuradio import gr\n\nclass\
      \ blk(gr.sync_block):  # other base classes are basic_block, decim_block, interp_block\n\
      \    \"\"\"Embedded Python Block example - a simple multiply const\"\"\"\n \
      \   def __init__(self, example_param=1.0, poisson_lambda=5.0, interval = 32000):\
      \  #change to numpy array size of inputs. only default arguments here\n    \
      \    \"\"\"arguments to this function show up as parameters in GRC\"\"\"\n \
      \       gr.sync_block.__init__(\n            self,\n            name='Channels\
      \ emulator',   # will show up in GRC\n            in_sig=[np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64],\n\
      \            out_sig=[np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64]\n\
      \        )\n        # if an attribute with the same name as a parameter is found,\n\
      \        # a callback is registered (properties work, too).\n        self.example_param\
      \ = example_param\n        self.poisson_lambda = poisson_lambda\n        stop_run_continuously\
      \ = self.run_continuously() # enable therding for scheduler\n        self.interval\
      \ = interval\n        self.active = [False,False,False,False,False,False,False]\n\
      \        self.channels = [0,0,0,0,0,0,0]#cahnge to size of inputs\n        for\
      \ j in range(0,len(self.channels)): #cahnge to synchronus\n        \tself.channels[j]\
      \ = datetime.now()\n\t\n    def work(self, input_items, output_items):\n   \
      \     \"\"\"example: multiply with constant\"\"\"\n        for i in range(0,np.size(input_items,0)):\n\
      \        \tif datetime.now() - self.channels[i] > (i+1)*timedelta(days=0, seconds=1,\
      \ microseconds=0, milliseconds=0, minutes=0, hours=0, weeks=0):  #if time passed\n\
      \        \t\t#print(self.active)\n        \t\tif self.active[i] == True:\n \
      \       \t\t\tself.active[i] = False\n        \t\telse: \n\t        \t\tself.active[i]\
      \ = True\n        \t\tself.channels[i] = datetime.now() \n        \tif self.active[i]\
      \ == True:\n        \t\tself.turnon(i,input_items, output_items)\n        \t\
      else:\n        \t\tself.turnoff(i,input_items, output_items)\n        \t\t\n\
      \        \t\t\t\n        \t\n        return len(output_items[0])\n\t\n    def\
      \ turnon(self,i, input_items, output_items):\n    \toutput_items[i][:] = input_items[i]\
      \ \n    \t\n    def turnoff(self,i, input_items, output_items):\n    \toutput_items[i][:]\
      \ = input_items[i] * 0\n    \t\n    def run_continuously(interval=1):\n    \t\
      cease_continuous_run = threading.Event()\n\n    \tclass ScheduleThread(threading.Thread):\n\
      \    \t    def run(cls):\n\t            while not cease_continuous_run.is_set():\n\
      \                        schedule.run_pending()\n                        time.sleep(interval)\n\
      \    \tcontinuous_thread = ScheduleThread()\n    \tcontinuous_thread.start()\n\
      \    \treturn cease_continuous_run\n"
    affinity: ''
    alias: ''
    comment: ''
    example_param: '1.0'
    interval: samp_rate
    maxoutbuf: '0'
    minoutbuf: '0'
    poisson_lambda: '1.0'
  states:
    _io_cache: ('Channels emulator', 'blk', [('example_param', '1.0'), ('poisson_lambda',
      '5.0'), ('interval', '32000')], [('0', 'complex', 1), ('1', 'complex', 1), ('2',
      'complex', 1), ('3', 'complex', 1), ('4', 'complex', 1), ('5', 'complex', 1),
      ('6', 'complex', 1)], [('0', 'complex', 1), ('1', 'complex', 1), ('2', 'complex',
      1), ('3', 'complex', 1), ('4', 'complex', 1), ('5', 'complex', 1), ('6', 'complex',
      1)], 'Embedded Python Block example - a simple multiply const', ['example_param',
      'interval', 'poisson_lambda'])
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [473, 328]
    rotation: 0
    state: enabled
- name: epy_block_1_0
  id: epy_block
  parameters:
    _source_code: "\"\"\"\nEmbedded Python Blocks:\n\nEach time this file is saved,\
      \ GRC will instantiate the first class it finds\nto get ports and parameters\
      \ of your block. The arguments to __init__  will\nbe the parameters. All of\
      \ them are required to have default values!\n\"\"\"\n\n\nimport numpy as np\n\
      import time\nimport schedule\nimport threading\nfrom datetime import datetime,\
      \ timedelta\nfrom timeslot import Timeslot\nfrom gnuradio import gr\n\nclass\
      \ blk(gr.sync_block):  # other base classes are basic_block, decim_block, interp_block\n\
      \    \"\"\"Embedded Python Block example - a simple multiply const\"\"\"\n \
      \   def __init__(self, example_param=1.0, poisson_lambda=5.0, interval = 32000):\
      \  #change to numpy array size of inputs. only default arguments here\n    \
      \    \"\"\"arguments to this function show up as parameters in GRC\"\"\"\n \
      \       gr.sync_block.__init__(\n            self,\n            name='Channels\
      \ emulator',   # will show up in GRC\n            in_sig=[np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64],\n\
      \            out_sig=[np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64]\n\
      \        )\n        # if an attribute with the same name as a parameter is found,\n\
      \        # a callback is registered (properties work, too).\n        self.example_param\
      \ = example_param\n        self.poisson_lambda = poisson_lambda\n        stop_run_continuously\
      \ = self.run_continuously() # enable therding for scheduler\n        self.interval\
      \ = interval\n        self.channels = [0,0,0,0,0,0,0]#cahnge to size of inputs\n\
      \        for j in range(0,len(self.channels)): #cahnge to synchronus\n     \
      \   \tself.channels[j] = datetime.now()\n\t\n    def work(self, input_items,\
      \ output_items):\n        \"\"\"example: multiply with constant\"\"\"\n    \
      \    for i in range(0,np.size(input_items,0)):\n        \tactive = np.random.poisson(self.poisson_lambda,\
      \ 1) > 5.0\n        \tif datetime.now() - self.channels[i] > timedelta(days=0,\
      \ seconds=1, microseconds=self.interval, milliseconds=0, minutes=0, hours=0,\
      \ weeks=0):\n        \t\tself.turnoff(i,input_items, output_items)\n       \
      \ \t\tif active == True:\n        \t\t\tself.channels[i] = datetime.now()\n\
      \        \telse:\n        \t\tself.turnon(i,input_items, output_items)\n   \
      \     return len(output_items[0])\n\t\n    def turnon(self,i, input_items, output_items):\n\
      \    \toutput_items[i][:] = input_items[i] * self.example_param\n    \t\n  \
      \  def turnoff(self,i, input_items, output_items):\n    \toutput_items[i][:]\
      \ = input_items[i] * 0\n    \t\n    def run_continuously(interval=1):\n    \t\
      cease_continuous_run = threading.Event()\n\n    \tclass ScheduleThread(threading.Thread):\n\
      \    \t    def run(cls):\n\t            while not cease_continuous_run.is_set():\n\
      \                        schedule.run_pending()\n                        time.sleep(interval)\n\
      \    \tcontinuous_thread = ScheduleThread()\n    \tcontinuous_thread.start()\n\
      \    \treturn cease_continuous_run\n"
    affinity: ''
    alias: ''
    comment: ''
    example_param: '1.0'
    interval: samp_rate
    maxoutbuf: '0'
    minoutbuf: '0'
    poisson_lambda: '1.0'
  states:
    _io_cache: ('Channels emulator', 'blk', [('example_param', '1.0'), ('poisson_lambda',
      '5.0'), ('interval', '32000')], [('0', 'complex', 1), ('1', 'complex', 1), ('2',
      'complex', 1), ('3', 'complex', 1), ('4', 'complex', 1), ('5', 'complex', 1),
      ('6', 'complex', 1)], [('0', 'complex', 1), ('1', 'complex', 1), ('2', 'complex',
      1), ('3', 'complex', 1), ('4', 'complex', 1), ('5', 'complex', 1), ('6', 'complex',
      1)], 'Embedded Python Block example - a simple multiply const', ['example_param',
      'interval', 'poisson_lambda'])
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [478, 608]
    rotation: 0
    state: disabled
- name: epy_block_2
  id: epy_block
  parameters:
    _source_code: "\"\"\"\nEmbedded Python Blocks:\n\nEach time this file is saved,\
      \ GRC will instantiate the first class it finds\nto get ports and parameters\
      \ of your block. The arguments to __init__  will\nbe the parameters. All of\
      \ them are required to have default values!\n\"\"\"\n\nimport numpy as np\n\
      from gnuradio import gr\n\n\nclass blk(gr.sync_block):  # other base classes\
      \ are basic_block, decim_block, interp_block\n    \"\"\"Embedded Python Block\
      \ example - a simple multiply const\"\"\"\n\n    def __init__(self, example_param=1.0):\
      \  # only default arguments here\n        \"\"\"arguments to this function show\
      \ up as parameters in GRC\"\"\"\n        gr.sync_block.__init__(\n         \
      \   self,\n            name='MAC Controller',   # will show up in GRC\n    \
      \        in_sig=[np.int16],\n            out_sig=[np.int16]\n        )\n   \
      \     # if an attribute with the same name as a parameter is found,\n      \
      \  # a callback is registered (properties work, too).\n        self.example_param\
      \ = example_param\n\n    def work(self, input_items, output_items):\n      \
      \  \"\"\"example: multiply with constant\"\"\"\n        output_items[0][:] =\
      \ input_items[0] * self.example_param\n        return len(output_items[0])\n"
    affinity: ''
    alias: ''
    comment: ''
    example_param: '1.0'
    maxoutbuf: '0'
    minoutbuf: '0'
  states:
    _io_cache: ('MAC Controller', 'blk', [('example_param', '1.0')], [('0', 'short',
      1)], [('0', 'short', 1)], 'Embedded Python Block example - a simple multiply
      const', ['example_param'])
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [1666, 653]
    rotation: 0
    state: disabled
- name: epy_block_3
  id: epy_block
  parameters:
    _source_code: "\"\"\"\nEmbedded Python Blocks:\n\nEach time this file is saved,\
      \ GRC will instantiate the first class it finds\nto get ports and parameters\
      \ of your block. The arguments to __init__  will\nbe the parameters. All of\
      \ them are required to have default values!\n\"\"\"\n\nimport os\nimport copy\n\
      import sys      \nimport time\nimport schedule\nimport threading\nfrom datetime\
      \ import datetime, timedelta\nfrom timeslot import Timeslot  \nimport numpy\
      \ as np\nimport matplotlib.pyplot as plt\nimport torch\nimport tensorflow as\
      \ tf\nfrom gnuradio import gr\nimport pmt\nfrom tensorflow.keras import Model\n\
      from tensorflow.keras.layers import Layer, Dense, Softmax\nfrom tensorflow.keras.layers\
      \ import Dense\n\nfrom gym import Env\nfrom gym.spaces import Discrete, Box\n\
      \nclass blk(gr.sync_block):  # other base classes are basic_block, decim_block,\
      \ interp_block\n    \"\"\"Embedded Python Block example - a simple multiply\
      \ const\"\"\"\n\n    def __init__(self,number_of_channels=7,seed = 0, num_GPU=0.0,\
      \ active_threshold= 0.9, slot_time = 1, window_size = 5, initial_channel = 0,\
      \ gamma = 1, learning_rate = 0.01):  # only default arguments here\n       \
      \ \"\"\"arguments to this function show up as parameters in GRC\"\"\"\n    \
      \    gr.sync_block.__init__(\n            self,\n            name='RL Channels\
      \ select mgoutay agent',   # will show up in GRC\n            in_sig=[np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64],\n\
      \            out_sig=[np.uint16]\n        )\n        # if an attribute with\
      \ the same name as a parameter is found,\n        # a callback is registered\
      \ (properties work, too).\n        \n        #initailize tensorflow and GPU\
      \ usage\n        print('Tensorflow version: ', tf.__version__)\n        gpus\
      \ = tf.config.experimental.list_physical_devices(\"GPU\")\n        print('Number\
      \ of GPUs available :', len(gpus))\n        \n        if num_GPU < len(gpus):\n\
      \        \ttf.config.experimental.set_visible_devices(gpus[num_GPU], 'GPU')\n\
      \        \ttf.config.experimental.set_memory_growth(gpus[num_GPU], True)\n \
      \       \tprint('Only GPU number', num_GPU, 'used')\n        tf.random.set_seed(seed)\n\
      \t\n        #initialize Channel sensing\n        self.active_threshold = active_threshold\n\
      \        self.number_of_channels = number_of_channels\n        if (self.number_of_channels+1)%2!=0:\n\
      \        \tprint('/!\\ Please enter an even number of channels')\n        self.channels\
      \ = np.zeros(self.number_of_channels)\n        self.slot_time = slot_time\n\
      \        self.last_datetime = datetime.now()\n        self.window_size = window_size\n\
      \        self.grid = np.zeros((self.window_size, self.number_of_channels))\n\
      \        self.initial_channel = initial_channel #change to random\n        self.grid_flag\
      \ = 0\n        \n        #initialize DQN env\n\n        #initialize DQN Neural\
      \ Network\n        self.gamma = gamma\n        self.learning_rate = learning_rate\n\
      \        self.actions_number = self.number_of_channels\n        self.epsilon\
      \ = 0.75\n        self.nb_trainings = 250\n        self.loss = []  # Keep trak\
      \ of the losses\n        \n        self.train_flag = False\n        self.done\
      \ = False\n        #self.env = self.channelenv(self.number_of_channels, self.initial_channel\
      \ , self.grid)\n        \n        \n        self.dq_env = self.DQEnv(self.number_of_channels,\
      \ self.window_size, False, self.grid.T)\n        self.agent_dq = self.DQAgent(self.number_of_channels,\
      \ self.learning_rate, self.gamma)\n        \n        print('##################################')\n\
      \        print('ENV initial state:')\n        print(self.dq_env.get_init_state())\n\
      \        print('##################################')\n      \n\n    def work(self,\
      \ input_items, output_items):\n        \"\"\"example: multiply with constant\"\
      \"\"\n        #sample channels\n        for i in range(0,np.size(input_items,0)):\n\
      \        \t#check slot time\n        \tif datetime.now() - self.last_datetime\
      \ > timedelta(days=0, seconds=self.slot_time, microseconds=0, milliseconds=0,\
      \ minutes=0, hours=0, weeks=0):\n                    self.grid = self.make_grid(self.grid,self.channels)\n\
      \                    self.last_datetime = datetime.now()\n                 \
      \   self.channels = np.zeros(self.number_of_channels)\n                    #print(self.grid.T)\n\
      \                    #self.env.transmit(self.grid.T)\n                    self.grid_flag\
      \ = self.grid_flag+1\n                    if self.done == True:\n          \
      \          \tstate = [self.channels.astype(np.float32), self.dq_env.one_hot(self.dq_env.init_state[0],\
      \ self.number_of_channels), 0]\n                    \toutput_items[0][:] = self.agent_dq.choose_action(state,\
      \ self.epsilon)\n        \tif (np.mean(input_items[i][:])>self.active_threshold):\n\
      \                    self.channels[i] = 1\n        \t\n        \n        if\
      \ (self.grid_flag == self.number_of_channels*self.window_size+1):#and(datetime.now()\
      \ - self.last_datetime > timedelta(days=0, seconds=self.slot_time, microseconds=0,\
      \ milliseconds=0, minutes=0, hours=0, weeks=0)):\n        \tself.train_flag\
      \ = True          \t\n        \tprint(self.grid.T)\n\t\t# Run 200 episodes\n\
      \        \tfor i in range(200):\n                    self.dq_env.initialize()\n\
      \                    state = self.dq_env.get_init_state()\n\t    \n        \
      \            action = self.agent_dq.choose_action(state, self.epsilon)\n   \
      \                 reward, new_state = self.dq_env.run(action)\n\t    \n    \
      \                self.agent_dq.insert_history(state, action, reward, new_state)\
      \ # Insert state, action, reward, new state in history \n\t    \n          \
      \          # While it's not the end of the episode\n                    while\
      \ new_state[2]==0 :\n                    \tstate = new_state\n\t\t\n       \
      \             \taction = self.agent_dq.choose_action(state, self.epsilon)\n\
      \                    \treward, new_state = self.dq_env.run(action)\n\t\t\n \
      \                   \tself.agent_dq.insert_history(state, action, reward, new_state)\
      \ # Insert state, action, reward, new state in history\n                   \
      \ \t\n        \tprint('Replay buffer History populated')\n        \t       \
      \          \t \n\n        \t\n        \t\n\n        if self.train_flag == True:\n\
      \t        print('Training...')\n        \tself.train_flag = False\n        \t\
      for i in range(self.nb_trainings):\n        \t\tself.dq_env.initialize()\n \
      \       \t\tstate = self.dq_env.get_init_state()\n    \n        \t\taction =\
      \ self.agent_dq.choose_action(state, self.epsilon)\n        \t\treward, new_state\
      \ = self.dq_env.run(action)\n        \t\tself.agent_dq.insert_history(state,\
      \ action, reward, new_state) # Insert state, action, reward, new state in history\
      \ \n        \t\tself.agent_dq.learn(10) # Each time we store a new history,\
      \ we perform a training on random data\n    \n        \t\t# While it's not the\
      \ end of the episode\n        \t\twhile new_state[2]==0 :\n        \t\t\tstate\
      \ = new_state\n        \t\t\taction = self.agent_dq.choose_action(state, self.epsilon)\n\
      \        \t\t\treward, new_state = self.dq_env.run(action)\n        \t\t\tself.agent_dq.insert_history(state,\
      \ action, reward, new_state) # Insert state, action, reward, new state in history\
      \ \n        \t\t\tself.agent_dq.learn(10) # Each time we store a new history,\
      \ we perform a training on random data\n    \n        \t\tself.loss.append(tf.reduce_mean(self.agent_dq.loss).numpy())\
      \ #Save the losses for future visualization\n        \t\t\t#Every 10 iterations\
      \ we copy the parameters of the online DQN to the offline DQN\n        \t\t\
      if (i+1)%10 == 0:\n        \t\t\tself.agent_dq.copy_parameters()\n        \t\
      \t\tprint((i+1), end=', ')\n        \tself.done = True\n      \n        \t\n\
      \        return len(output_items[0])\n        \n    \n    class DQEnv():\n \
      \   \tdef __init__(self, nb_channels = 7, nb_states=5, random=False , grid =\
      \ []):\n        \n\t    \tif (nb_channels+1)%2!=0:\n\t    \t\tprint('/!\\ Please\
      \ enter an even number of channels')\n            \n\t    \tself.nb_ch = nb_channels\n\
      \t    \tself.nb_states = nb_states\n        \n\t    \tself.grid = grid\n   \
      \     \n\t    \tself.init_state = [int((self.nb_ch-1)/2), int(self.nb_states-1)]\n\
      \t    \tself.ch_state = self.init_state[1]\n\t    \tself.curr_ch = self.init_state[0]\n\
      \t    \tself.sent_mess = 0\n        \n\t    \tprint('This environment has '\
      \ + str(self.nb_ch) +' different channels')\n        \n    \tdef run(self, action):\n\
      \        \n\t    \tself.sent_mess += 1\n        \n\t    \tself.ch_state  = (self.ch_state\
      \ + 1)%self.nb_states\n        \n\t    \tself.curr_ch = (self.curr_ch + action)%self.nb_ch\n\
      \        \n\t    \treward = self.grid[self.curr_ch, self.ch_state]\n       \
      \ \n\t    \tif self.sent_mess != self.nb_states: \n\t    \t\tend = 0\n\t   \
      \ \telse :\n\t    \t\tend = 1\n        \n\t    \treturn(reward, [self.grid[:,\
      \ self.ch_state], self.one_hot(self.curr_ch, self.nb_ch), end])\n    \n    \t\
      def get_init_state(self):\n\t    \treturn [self.grid[:, self.init_state[1]],\
      \ self.one_hot(self.init_state[0], self.nb_ch), 0]\n    \n    \tdef initialize(self):\n\
      \t    \tself.ch_state = self.init_state[1]\n\t    \tself.curr_ch = self.init_state[0]\n\
      \t    \tself.sent_mess = 0\n        \n    \tdef one_hot(self, index, depth):\n\
      \t    \toh = np.zeros(depth, dtype=np.float32)\n\t    \toh[index] = 1\n\t  \
      \  \treturn oh\n    \n    \n    class DQAgent():\n    \tdef __init__(self, nb_channels,\
      \ learning_rate, gamma):\n    \t\tself.nb_ch = nb_channels\n    \t\tself.nb_actions\
      \ = 3\n    \t\tself.gamma = gamma\n    \t\tself.learning_rate = learning_rate\n\
      \    \t\t\n    \t\tself.history_length = 1000\n    \t\tself.history = [[]for\
      \ i in range(self.history_length)]\n    \t\tself.history_idx = 0\n    \t\t\n\
      \    \t\t#Create and initialize the online DQN\n    \t\tself.DQN_online = tf.keras.models.Sequential([Dense(2*self.nb_ch,\
      \ activation='relu'), Dense(self.nb_actions, activation='softplus') #Outputs\
      \ positive values \n    \t\t])\n    \t\tself.DQN_online.build(input_shape=(None,\
      \ self.nb_ch*2)) #Build the model to create the weights\n    \t\t\n    \t\t\
      #Create and initialize the offline DQN\n    \t\tself.DQN_offline = tf.keras.models.Sequential([Dense(2*self.nb_ch,\
      \ activation='relu'), Dense(self.nb_actions, activation='softplus') #Outputs\
      \ positive values\n    \t\t])\n    \t\tself.DQN_offline.build(input_shape=(None,\
      \ self.nb_ch*2)) #Build the model to create the weights\n    \t\t\n    \t\t\
      self.copy_parameters() #Copy the weights of the online network to the offline\
      \ network\n    \t\t\n    \t\tself.loss_func = tf.keras.losses.MSE\n    \t\t\
      self.optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n        \n  \
      \      \n    \tdef choose_action(self, state, epsilon):\n    \t\t\"\"\"Implements\
      \ an epsilon-greedy policy\"\"\"\n        \n    \t\t# Explore ?\n    \t\tif\
      \ np.random.uniform(size=1) < epsilon :\n    \t\t\taction =  np.random.randint(low\
      \ = -1, high = 2) # Take one action randomly {-1, 0, 1} with probability epsilon\n\
      \        \n    \t\t#Choose the best action\n    \t\telse:    \n    \t\t\tsensed_ch,\
      \ curr_ch, end = state #Decomposes the state\n    \t\t\t'''\n    \t\t\t* The\
      \ shapes are :\n    \t\t\t- curr_ch : [self.nb_ch]\n    \t\t\t- sensed_ch :\
      \ [self.nb_ch]\n    \t\t\t- end : one integer\n    \t\t\t'''\n    \t\t\tDQN_input\
      \ = tf.concat([sensed_ch, curr_ch], axis=0)[tf.newaxis, :] #Create a state vector,\
      \ which is the DQN input. Shape : [1, 2*self.nb_ch]\n    \t\t\toutputs = self.DQN_online(DQN_input).numpy()\
      \ #Get the predicted Q values corresponding to the 3 actions\n    \t\t\taction\
      \ = np.argmax(outputs)-1 #Take the action that has the highest predicted Q value\
      \ (-1, 0, 1)\n            \n    \t\treturn action\n    \n    \tdef learn(self,\
      \ batch_size):\n    \t\t\"\"\"Sample experiences from the history and performs\
      \ SGD\"\"\"\n    \t\t\n    \t\t# Samples random experiences from the history\n\
      \    \t\tidx = np.random.choice(range(self.history_length), batch_size, replace=False)\
      \ # Create random indexes \n    \t\trdm_exp =  [self.history[i] for i in idx]\
      \ # Take experiences corresponding to the random indexes\n    \t\t\n    \t\t\
      # Each experience is written in this format : [state_vec, end, action, reward,\
      \ n_state_vec, n_end] (see insert_history method)\n    \t\t\n    \t\t# Create\
      \ 6 batches : states_vec, end_boolean, actions, rewards, new states_vec, new_end_boolean\n\
      \    \t\tstates_vec = np.array([rdm_exp[i][0] for i in range(batch_size)]) #\
      \ Shape : [Bs, 2*self.nb_ch]\n    \t\tend = np.array([rdm_exp[i][1] for i in\
      \ range(batch_size)]) # Shape : [Bs]\n    \t\tactions = np.array([rdm_exp[i][2]\
      \ for i in range(batch_size)]) # Shape : [BS]\n    \t\trewards = np.array([rdm_exp[i][3]\
      \ for i in range(batch_size)]) # Shape : [BS]\n    \t\tn_states_vec = np.array([rdm_exp[i][4]\
      \ for i in range(batch_size)]) # Shape : [Bs, 2*self.nb_ch]\n    \t\tn_end =\
      \ np.array([rdm_exp[i][5] for i in range(batch_size)]) # Shape : [BS]\n    \t\
      \t\n    \t\t#Compute the best q_value for the new states\n    \t\tmax_n_q_values\
      \ = tf.reduce_max(self.DQN_offline(n_states_vec), axis=1).numpy()\n\n    \t\t\
      with tf.GradientTape() as tape:\n    \t\t\t#Forward pass through the online\
      \ network to predict the q_values\n    \t\t\tpred_q_values = self.DQN_online(states_vec)\n\
      \    \t\t\t\n    \t\t\t# Compute targets\n    \t\t\ttargets = pred_q_values.numpy()\n\
      \    \t\t\ttargets[np.arange(targets.shape[0]), actions+1]= rewards + (1-n_end)\
      \ * self.gamma * max_n_q_values\n    \t\t\t\n    \t\t\t# Evaluate the loss\n\
      \    \t\t\tself.loss = self.loss_func(pred_q_values, targets)\n        \n  \
      \  \t\t# Compute gradients and perform the gradient descent\n    \t\tgradients\
      \ = tape.gradient(self.loss, self.DQN_online.trainable_weights)\n    \t\tself.optimizer.apply_gradients(zip(gradients,\
      \ self.DQN_online.trainable_weights))  \n    \n    \tdef insert_history(self,\
      \ state, action, reward, n_state):\n    \t\t\"\"\"Insert experience in history\"\
      \"\"\n        \n    \t\tsensed_ch, curr_ch, end = state\n    \t\tstate_vec =\
      \ np.concatenate([sensed_ch, curr_ch], axis=0) # Create the state vector for\
      \ the state\n        \n    \t\tn_sensed_ch, n_curr_ch, n_end = n_state\n   \
      \ \t\tn_state_vec = np.concatenate([n_sensed_ch, n_curr_ch], axis=0) # Create\
      \ the state vector for the new state\n\n    \t\tself.history[self.history_idx]\
      \ = [state_vec, end, action, reward, n_state_vec, n_end] # Insert everything\
      \ in the history\n        \n    \t\tself.history_idx = (self.history_idx+1)%self.history_length\
      \ # Move the history_idx by one\n    \n    \tdef copy_parameters(self):\n  \
      \  \t\t\"\"\"Copy the parameters of the online network to the offline network\"\
      \"\"\n\n    \t\tweights = self.DQN_online.get_weights()\n    \t\tself.DQN_offline.set_weights(weights)\n\
      \n\n\n\t    \t\n    def build_model(states, actions):\n    \tmodel = Sequential()\n\
      \    \tmodel.add(Dense(24, activation='relu', input_shape=states))\n    \tmodel.add(Dense(actions,\
      \ activation='linear'))\n    \treturn model\n\t\t\n\n    def make_grid(self,grid,channels):\n\
      \        grid = np.append(grid, channels).reshape(self.window_size+1,7)\n  \
      \      grid = np.delete(grid, 0, 0)\n        grid = grid.reshape(self.window_size,self.number_of_channels)\n\
      \        return grid.astype(np.float32)\n"
    active_threshold: '0.3'
    affinity: ''
    alias: ''
    comment: ''
    gamma: '1'
    initial_channel: '0'
    learning_rate: '0.01'
    maxoutbuf: '0'
    minoutbuf: '0'
    num_GPU: '1'
    number_of_channels: '7'
    seed: '1'
    slot_time: '1'
    window_size: '5'
  states:
    _io_cache: ('RL Channels select mgoutay agent', 'blk', [('number_of_channels',
      '7'), ('seed', '0'), ('num_GPU', '0.0'), ('active_threshold', '0.9'), ('slot_time',
      '1'), ('window_size', '5'), ('initial_channel', '0'), ('gamma', '1'), ('learning_rate',
      '0.01')], [('0', 'complex', 1), ('1', 'complex', 1), ('2', 'complex', 1), ('3',
      'complex', 1), ('4', 'complex', 1), ('5', 'complex', 1), ('6', 'complex', 1)],
      [('0', 'short', 1)], 'Embedded Python Block example - a simple multiply const',
      ['active_threshold', 'gamma', 'initial_channel', 'learning_rate', 'number_of_channels',
      'slot_time', 'window_size'])
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [1375, 537]
    rotation: 0
    state: disabled
- name: epy_block_3_0
  id: epy_block
  parameters:
    _source_code: "\"\"\"\nEmbedded Python Blocks:\n\nEach time this file is saved,\
      \ GRC will instantiate the first class it finds\nto get ports and parameters\
      \ of your block. The arguments to __init__  will\nbe the parameters. All of\
      \ them are required to have default values!\n\"\"\"\n\nimport os\nimport copy\n\
      import sys      \nimport time\nimport schedule\nimport threading\nfrom datetime\
      \ import datetime, timedelta\nfrom timeslot import Timeslot  \nimport numpy\
      \ as np\nimport matplotlib.pyplot as plt\nimport torch\nimport tensorflow as\
      \ tf\nfrom gnuradio import gr\nimport pmt\nfrom tensorflow.keras import Model\n\
      from tensorflow.keras.layers import Layer, Dense, Softmax\nfrom tensorflow.keras.layers\
      \ import Dense\n\nfrom gym import Env\nfrom gym.spaces import Discrete, Box\n\
      \n\nfrom matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg\nfrom matplotlib.figure\
      \ import Figure\nfrom PyQt5 import QtCore #conda install pyqt\nfrom PyQt5 import\
      \ QtWidgets\n\nclass blk(gr.sync_block):  # other base classes are basic_block,\
      \ decim_block, interp_block\n    \"\"\"Embedded Python Block example - a simple\
      \ multiply const\"\"\"\n\n    def __init__(self,number_of_channels=7,seed =\
      \ 0, num_GPU=0.0, active_threshold= 0.9, slot_time = 1, window_size = 5, initial_channel\
      \ = 0, gamma = 1, learning_rate = 0.01, epsilon = 1):  # only default arguments\
      \ here\n        \"\"\"arguments to this function show up as parameters in GRC\"\
      \"\"\n        gr.sync_block.__init__(\n            self,\n            name='RL\
      \ Channels select self agent',   # will show up in GRC\n            in_sig=[np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64],\n\
      \            out_sig=[np.uint16]\n        )\n        # if an attribute with\
      \ the same name as a parameter is found,\n        # a callback is registered\
      \ (properties work, too).\n        \n        #initailize tensorflow and GPU\
      \ usage\n        print('Tensorflow version: ', tf.__version__)\n        gpus\
      \ = tf.config.experimental.list_physical_devices(\"GPU\")\n        print('Number\
      \ of GPUs available :', len(gpus))\n        \n        if num_GPU < len(gpus):\n\
      \        \ttf.config.experimental.set_visible_devices(gpus[num_GPU], 'GPU')\n\
      \        \ttf.config.experimental.set_memory_growth(gpus[num_GPU], True)\n \
      \       \tprint('Only GPU number', num_GPU, 'used')\n        tf.random.set_seed(seed)\n\
      \t\n        #initialize Channel sensing\n        self.active_threshold = active_threshold\n\
      \        self.number_of_channels = number_of_channels\n        if (self.number_of_channels+1)%2!=0:\n\
      \        \tprint('/!\\ Please enter an even number of channels')\n        self.channels\
      \ = np.zeros(self.number_of_channels)\n        self.slot_time = slot_time\n\
      \        self.last_datetime = datetime.now()\n        self.window_size = window_size\n\
      \        self.grid = np.zeros((self.window_size, self.number_of_channels))\n\
      \        self.initial_channel = initial_channel #change to random\n        self.grid_flag\
      \ = 0\n        self.channel_decision = self.initial_channel\n        \n    \
      \    self.collision = 0\n        self.total_error = 0 \n        self.total_count\
      \ = 1 \n        #initialize DQN env\n        #self.dq_env = self.DQEnv(self.number_of_channels,\
      \ self.window_size, False, self.grid.T)\n\n        #initialize DQN Neural Network\n\
      \        self.gamma = gamma\n        self.learning_rate = learning_rate\n  \
      \      self.actions_number = self.number_of_channels\n        self.epsilon =\
      \ epsilon\n        self.nb_trainings = 250\n        self.loss = []  # Keep trak\
      \ of the losses\n        \n        self.train_flag = False\n        self.done\
      \ = False\n        \n        self.agent_dq = self.DQAgent(self.number_of_channels,\
      \ self.learning_rate, self.gamma)\n        #sensed_ch, curr_ch, end = self.dq_env.get_init_state()\n\
      \        #print('##################################')\n        #print('ENV initial\
      \ state:')\n        #print('State: ', sensed_ch,', Current Channel: ', curr_ch)\n\
      \        #print('##################################')\n      \n\n    def work(self,\
      \ input_items, output_items):       \t\n        #Check slot time\n        if\
      \ (datetime.now() - self.last_datetime) > timedelta(days=0, seconds=self.slot_time,\
      \ microseconds=0, milliseconds=0, minutes=0, hours=0, weeks=0):\n          \
      \  #Sample and mark channels activity\n            for i in range(0,np.size(input_items,0)):\n\
      \            \t#Energy detected during time slot, mark as occupied \n      \
      \      \tif (np.mean(abs(input_items[i][:]))>self.active_threshold):\n     \
      \       \t\tself.channels[i] = 1\n            print(self.channels,'    ', datetime.now())\
      \   \n            #when training is done, make a decision\n            if self.done\
      \ == True:\n            \tstate = [self.channels.astype(np.float32), self.dq_env.one_hot(self.dq_env.init_state[0],\
      \ self.number_of_channels), 0]\n            \tself.channel_decision = self.agent_dq.choose_action(state,\
      \ self.epsilon)\n            \tself.total_count += 1\n            \tself.total_ratio\
      \ = (self.total_error)/self.total_count\n            \tprint('CH: ',self.channels)\n\
      \            \ttemp = np.zeros(np.size(self.channels))\n            \ttemp[self.channel_decision]=1\n\
      \            \tprint('DC: ',temp)\n            \tprint('count: ', self.total_count,\
      \ ' Errors: ', self.total_error, ' ratio: ',self.total_ratio)\n         \t\n\
      \            \tif self.channels[self.channel_decision] == 1:\n            \t\
      \tself.collision += 1             \n            self.grid = self.make_grid(self.grid,(self.channels))\n\
      \            self.last_datetime = datetime.now()\n            self.channels\
      \ = np.zeros(self.number_of_channels)\n            #print(self.grid)\n     \
      \       \n            self.grid_flag += 1\n        output_items[0][:]= self.channel_decision\
      \      \n        if self.collision == 1:\n        \tself.total_error += 1\n\
      \        \tself.collision = 0\n        \t#self.grid_flag = self.window_size+1\n\
      \        \t\n\n        \t\n        #Full window sized grid\n        if (self.grid_flag\
      \ == self.window_size+1): \n\t\t\n        \t#initialize DQN env\n        \t\
      self.dq_env = self.DQEnv(self.number_of_channels, self.window_size, False, self.grid.T)\n\
      \t\n\t\t# Run 200 episodes\n        \tfor i in range(200):\n               \
      \     \n                    self.dq_env.initialize()\n                    \n\
      \                    state = self.dq_env.get_init_state()\n                \
      \    action = self.agent_dq.choose_action(state, self.epsilon)\n           \
      \         reward, new_state = self.dq_env.run(action)\n\t    \n            \
      \        self.agent_dq.insert_history(state, action, reward, new_state) # Insert\
      \ state, action, reward, new state in history \n\t    \n                   \
      \ # While it's not the end of the episode\n                    while new_state[2]==0\
      \ :\n                    \tstate = new_state\n                    \taction =\
      \ self.agent_dq.choose_action(state, self.epsilon)\n                    \treward,\
      \ new_state = self.dq_env.run(action)\n\t\t\n                    \tself.agent_dq.insert_history(state,\
      \ action, reward, new_state) # Insert state, action, reward, new state in history\n\
      \        \t\n        \tself.train_flag = True                      \t\n    \
      \    \tprint('Replay buffer History populated')\n        \t                \
      \ \t \n\n        \t\n        \t\n\n        if self.train_flag == True:\n\t \
      \       print('Training...')\n        \tself.train_flag = False\n        \t\n\
      \        \tfor i in range(self.nb_trainings):\n        \t\tself.dq_env.initialize()\n\
      \        \t\tstate = self.dq_env.get_init_state()\n    \n        \t\taction\
      \ = self.agent_dq.choose_action(state, self.epsilon)\n        \t\treward, new_state\
      \ = self.dq_env.run(action)\n        \t\tself.agent_dq.insert_history(state,\
      \ action, reward, new_state) # Insert state, action, reward, new state in history\
      \ \n        \t\tself.agent_dq.learn(10) # Each time we store a new history,\
      \ we perform a training on random data\n    \n        \t\t# While it's not the\
      \ end of the episode\n        \t\twhile new_state[2]==0 :\n        \t\t\tstate\
      \ = new_state\n        \t\t\taction = self.agent_dq.choose_action(state, self.epsilon)\n\
      \        \t\t\treward, new_state = self.dq_env.run(action)\n        \t\t\tself.agent_dq.insert_history(state,\
      \ action, reward, new_state) # Insert state, action, reward, new state in history\
      \ \n        \t\t\tself.agent_dq.learn(10) # Each time we store a new history,\
      \ we perform a training on random data\n    \n        \t\tself.loss.append(tf.reduce_mean(self.agent_dq.loss).numpy())\
      \ #Save the losses for future visualization\n        \t\t\t#Every 10 iterations\
      \ we copy the parameters of the online DQN to the offline DQN\n        \t\t\
      if (i+1)%10 == 0:\n        \t\t\tself.agent_dq.copy_parameters()\n        \t\
      \t\tprint((i+1), end=', ')\n        \tself.done = True\n        \t#plt.semilogy(np.arange(len(self.loss)),\
      \ self.loss)\n        \t#plt.show()\n        \t\n        return len(output_items[0])\n\
      \        \n\n    \n    \n    class DQEnv():\n    \tdef __init__(self, nb_channels\
      \ = 7, nb_states=5, random=False , grid = []):\n        \n\t    \tif (nb_channels+1)%2!=0:\n\
      \t    \t\tprint('/!\\ Please enter an even number of channels')\n          \
      \  \n\t    \tself.nb_ch = nb_channels\n\t    \tself.nb_states = nb_states\n\
      \        \n\t    \tself.grid = grid\n\t    \tprint ('env grid:')\n\t    \tprint(self.grid)\n\
      \t    \tself.init_state = [int((self.nb_ch-1)/2), int(self.nb_states-1)]\n\t\
      \    \tself.ch_state = self.init_state[1]\n\t    \tself.curr_ch = self.init_state[0]\n\
      \t    \tself.sent_mess = 0\n        \n\t    \tprint('This environment has '\
      \ + str(self.nb_ch) +' different channels')\n        \n    \tdef run(self, action):\n\
      \        \n\t    \tself.sent_mess += 1\n        \n\t    \tself.ch_state  = (self.ch_state\
      \ + 1)%self.nb_states\n        \n\t    \tself.curr_ch = (self.curr_ch + action)%self.nb_ch\n\
      \        \n\t    \treward = np.abs(self.grid[self.curr_ch, self.ch_state]-1)\n\
      \        \n\t    \tif self.sent_mess != self.nb_states: \n\t    \t\tend = 0\n\
      \t    \telse :\n\t    \t\tend = 1\n        \n\t    \treturn(reward, [self.grid[:,\
      \ self.ch_state], self.one_hot(self.curr_ch, self.nb_ch), end])\n    \n    \t\
      def get_init_state(self):\n\t    \treturn [self.grid[:, self.init_state[1]],\
      \ self.one_hot(self.init_state[0], self.nb_ch), 0]\n    \n    \tdef initialize(self):\n\
      \t    \tself.ch_state = self.init_state[1]\n\t    \tself.curr_ch = self.init_state[0]\n\
      \t    \tself.sent_mess = 0\n        \n    \tdef one_hot(self, index, depth):\n\
      \t    \toh = np.zeros(depth, dtype=np.float32)\n\t    \toh[index] = 1\n\t  \
      \  \treturn oh\n    \n    \n    class DQAgent():\n    \tdef __init__(self, nb_channels,\
      \ learning_rate, gamma):\n    \t\tself.nb_ch = nb_channels\n    \t\tself.nb_actions\
      \ = nb_channels\n    \t\tself.gamma = gamma\n    \t\tself.learning_rate = learning_rate\n\
      \    \t\t\n    \t\tself.history_length = 200\n    \t\tself.history = [[]for\
      \ i in range(self.history_length)]\n    \t\tself.history_idx = 0\n    \t\t\n\
      \    \t\t#Create and initialize the online DQN\n    \t\tself.DQN_online = tf.keras.models.Sequential([Dense(2*self.nb_ch,\
      \ activation='relu'), Dense(self.nb_actions, activation='softplus') #Outputs\
      \ positive values \n    \t\t])\n    \t\tself.DQN_online.build(input_shape=(None,\
      \ self.nb_ch*2)) #Build the model to create the weights\n    \t\t\n    \t\t\
      #Create and initialize the offline DQN\n    \t\tself.DQN_offline = tf.keras.models.Sequential([Dense(2*self.nb_ch,\
      \ activation='relu'), Dense(self.nb_actions, activation='softplus') #Outputs\
      \ positive values\n    \t\t])\n    \t\tself.DQN_offline.build(input_shape=(None,\
      \ self.nb_ch*2)) #Build the model to create the weights\n    \t\t\n    \t\t\
      self.copy_parameters() #Copy the weights of the online network to the offline\
      \ network\n    \t\t\n    \t\tself.loss_func = tf.keras.losses.MSE\n    \t\t\
      self.optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n        \n  \
      \      \n    \tdef choose_action(self, state, epsilon):\n    \t\t\"\"\"Implements\
      \ an epsilon-greedy policy\"\"\"\n        \n    \t\t# Explore ?\n    \t\tif\
      \ np.random.uniform(size=1) < epsilon :\n    \t\t\taction =  np.random.randint(low\
      \ = 0, high = 7) ####### Take one action randomly {-1, 0, 1} with probability\
      \ epsilon\n        \n    \t\t#Choose the best action\n    \t\telse:    \n  \
      \  \t\t\tsensed_ch, curr_ch, end = state #Decomposes the state\n    \t\t\t'''\n\
      \    \t\t\t* The shapes are :\n    \t\t\t- curr_ch : [self.nb_ch]\n    \t\t\t\
      - sensed_ch : [self.nb_ch]\n    \t\t\t- end : one integer\n    \t\t\t'''\n \
      \   \t\t\tDQN_input = tf.concat([sensed_ch, curr_ch], axis=0)[tf.newaxis, :]\
      \ #Create a state vector, which is the DQN input. Shape : [1, 2*self.nb_ch]\n\
      \    \t\t\toutputs = self.DQN_online(DQN_input).numpy() #Get the predicted Q\
      \ values corresponding to the 3 actions\n    \t\t\taction = np.argmax(outputs)-1\
      \ #Take the action that has the highest predicted Q value (-1, 0, 1)\n     \
      \       \n    \t\treturn action\n    \n    \tdef learn(self, batch_size):\n\
      \    \t\t\"\"\"Sample experiences from the history and performs SGD\"\"\"\n\
      \    \t\t\n    \t\t# Samples random experiences from the history\n    \t\tidx\
      \ = np.random.choice(range(self.history_length), batch_size, replace=False)\
      \ # Create random indexes \n    \t\trdm_exp =  [self.history[i] for i in idx]\
      \ # Take experiences corresponding to the random indexes\n    \t\t\n    \t\t\
      # Each experience is written in this format : [state_vec, end, action, reward,\
      \ n_state_vec, n_end] (see insert_history method)\n    \t\t\n    \t\t# Create\
      \ 6 batches : states_vec, end_boolean, actions, rewards, new states_vec, new_end_boolean\n\
      \    \t\tstates_vec = np.array([rdm_exp[i][0] for i in range(batch_size)]) #\
      \ Shape : [Bs, 2*self.nb_ch]\n    \t\tend = np.array([rdm_exp[i][1] for i in\
      \ range(batch_size)]) # Shape : [Bs]\n    \t\tactions = np.array([rdm_exp[i][2]\
      \ for i in range(batch_size)]) # Shape : [BS]\n    \t\trewards = np.array([rdm_exp[i][3]\
      \ for i in range(batch_size)]) # Shape : [BS]\n    \t\tn_states_vec = np.array([rdm_exp[i][4]\
      \ for i in range(batch_size)]) # Shape : [Bs, 2*self.nb_ch]\n    \t\tn_end =\
      \ np.array([rdm_exp[i][5] for i in range(batch_size)]) # Shape : [BS]\n    \t\
      \t\n    \t\t#Compute the best q_value for the new states\n    \t\tmax_n_q_values\
      \ = tf.reduce_max(self.DQN_offline(n_states_vec), axis=1).numpy()\n\n    \t\t\
      with tf.GradientTape() as tape:\n    \t\t\t#Forward pass through the online\
      \ network to predict the q_values\n    \t\t\tpred_q_values = self.DQN_online(states_vec)\n\
      \    \t\t\t\n    \t\t\t# Compute targets\n    \t\t\ttargets = pred_q_values.numpy()\n\
      \    \t\t\ttargets[np.arange(targets.shape[0]), actions]= rewards + (1-n_end)\
      \ * self.gamma * max_n_q_values\n    \t\t\t\n    \t\t\t# Evaluate the loss\n\
      \    \t\t\tself.loss = self.loss_func(pred_q_values, targets)\n        \n  \
      \  \t\t# Compute gradients and perform the gradient descent\n    \t\tgradients\
      \ = tape.gradient(self.loss, self.DQN_online.trainable_weights)\n    \t\tself.optimizer.apply_gradients(zip(gradients,\
      \ self.DQN_online.trainable_weights))  \n    \n    \tdef insert_history(self,\
      \ state, action, reward, n_state):\n    \t\t\"\"\"Insert experience in history\"\
      \"\"\n        \n    \t\tsensed_ch, curr_ch, end = state\n    \t\tstate_vec =\
      \ np.concatenate([sensed_ch, curr_ch], axis=0) # Create the state vector for\
      \ the state\n        \n    \t\tn_sensed_ch, n_curr_ch, n_end = n_state\n   \
      \ \t\tn_state_vec = np.concatenate([n_sensed_ch, n_curr_ch], axis=0) # Create\
      \ the state vector for the new state\n\n    \t\tself.history[self.history_idx]\
      \ = [state_vec, end, action, reward, n_state_vec, n_end] # Insert everything\
      \ in the history\n        \n    \t\tself.history_idx = (self.history_idx+1)%self.history_length\
      \ # Move the history_idx by one\n    \n    \tdef copy_parameters(self):\n  \
      \  \t\t\"\"\"Copy the parameters of the online network to the offline network\"\
      \"\"\n\n    \t\tweights = self.DQN_online.get_weights()\n    \t\tself.DQN_offline.set_weights(weights)\n\
      \n\t\t\n\n    def make_grid(self,grid,channels):\n        grid = np.append(grid,\
      \ channels).reshape(self.window_size+1,7)\n        grid = np.delete(grid, 0,\
      \ 0)\n        grid = grid.reshape(self.window_size,self.number_of_channels)\n\
      \        return grid.astype(np.float32)\n"
    active_threshold: '0.9'
    affinity: ''
    alias: ''
    comment: ''
    epsilon: '1'
    gamma: '1'
    initial_channel: '6'
    learning_rate: '0.01'
    maxoutbuf: '0'
    minoutbuf: '0'
    num_GPU: '1'
    number_of_channels: '7'
    seed: '1'
    slot_time: '1'
    window_size: '128'
  states:
    _io_cache: ('RL Channels select self agent', 'blk', [('number_of_channels', '7'),
      ('seed', '0'), ('num_GPU', '0.0'), ('active_threshold', '0.9'), ('slot_time',
      '1'), ('window_size', '5'), ('initial_channel', '0'), ('gamma', '1'), ('learning_rate',
      '0.01'), ('epsilon', '1')], [('0', 'complex', 1), ('1', 'complex', 1), ('2',
      'complex', 1), ('3', 'complex', 1), ('4', 'complex', 1), ('5', 'complex', 1),
      ('6', 'complex', 1)], [('0', 'short', 1)], 'Embedded Python Block example -
      a simple multiply const', ['active_threshold', 'epsilon', 'gamma', 'initial_channel',
      'learning_rate', 'number_of_channels', 'slot_time', 'window_size'])
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [1373, 901]
    rotation: 0
    state: enabled
- name: epy_block_4
  id: epy_block
  parameters:
    _source_code: "\"\"\"\nEmbedded Python Blocks:\n\nEach time this file is saved,\
      \ GRC will instantiate the first class it finds\nto get ports and parameters\
      \ of your block. The arguments to __init__  will\nbe the parameters. All of\
      \ them are required to have default values!\n\"\"\"\n\nimport numpy as np\n\
      from gnuradio import gr\nimport time\nfrom datetime import datetime, timedelta\n\
      \nclass blk(gr.sync_block):  # other base classes are basic_block, decim_block,\
      \ interp_block\n    \"\"\"Embedded Python Block example - a simple multiply\
      \ const\"\"\"\n\n    def __init__(self, example_param=1.0):  # only default\
      \ arguments here\n        \"\"\"arguments to this function show up as parameters\
      \ in GRC\"\"\"\n        gr.sync_block.__init__(\n            self,\n       \
      \     name='Selected Channel',   # will show up in GRC\n            in_sig=[np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.uint16],\n\
      \            out_sig=[np.complex64]\n        )\n        # if an attribute with\
      \ the same name as a parameter is found,\n        # a callback is registered\
      \ (properties work, too).\n        self.example_param = example_param\n    \
      \    self.last_datetime = datetime.now()\n        self.index = 0\n\n    def\
      \ work(self, input_items, output_items):\n        \"\"\"example: multiply with\
      \ constant\"\"\"\n        if (datetime.now() - self.last_datetime) > timedelta(days=0,\
      \ seconds=1, microseconds=0, milliseconds=0, minutes=0, hours=0, weeks=0):\n\
      \            #when training is done, make a decision  \n            self.last_datetime\
      \ = datetime.now()\n            self.index = input_items[7][0]\n           \
      \ #print(self.index)\n            if self.index < 0 or self.index > 6:\n   \
      \         \tself.index = 0\n\n        output_items[0][:] = input_items[self.index][:]\n\
      \        return len(output_items[0])\n"
    affinity: ''
    alias: ''
    comment: ''
    example_param: '1.0'
    maxoutbuf: '0'
    minoutbuf: '0'
  states:
    _io_cache: ('Selected Channel', 'blk', [('example_param', '1.0')], [('0', 'complex',
      1), ('1', 'complex', 1), ('2', 'complex', 1), ('3', 'complex', 1), ('4', 'complex',
      1), ('5', 'complex', 1), ('6', 'complex', 1), ('7', 'short', 1)], [('0', 'complex',
      1)], 'Embedded Python Block example - a simple multiply const', ['example_param'])
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [1616, 105]
    rotation: 0
    state: true
- name: note_0
  id: note
  parameters:
    alias: ''
    comment: ''
    note: Deterministic channals
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [466, 558]
    rotation: 0
    state: true
- name: note_0_0
  id: note
  parameters:
    alias: ''
    comment: ''
    note: Probalistic channals
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [470, 838]
    rotation: 0
    state: disabled
- name: note_1
  id: note
  parameters:
    alias: ''
    comment: ''
    note: return feedback from Rx as reward
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [1385, 768]
    rotation: 0
    state: true
- name: qtgui_freq_sink_x_0
  id: qtgui_freq_sink_x
  parameters:
    affinity: ''
    alias: ''
    alpha1: '1.0'
    alpha10: '1.0'
    alpha2: '1.0'
    alpha3: '1.0'
    alpha4: '1.0'
    alpha5: '1.0'
    alpha6: '1.0'
    alpha7: '1.0'
    alpha8: '3'
    alpha9: '1.0'
    autoscale: 'False'
    average: '1.0'
    axislabels: 'True'
    bw: samp_rate
    color1: '"blue"'
    color10: '"dark blue"'
    color2: '"red"'
    color3: '"green"'
    color4: '"black"'
    color5: '"cyan"'
    color6: '"magenta"'
    color7: '"yellow"'
    color8: '"dark blue"'
    color9: '"dark green"'
    comment: ''
    ctrlpanel: 'False'
    fc: '0'
    fftsize: '1024'
    freqhalf: 'True'
    grid: 'False'
    gui_hint: ''
    label: Relative Gain
    label1: CH1
    label10: ''''''
    label2: CH2
    label3: CH3
    label4: CH4
    label5: CH5
    label6: CH6
    label7: CH7
    label8: '''CHOISE'''
    label9: ''''''
    legend: 'True'
    maxoutbuf: '0'
    minoutbuf: '0'
    name: '""'
    nconnections: '8'
    showports: 'False'
    tr_chan: '0'
    tr_level: '0.0'
    tr_mode: qtgui.TRIG_MODE_FREE
    tr_tag: '""'
    type: complex
    units: dB
    update_time: '0.10'
    width1: '1'
    width10: '1'
    width2: '1'
    width3: '1'
    width4: '1'
    width5: '1'
    width6: '1'
    width7: '1'
    width8: '1'
    width9: '1'
    wintype: firdes.WIN_BLACKMAN_hARRIS
    ymax: '10'
    ymin: '-140'
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [969, -89]
    rotation: 90
    state: true
- name: qtgui_number_sink_0
  id: qtgui_number_sink
  parameters:
    affinity: ''
    alias: ''
    autoscale: 'False'
    avg: '0'
    color1: ("black", "black")
    color10: ("black", "black")
    color2: ("black", "black")
    color3: ("black", "black")
    color4: ("black", "black")
    color5: ("black", "black")
    color6: ("black", "black")
    color7: ("black", "black")
    color8: ("black", "black")
    color9: ("black", "black")
    comment: ''
    factor1: '1'
    factor10: '1'
    factor2: '1'
    factor3: '1'
    factor4: '1'
    factor5: '1'
    factor6: '1'
    factor7: '1'
    factor8: '1'
    factor9: '1'
    graph_type: qtgui.NUM_GRAPH_HORIZ
    gui_hint: ''
    label1: ''
    label10: ''
    label2: ''
    label3: ''
    label4: ''
    label5: ''
    label6: ''
    label7: ''
    label8: ''
    label9: ''
    max: '7'
    min: '0'
    name: '""'
    nconnections: '1'
    type: short
    unit1: ''
    unit10: ''
    unit2: ''
    unit3: ''
    unit4: ''
    unit5: ''
    unit6: ''
    unit7: ''
    unit8: ''
    unit9: ''
    update_time: '0.10'
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [1676, 385]
    rotation: 0
    state: true
- name: qtgui_waterfall_sink_x_0
  id: qtgui_waterfall_sink_x
  parameters:
    affinity: ''
    alias: ''
    alpha1: '0.5'
    alpha10: '1.0'
    alpha2: '0.5'
    alpha3: '1.0'
    alpha4: '1.0'
    alpha5: '1.0'
    alpha6: '1.0'
    alpha7: '1.0'
    alpha8: '1.0'
    alpha9: '1.0'
    axislabels: 'True'
    bw: samp_rate
    color1: '3'
    color10: '0'
    color2: '2'
    color3: '0'
    color4: '0'
    color5: '0'
    color6: '0'
    color7: '0'
    color8: '0'
    color9: '0'
    comment: ''
    fc: '0'
    fftsize: '1024'
    freqhalf: 'True'
    grid: 'True'
    gui_hint: ''
    int_max: '10'
    int_min: '-140'
    label1: ''
    label10: ''
    label2: ''
    label3: ''
    label4: ''
    label5: ''
    label6: ''
    label7: ''
    label8: ''
    label9: ''
    legend: 'True'
    maxoutbuf: '0'
    minoutbuf: '0'
    name: '""'
    nconnections: '2'
    showports: 'False'
    type: complex
    update_time: '0.10'
    wintype: firdes.WIN_BLACKMAN_hARRIS
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [1921, 53]
    rotation: 0
    state: true
- name: zeromq_pub_msg_sink_0
  id: zeromq_pub_msg_sink
  parameters:
    address: ''
    affinity: ''
    alias: ''
    bind: 'True'
    comment: ''
    timeout: '100'
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [1851, 813]
    rotation: 0
    state: disabled
- name: zeromq_pull_msg_source_0
  id: zeromq_pull_msg_source
  parameters:
    address: ''
    affinity: ''
    alias: ''
    bind: 'False'
    comment: ''
    maxoutbuf: '0'
    minoutbuf: '0'
    timeout: '100'
  states:
    bus_sink: false
    bus_source: false
    bus_structure: null
    coordinate: [1384, 819]
    rotation: 0
    state: disabled

connections:
- [analog_sig_source_x_0, '0', epy_block_1, '0']
- [analog_sig_source_x_0, '0', epy_block_1_0, '0']
- [analog_sig_source_x_0, '0', epy_block_4, '0']
- [analog_sig_source_x_0_0, '0', epy_block_1, '1']
- [analog_sig_source_x_0_0, '0', epy_block_1_0, '1']
- [analog_sig_source_x_0_0, '0', epy_block_4, '1']
- [analog_sig_source_x_0_0_0, '0', epy_block_1, '2']
- [analog_sig_source_x_0_0_0, '0', epy_block_1_0, '2']
- [analog_sig_source_x_0_0_0, '0', epy_block_4, '2']
- [analog_sig_source_x_0_0_0_0, '0', epy_block_1, '3']
- [analog_sig_source_x_0_0_0_0, '0', epy_block_1_0, '3']
- [analog_sig_source_x_0_0_0_0, '0', epy_block_4, '3']
- [analog_sig_source_x_0_0_0_0_0, '0', epy_block_1, '4']
- [analog_sig_source_x_0_0_0_0_0, '0', epy_block_1_0, '4']
- [analog_sig_source_x_0_0_0_0_0, '0', epy_block_4, '4']
- [analog_sig_source_x_0_0_0_0_0_0, '0', epy_block_1, '5']
- [analog_sig_source_x_0_0_0_0_0_0, '0', epy_block_1_0, '5']
- [analog_sig_source_x_0_0_0_0_0_0, '0', epy_block_4, '5']
- [analog_sig_source_x_0_0_0_0_0_0_0, '0', epy_block_1, '6']
- [analog_sig_source_x_0_0_0_0_0_0_0, '0', epy_block_1_0, '6']
- [analog_sig_source_x_0_0_0_0_0_0_0, '0', epy_block_4, '6']
- [blocks_add_xx_0_0_0_0_0_0_0_0, '0', qtgui_waterfall_sink_x_0, '0']
- [channels_channel_model_0, '0', blocks_add_xx_0_0_0_0_0_0_0_0, '0']
- [channels_channel_model_0, '0', epy_block_0, '0']
- [channels_channel_model_0, '0', epy_block_3, '0']
- [channels_channel_model_0, '0', epy_block_3_0, '0']
- [channels_channel_model_0, '0', qtgui_freq_sink_x_0, '0']
- [channels_channel_model_0_0, '0', blocks_add_xx_0_0_0_0_0_0_0_0, '1']
- [channels_channel_model_0_0, '0', epy_block_0, '1']
- [channels_channel_model_0_0, '0', epy_block_3, '1']
- [channels_channel_model_0_0, '0', epy_block_3_0, '1']
- [channels_channel_model_0_0, '0', qtgui_freq_sink_x_0, '1']
- [channels_channel_model_0_0_0, '0', blocks_add_xx_0_0_0_0_0_0_0_0, '2']
- [channels_channel_model_0_0_0, '0', epy_block_0, '2']
- [channels_channel_model_0_0_0, '0', epy_block_3, '2']
- [channels_channel_model_0_0_0, '0', epy_block_3_0, '2']
- [channels_channel_model_0_0_0, '0', qtgui_freq_sink_x_0, '2']
- [channels_channel_model_0_0_0_0, '0', blocks_add_xx_0_0_0_0_0_0_0_0, '3']
- [channels_channel_model_0_0_0_0, '0', epy_block_0, '3']
- [channels_channel_model_0_0_0_0, '0', epy_block_3, '3']
- [channels_channel_model_0_0_0_0, '0', epy_block_3_0, '3']
- [channels_channel_model_0_0_0_0, '0', qtgui_freq_sink_x_0, '3']
- [channels_channel_model_0_0_0_0_0, '0', blocks_add_xx_0_0_0_0_0_0_0_0, '4']
- [channels_channel_model_0_0_0_0_0, '0', epy_block_0, '4']
- [channels_channel_model_0_0_0_0_0, '0', epy_block_3, '4']
- [channels_channel_model_0_0_0_0_0, '0', epy_block_3_0, '4']
- [channels_channel_model_0_0_0_0_0, '0', qtgui_freq_sink_x_0, '4']
- [channels_channel_model_0_0_0_0_0_0, '0', blocks_add_xx_0_0_0_0_0_0_0_0, '5']
- [channels_channel_model_0_0_0_0_0_0, '0', epy_block_0, '5']
- [channels_channel_model_0_0_0_0_0_0, '0', epy_block_3, '5']
- [channels_channel_model_0_0_0_0_0_0, '0', epy_block_3_0, '5']
- [channels_channel_model_0_0_0_0_0_0, '0', qtgui_freq_sink_x_0, '5']
- [channels_channel_model_0_0_0_0_0_0_0, '0', blocks_add_xx_0_0_0_0_0_0_0_0, '6']
- [channels_channel_model_0_0_0_0_0_0_0, '0', epy_block_0, '6']
- [channels_channel_model_0_0_0_0_0_0_0, '0', epy_block_3, '6']
- [channels_channel_model_0_0_0_0_0_0_0, '0', epy_block_3_0, '6']
- [channels_channel_model_0_0_0_0_0_0_0, '0', qtgui_freq_sink_x_0, '6']
- [channels_channel_model_0_1, '0', qtgui_freq_sink_x_0, '7']
- [channels_channel_model_0_1, '0', qtgui_waterfall_sink_x_0, '1']
- [epy_block_0, '0', epy_block_2, '0']
- [epy_block_0, '0', qtgui_number_sink_0, '0']
- [epy_block_1, '0', channels_channel_model_0, '0']
- [epy_block_1, '1', channels_channel_model_0_0, '0']
- [epy_block_1, '2', channels_channel_model_0_0_0, '0']
- [epy_block_1, '3', channels_channel_model_0_0_0_0, '0']
- [epy_block_1, '4', channels_channel_model_0_0_0_0_0, '0']
- [epy_block_1, '5', channels_channel_model_0_0_0_0_0_0, '0']
- [epy_block_1, '6', channels_channel_model_0_0_0_0_0_0_0, '0']
- [epy_block_1_0, '0', channels_channel_model_0, '0']
- [epy_block_1_0, '1', channels_channel_model_0_0, '0']
- [epy_block_1_0, '2', channels_channel_model_0_0_0, '0']
- [epy_block_1_0, '3', channels_channel_model_0_0_0_0, '0']
- [epy_block_1_0, '4', channels_channel_model_0_0_0_0_0, '0']
- [epy_block_1_0, '5', channels_channel_model_0_0_0_0_0_0, '0']
- [epy_block_1_0, '6', channels_channel_model_0_0_0_0_0_0_0, '0']
- [epy_block_2, '0', blocks_null_sink_0, '0']
- [epy_block_3, '0', qtgui_number_sink_0, '0']
- [epy_block_3_0, '0', epy_block_4, '7']
- [epy_block_3_0, '0', qtgui_number_sink_0, '0']
- [epy_block_4, '0', channels_channel_model_0_1, '0']
- [zeromq_pull_msg_source_0, out, zeromq_pub_msg_sink_0, in]

metadata:
  file_format: 1
