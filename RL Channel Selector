"""
Embedded Python Blocks:

Each time this file is saved, GRC will instantiate the first class it finds
to get ports and parameters of your block. The arguments to __init__  will
be the parameters. All of them are required to have default values!
"""

import os
import copy
import sys      
import time
import schedule
import threading
from datetime import datetime, timedelta
from timeslot import Timeslot  
import numpy as np
import matplotlib.pyplot as plt
import torch
import tensorflow as tf
from gnuradio import gr
import pmt
from tensorflow.keras import Model
from tensorflow.keras.layers import Layer, Dense, Softmax
from tensorflow.keras.layers import Dense

from gym import Env
from gym.spaces import Discrete, Box


from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg
from matplotlib.figure import Figure
from PyQt5 import QtCore #conda install pyqt
from PyQt5 import QtWidgets
HISTORY_BUFFER_LEN = 2000

class blk(gr.sync_block):  # other base classes are basic_block, decim_block, interp_block
    """Embedded Python Block example - a simple multiply const"""

    def __init__(self,number_of_channels=7,seed = 0, num_GPU=0.0, active_threshold= 0.9, slot_time = 1, window_size = 5, initial_channel = 0, gamma = 1, learning_rate = 0.01, epsilon = 1):  # only default arguments here
        """arguments to this function show up as parameters in GRC"""
        gr.sync_block.__init__(
            self,
            name='RL Channels select self agent',   # will show up in GRC
            in_sig=[np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64,np.complex64],
            out_sig=[np.uint16]
        )
        # if an attribute with the same name as a parameter is found,
        # a callback is registered (properties work, too).
        
        #initailize tensorflow and GPU usage
        print('Tensorflow version: ', tf.__version__)
        gpus = tf.config.experimental.list_physical_devices("GPU")
        print('Number of GPUs available :', len(gpus))
        
        if num_GPU < len(gpus):
        	tf.config.experimental.set_visible_devices(gpus[num_GPU], 'GPU')
        	tf.config.experimental.set_memory_growth(gpus[num_GPU], True)
        	print('Only GPU number', num_GPU, 'used')
        tf.random.set_seed(seed)
	
        #initialize Channel sensing
        self.active_threshold = active_threshold
        self.number_of_channels = number_of_channels
        if (self.number_of_channels+1)%2!=0:
        	print('/!\ Please enter an even number of channels')
        self.channels = np.zeros(self.number_of_channels)
        self.slot_time = slot_time
        self.last_datetime = datetime.now()
        self.window_size = window_size
        self.grid = np.zeros((self.window_size, self.number_of_channels))
        self.initial_channel = initial_channel #change to random
        self.grid_flag = 0
        self.channel_decision = self.initial_channel
        
        self.collision = 0
        self.total_error = 0 
        self.total_count = 1 
        #initialize DQN env
        #self.dq_env = self.DQEnv(self.number_of_channels, self.window_size, False, self.grid.T)

        #initialize DQN Neural Network
        self.gamma = gamma
        self.learning_rate = learning_rate
        self.actions_number = self.number_of_channels
        self.epsilon = epsilon
        self.nb_trainings = 400
        self.loss = []  # Keep trak of the losses
        self.channel_decision = self.initial_channel
        self.train_flag = False
        self.done = False
        
        self.agent_dq = self.DQAgent(self.number_of_channels, self.learning_rate, self.gamma)
        #sensed_ch, curr_ch, end = self.dq_env.get_init_state()
        #print('##################################')
        #print('ENV initial state:')
        #print('State: ', sensed_ch,', Current Channel: ', curr_ch)
        #print('##################################')
      

    def work(self, input_items, output_items):       	
        #Check slot time
        if (datetime.now() - self.last_datetime) > timedelta(days=0, seconds=self.slot_time, microseconds=0, milliseconds=0, minutes=0, hours=0, weeks=0):
            #Sample and mark channels activity
            for i in range(0,np.size(input_items,0)):
            	#Energy detected during time slot, mark as occupied 
            	if (np.mean(abs(input_items[i][:]))>self.active_threshold):
            		self.channels[i] = 1
            for j in range (np.size(self.channels)): self.channels[j] = np.abs(self.channels[j]-1)#flip ones and zeros
            print(self.channels,'    ', datetime.now())   
            #when training is done, make a decision
            if self.done == True:
            	state = [self.channels.astype(np.float32), self.dq_env.one_hot(self.channel_decision, self.number_of_channels), 0]
            	self.channel_decision = self.agent_dq.choose_action(state, 0)# epsilon = 0 only expoiltation wehn not training
            	self.total_count += 1
            	self.total_ratio = (self.total_error)/self.total_count
            	print('CH: ',self.channels)
            	temp = np.zeros(np.size(self.channels))
            	temp[self.channel_decision]=1
            	print('DC: ',temp)
            	print('count: ', self.total_count, ' Errors: ', self.total_error, ' ratio: ',self.total_ratio)
         	
            	if self.channels[self.channel_decision] == 0:
            		self.collision += 1             
            self.grid = self.make_grid(self.grid,(self.channels))
            self.last_datetime = datetime.now()
            self.channels = np.zeros(self.number_of_channels)
            #print(self.grid)
            
            self.grid_flag += 1
        output_items[0][:]= self.channel_decision      
        if self.collision == 1:
        	self.total_error += 1
        	self.collision = 0
        	#self.grid_flag = self.window_size+1
        	

        	
        #Full window sized grid
        if (self.grid_flag == self.window_size+1): 
		
        	#initialize DQN env
        	self.dq_env = self.DQEnv(self.number_of_channels, self.window_size, self.initial_channel, self.grid.T)
	
		# Run 200 episodes
        	for i in range(HISTORY_BUFFER_LEN):
                    
                    self.dq_env.initialize()
                    
                    state = self.dq_env.get_init_state()
                    action = self.agent_dq.choose_action(state, self.epsilon)
                    reward, new_state = self.dq_env.run(action)
	    
                    self.agent_dq.insert_history(state, action, reward, new_state) # Insert state, action, reward, new state in history 
	    
                    # While it's not the end of the episode
                    while new_state[2]==0 :
                    	state = new_state
                    	action = self.agent_dq.choose_action(state, self.epsilon)
                    	reward, new_state = self.dq_env.run(action)
		
                    	self.agent_dq.insert_history(state, action, reward, new_state) # Insert state, action, reward, new state in history
        	
        	self.train_flag = True                      	
        	print('Replay buffer History populated')
        	                 	 

        	
        	

        if self.train_flag == True:
	        print('Training...')
        	self.train_flag = False
        	
        	for i in range(self.nb_trainings):
        		self.dq_env.initialize()
        		state = self.dq_env.get_init_state()
    
        		action = self.agent_dq.choose_action(state, self.epsilon)
        		reward, new_state = self.dq_env.run(action)
        		self.agent_dq.insert_history(state, action, reward, new_state) # Insert state, action, reward, new state in history 
        		self.agent_dq.learn(64) # Each time we store a new history, we perform a training on random data
    
        		# While it's not the end of the episode
        		while new_state[2]==0 :
        			state = new_state
        			action = self.agent_dq.choose_action(state, self.epsilon)
        			reward, new_state = self.dq_env.run(action)
        			self.agent_dq.insert_history(state, action, reward, new_state) # Insert state, action, reward, new state in history 
        			self.agent_dq.learn(64) # Each time we store a new history, we perform a training on random data
    
        		self.loss.append(tf.reduce_mean(self.agent_dq.loss).numpy()) #Save the losses for future visualization
        			#Every 10 iterations we copy the parameters of the online DQN to the offline DQN
        		if (i+1)%10 == 0:
        			self.agent_dq.copy_parameters()
        			print((i+1), end=', ')
        	self.done = True
        	#plt.semilogy(np.arange(len(self.loss)), self.loss)
        	#plt.show()
        	self.evaluate_dq_agent(self.agent_dq , self.dq_env.grid)
        return len(output_items[0])
        

    
    
    class DQEnv():
    	def __init__(self, nb_channels = 7, nb_states=5, initial_state = 0 , grid = []):
        
	    	if (nb_channels+1)%2!=0:
	    		print('/!\ Please enter an even number of channels')
            
	    	self.nb_ch = nb_channels
	    	self.nb_states = nb_states
        
	    	self.grid = grid
	    	print ('env grid:')
	    	print(self.grid)
	    	self.init_state = [int(initial_state), int(self.nb_states-1)]
	    	self.ch_state = self.init_state[1]
	    	self.curr_ch = self.init_state[0]
	    	self.sent_mess = 0
        
	    	print('This environment has ' + str(self.nb_ch) +' different channels')
        
    	def run(self, action):
        
	    	self.sent_mess += 1
        
	    	self.ch_state  = (self.ch_state + 1)%self.nb_states
        
	    	self.curr_ch = action
	    	
	    	reward = np.abs(self.grid[self.curr_ch, self.ch_state])
        
	    	if self.sent_mess != self.nb_states: 
	    		end = 0
	    	else :
	    		end = 1
        
	    	return(reward, [self.grid[:, self.ch_state], self.one_hot(self.curr_ch, self.nb_ch), end])
    
    	def get_init_state(self):
	    	return [self.grid[:, self.init_state[1]], self.one_hot(self.init_state[0], self.nb_ch), 0]
    
    	def initialize(self):
	    	self.ch_state = self.init_state[1]
	    	self.curr_ch = self.init_state[0]
	    	self.sent_mess = 0
        
    	def one_hot(self, index, depth):
	    	oh = np.zeros(depth, dtype=np.float32)
	    	oh[index] = 1
	    	return oh
    
    
    class DQAgent():
    	def __init__(self, nb_channels, learning_rate, gamma):
    		self.nb_ch = nb_channels
    		self.nb_actions = nb_channels
    		self.gamma = gamma
    		self.learning_rate = learning_rate
    		
    		self.history_length = 2000
    		self.history = [[]for i in range(self.history_length)]
    		self.history_idx = 0
    		
    		#Create and initialize the online DQN
    		self.DQN_online = tf.keras.models.Sequential([Dense(2*self.nb_ch, activation='relu'), Dense(self.nb_actions, activation='softplus') #Outputs positive values 
    		])
    		self.DQN_online.build(input_shape=(None, self.nb_ch*2)) #Build the model to create the weights
    		
    		#Create and initialize the offline DQN
    		self.DQN_offline = tf.keras.models.Sequential([Dense(2*self.nb_ch, activation='relu'), Dense(self.nb_actions, activation='softplus') #Outputs positive values
    		])
    		self.DQN_offline.build(input_shape=(None, self.nb_ch*2)) #Build the model to create the weights
    		
    		self.copy_parameters() #Copy the weights of the online network to the offline network
    		
    		self.loss_func = tf.keras.losses.MSE
    		self.optimizer = tf.keras.optimizers.Adam(self.learning_rate)
        
        
    	def choose_action(self, state, epsilon):
    		"""Implements an epsilon-greedy policy"""
        
    		# Explore ?
    		if np.random.uniform(size=1) < epsilon :
    			action =  np.random.randint(low = 0, high = 6) ####### Take one action randomly {0, 1, 2, 3, 4, 5, 6} with probability epsilon
    			
        
    		#Choose the best action
    		else:    
    			sensed_ch, curr_ch, end = state #Decomposes the state
    			'''
    			* The shapes are :
    			- curr_ch : [self.nb_ch]
    			- sensed_ch : [self.nb_ch]
    			- end : one integer
    			'''
    			DQN_input = tf.concat([sensed_ch, curr_ch], axis=0)[tf.newaxis, :] #Create a state vector, which is the DQN input. Shape : [1, 2*self.nb_ch]
    			outputs = self.DQN_online(DQN_input).numpy() #Get the predicted Q values corresponding to the 3 actions
    			action = np.argmax(outputs)#-1 #Take the action that has the highest predicted Q value (0, 1, 2, 3, 4, 5, 6)
    			#print(outputs)
    		return action
    
    	def learn(self, batch_size):
    		"""Sample experiences from the history and performs SGD"""
    		
    		# Samples random experiences from the history
    		idx = np.random.choice(range(self.history_length), batch_size, replace=False) # Create random indexes 
    		rdm_exp =  [self.history[i] for i in idx] # Take experiences corresponding to the random indexes
    		
    		# Each experience is written in this format : [state_vec, end, action, reward, n_state_vec, n_end] (see insert_history method)
    		
    		# Create 6 batches : states_vec, end_boolean, actions, rewards, new states_vec, new_end_boolean
    		states_vec = np.array([rdm_exp[i][0] for i in range(batch_size)]) # Shape : [Bs, 2*self.nb_ch]
    		end = np.array([rdm_exp[i][1] for i in range(batch_size)]) # Shape : [Bs]
    		actions = np.array([rdm_exp[i][2] for i in range(batch_size)]) # Shape : [BS]
    		rewards = np.array([rdm_exp[i][3] for i in range(batch_size)]) # Shape : [BS]
    		n_states_vec = np.array([rdm_exp[i][4] for i in range(batch_size)]) # Shape : [Bs, 2*self.nb_ch]
    		n_end = np.array([rdm_exp[i][5] for i in range(batch_size)]) # Shape : [BS]
    		
    		#Compute the best q_value for the new states
    		max_n_q_values = tf.reduce_max(self.DQN_offline(n_states_vec), axis=1).numpy()

    		with tf.GradientTape() as tape:
    			#Forward pass through the online network to predict the q_values
    			pred_q_values = self.DQN_online(states_vec)
    			
    			# Compute targets
    			targets = pred_q_values.numpy()
    			targets[np.arange(targets.shape[0]), actions]= rewards + (1-n_end) * self.gamma * max_n_q_values
    			
    			# Evaluate the loss
    			self.loss = self.loss_func(pred_q_values, targets)
        
    		# Compute gradients and perform the gradient descent
    		gradients = tape.gradient(self.loss, self.DQN_online.trainable_weights)
    		self.optimizer.apply_gradients(zip(gradients, self.DQN_online.trainable_weights))  
    
    	def insert_history(self, state, action, reward, n_state):
    		"""Insert experience in history"""
        
    		sensed_ch, curr_ch, end = state
    		state_vec = np.concatenate([sensed_ch, curr_ch], axis=0) # Create the state vector for the state
        
    		n_sensed_ch, n_curr_ch, n_end = n_state
    		n_state_vec = np.concatenate([n_sensed_ch, n_curr_ch], axis=0) # Create the state vector for the new state

    		self.history[self.history_idx] = [state_vec, end, action, reward, n_state_vec, n_end] # Insert everything in the history
        
    		self.history_idx = (self.history_idx+1)%self.history_length # Move the history_idx by one
    
    	def copy_parameters(self):
    		"""Copy the parameters of the online network to the offline network"""

    		weights = self.DQN_online.get_weights()
    		self.DQN_offline.set_weights(weights)

		

    def make_grid(self,grid,channels):
        grid = np.append(grid, channels).reshape(self.window_size+1,7)
        grid = np.delete(grid, 0, 0)
        grid = grid.reshape(self.window_size,self.number_of_channels)
        return grid.astype(np.float32)
    
    def evaluate_dq_agent(self, agent, grid):

        action_history=[]
        tot_reward = 0
    
        self.dq_env.initialize()
        first_state = self.dq_env.get_init_state()
        action = agent.choose_action(first_state, epsilon = 0)
        action_history.append(action)
        reward, new_state = self.dq_env.run(action)
        tot_reward += reward
    
        for j in range(grid.shape[1]-1):
            state = new_state
            action = agent.choose_action(state, epsilon = 0)
            action_history.append(action)
            reward, new_state = self.dq_env.run(action)
            tot_reward += reward
            
        choosen_channels = [(grid.shape[0]-1)/2]
        for i in range(len(action_history)):
        	choosen_channels.append((choosen_channels[i]+action_history[i])%grid.shape[0])
        choosen_channels = choosen_channels[1:]
        #plt.imshow(np.flip(grid, axis=0), origin="lower", cmap='gray', vmin=0, vmax=1)
        #for i in range(len(choosen_channels)):
        #	plt.scatter(i, grid.shape[0]-1-choosen_channels[i], color='r')
       # plt.show()
        print(str(int(tot_reward))+'/'+str(grid.shape[1])+' packets have been transmitted')
        return tot_reward
